####################################################################################################
# 1. 필요패키지 준비
####################################################################################################

# --------------------------------------------------
# 1.1 여러 패키지 동시 인스톨 & 로딩
# --------------------------------------------------
# !!!! 가급적 KoNLP패키지를 먼저 인스톨할 수 있도록 배치순서를 제일먼저 설정함

# 필요한 패키지 목록 생성
pkg <- c('KoNLP', 'stringr', 'magrittr', 'purrr', 'dplyr', 'readr', 
         'readxl', 'openxlsx', 'r2excel', 'tm', 'RWeka')

# 필요패키지 설지여부를 체크해 미설치패키지 목록을 저장
new_pkg <- pkg[!(pkg %in% rownames(installed.packages()))]

# 미설치 패키지 목록이 1개라도 있으면, 일괄 인스톨 실시
if (length(new_pkg)) install.packages(new_pkg, dependencies = TRUE)
# if (length(new_pkg)) install.packages(new_pkg, dependencies = TRUE, repos="http://R-Forge.R-project.org")

# 필요패키지를 일괄 로딩실시
suppressMessages(sapply(pkg, require, character.only = TRUE))  
# - suppressMessages() 함수를 통해 패키지 로딩시 나타나는 복잡한 설명/진행상황 문구 출력억제

# --------------------------------------------------
# 2.2 KoNLP 형태소 사전 로딩
# --------------------------------------------------

# 2.2.1 NIA사전 로딩
# --------------------------------------------------
useNIADic()
# - 약 98만(983012) 개 정도의 한글단어가 등임록되어 있음
# - NIA사전 로딩후 다시 KoNLP라이브러리의 current 폴더에 있는 
#   dic_user.txt파일을 오픈해 보면 NIA사전의 단어목록이 업데이트 되어 있음을 알 수 있음

# ==> 3가지 사전을 모두 로딩할 필요는 없으며, 가장 나중에 로딩한 사전이 디폴트사전으로 설정됨
#     통상적으로 가장 포괄적인 형태소데이터가 들어가 있는 useNIADic() 사전을 로딩해서 사용하면됨


# 2.2.2 세종사전만 별도로 로딩해서 사용가능함
# useSejongDic()
# --------------------------------------------------
# - 약 37만(370957개) = 세종사전(8만7천여개) + 시스템사전(28만여개) 단어를 로딩해서 준비시킴 
# - 세종사전 로딩후 다시 KoNLP라이브러리의 current 폴더에 있는 
#   dic_user.txt파일을 오픈해 보면 세종사전의 단어목록이 업데이트 되어 있음을 알 수 있음
# - 별도로 useSystemDic()으로 로딩하면, 약 28만(283949) 여개 단어가 포함된 시스템사전만을 이용하는 것임

####################################################################################################
# 2. NIADic활용 한글명사단어 추출: 개별 텍스트자료 대상  
####################################################################################################

# --------------------------------------------------
# 2.1 개별 텍스트자료 준비
# --------------------------------------------------

# 2.1.1 코퍼스객체 로딩
# --------------------------------------------------

# 준비된 코퍼스객체 데이터파일 로딩
load('./datatm/ymbaek_papers_kor_pp.Rdata')
# - 다양한 전처리 작업을 거친 코퍼스객체를 저장한 .Rdata파일을 로딩

# 메모리에 로딩된 코퍼스객체 확인
ls()
# - ymbaek_papers_kor_pp.Rdata이라는 데이터파일이 로딩되면서, 
#   같이 묶여 있던 my객체(원본코퍼스객체), my_pp_gram객체(엔그램까지 전처리 적용)가 
#   각각 별도 객체로 메모리 상에 존재함 

# 전처리된 코퍼스객체 기본조회
my_pp_gram

# 2.1.2 전처리가 적용된 코퍼스객체 문서 중에서 1개를 추출
# --------------------------------------------------

# 말뭉치객체 내용 중 특정한 문서 1개를 별도로 추출해 분석대상 문자열로 만듬
txt <- my_pp_gram[[1]]$content %>% print
class(txt)
# - 이 객체는 주어진 한글텍스트셋 안에 있는 문단이나 문장 구조는 상관없이
#   하나의 문자열데이터로 고려하여 분석하는 것임

# --------------------------------------------------
# 2.2 개별 텍스트자료에 대한 한글명사단어 분석
# --------------------------------------------------

# 2.2.1 KoNLP::extractNoun()함수를 이용한 한글명사추출
# --------------------------------------------------
# 문자열에서 한글명사 추출내용 파악
sprintf('개별 텍스트자료에서 한글명사 추출내용 파악:')
txt_wd <- extractNoun(txt) %>% print
txt_wd %>% length
# - 한글텍스트셋에 들어 있는 다양한 문구/단어들을 형태소로 분해한 다음에 
#   여러가지 품사들 중에서 의미파악이 상대적으로 쉬운 명사를 중심으로 추출함  


# 2.2.2 추출된 한글명사단어 패턴을 통한 전처리 방향성 
# --------------------------------------------------
# - 문자열데이터의 내용순서에 따라 분리된 단어패턴을 보고 
#   어떠한 방향으로 전처를 진행할지 판단해야함(전처리 적용순서는 상관없음)

# [A] 신규단어등록 필요
#     - 기본적으로 NIADic형태소사전을 이용해 한글명사단어를 추출하고, 
#       제대로 추출이 안되는 경우 직접 텍스트셋에 접근해 필요한 단어를 결합/변경/삽입해서 추출함

#     (1) NIADic형태소사전에 신규단어를 추가등록해 추출하는 방법 
#         (1.1) NIADic에 필요한 신규단어를 하나하나 등록 
#               - 신규단어를 직접 하나하나 NIADic에 직접 입력해
#                 사용자사전 목록을 업데이트해 나가면서 전처리 가능함 
#         (1.2) 별도의 파일형식에 신규단어 pool을 준비
#               - 별도의 나만의 신규단어사전을 파일로 만들어 놓고 
#                 신규단어목록을 한꺼번에 NIADic에 업데이트 할 수 있음

#     (2) 텍스트셋에 직접 접근해 필요한 신규단어를 결합/변경/삽입해 추출하는 방법  
#         (2.1) 결합방식으로 신규단어 직접 텍스트셋에 업데이트
#               - NIADic사전에서 추출/분리했으나 결합단어로 만들어야 의미가 분명해지는 단어
#               - 예: "전화" "조사" ==> 전화조사
#         (2.2) 변경방식으로 신규단어 직접 텍스트셋에 업데이트
#              - NIADIC사전에서 제대로 추출/분리하지 못해 수정/변경해서 만들어야 하는 단어 
#              - 예: "확률" "적" "표집방법에" ==> 비확률적표집방법
#         (2.3) 삽입방식으로 누락된 신규단어 직접 텍스트셋에 업데이트
#              - NIADic사전에서 추출하지 못해 누락되어 직접 추가삽입 해야하는 단어
#              - 예: "감에" "감을" "감도" ==> 감 

# [B] 불용어처리 필요
#     - 한글단어명사로 제대로 추출되었거나 빈도수가 많은 단어라 할지라도 
#       의미파악에 도움이 안되는 경우 불용어로 간주하여 삭제처리함

#     (1) 글자수를 기준으로 불필요 단어 제거
#         - 글자수1개부터 시작해 글자사n개까지 레벨별로 해당단어목록을 확인해
#           불용어로 판단되는 단어들을 직접 삭제처리함
#         - 특정글자수에 해당하는 단어들을 추출해서 유지대상단어와 삭제대상단어가 몇 개가 있는지 비교함
#         - 삭제대상 단어가 적으면, 직접 삭제대상 단어를 지정해 텍스트셋에서 삭제처리함
#         - 유지대상 단어가 적으면, 특정글자수 단어들에서 유지대상 단어를 차집합으로 제외시켜 
#           삭제대상 단어만을 추출해 텍스트셋에서 삭제처리함 

#     (2) 발현빈도수를 기준으로 불필요 단어 제거
#         - 단어별 발현한 빈도수를 카운팅하여 내림차순으로 정렬한 다음에
#           일정한 빈도수이하를 기록한 단어들을 불용어로 판단하여 삭제처리함
#         - 분석대상 텍스트셋에 따라 기준으로 설정할 발현빈도수는 다를 수 있음

#     (3) 직접 1:1 삭제방식으로 불필요 단어 제거
#         - 글자수나 발현빈도수를 기준으로 불용어처리를 한 다음에 
#           남아 있는 텍스트셋을 구성하는 단어들 중에서 
#           분석대상으로서 가치가 낮은 단어를 불용어로 추가판단하여 직접 1:1로 삭제처리함

#     (4) 불용어 목록 파일저장
#         - 글자수나 발현빈도수 기준으로 선정한 불용어목록과 직접 1:1로 삭제처리한 불용어목록들을 
#           별도의 파일(플레인텍스트/엑셀)에 저장해서 언제든지 메모리로 로딩해 재활용 할 수 있음



# (C) 어근동일화 필요 ==> 단어의미(어간)은 동일/유사한데 어미에서 다양한 변화가 주어지는 경우
# - (1) 일정한 패턴이 있는 동의/유사어들은 정규표현식을 통해서 일괄 특정단어로 동일화함
# - (2) 다양한 철자로 인해서 일정한 패턴으로 검색하기 어려운 경우에는
#         직접 1:1변환방식으로 특정단어로 변경을 해나가면서 전처리 가능함 
# - (3) 별도의 나만의 동의/유사어사전을 만들어 놓고 
#         해당하는 신규단어 하나하나를 등록해서 일괄 동일화함 
#         (소스코드상에 벡터객체로, 또는 csv/엑셀파일로)


# 2.2.3 개별 텍스트자료에서 한글명사 추출내용을 문자순서 기준 빈도수 파악
# --------------------------------------------------
sprintf('개별 텍스트자료에서 한글명사 추출내용을 문자순서 기준으로 발현빈도 정렬:')
txt_wd_tb <- txt_wd %>% table %>% print
txt_wd_tb %>% sum
# - 추출된 한글명사 단어들을 원래 개별 텍스트자료의 내용순서가 아닌  
#   문자순서를 기준으로 정렬해 파악함
# - 이 조회내용을 토대로 명사적 어간표현은 동일한데 어미가 달라서 
#   다른 단어로 카운팅되는 표현들을 찾아내어 전처리 방향성을 검토해야 함
# - 앞서 (1) 신규단어등록, (2) 불용어처리, (3) 어근동일화 라는 3가지 전처리 관점에서 검토필요

sprintf('개별 텍스트자료에서 한글명사 추출내용을 발현빈도수를 기준으로 정렬:')
txt_wd_fq <- txt_wd %>% table %>% sort(decreasing = TRUE) %>% print
txt_wd_fq %>% sum
# - 추출된 한글명사 단어들을 원래 개별 텍스트자료의 내용순서가 아닌  
#   출현빈도수를 기준으로 정렬해 파악함
# - 이 조회내용을 토대로 어떠한 빈도가 높은 한글명사 단어에 집중해서 
#   전처리가 필요한지를 검토해야 함
# - 앞서 (1) 신규단어등록, (2) 불용어처리, (3) 어근동일화 라는 3가지 전처리 관점에서 검토필요


# 2.2.4 개별 텍스트자료에서 한글명사 추출내용을 데이터프레임 구조로 파악
# --------------------------------------------------
# 개별 텍스트자료에서 추출한 한글명사들의 발현빈도수를 별도의 데이터프레임 객체로 저장
txt_wd_df <- data.frame(term = names(txt_wd_tb), freq = c(txt_wd_tb), row.names = NULL)

# 개별 텍스트자료에서 추출한 한글명사들을 문자순서를 기준으로 정렬
txt_wd_df %>% print %>% nrow
# - 이 조회내용을 토대로 명사적 어간표현은 동일한데 어미가 달라서 
#   다른 단어로 카운팅되는 표현들을 찾아내어 전처리 방향성을 검토해야 함
# - 앞서 (1) 신규단어등록, (2) 불용어처리, (3) 어근동일화 라는 3가지 전처리 관점에서 검토필요

# 개별 텍스트자료에서 추출한 한글명사들을 발현 빈도수를 기준으로 내림차순 정렬
txt_wd_df %>% dplyr::arrange(desc(freq), term) %>% print
# - 이 조회내용을 토대로 어떠한 빈도가 높은 한글명사 단어에 집중해서 
#   전처리가 필요한지를 검토해야 함
# - 앞서 (1) 신규단어등록, (2) 불용어처리, (3) 어근동일화 라는 3가지 전처리 관점에서 검토필요

# 전체 말뭉치에서 한글명사 추출내용을 일정수준 이상의 빈도수를 기준으로 내림차순 정렬
txt_wd_df %>% arrange(desc(freq), term) %>% filter(freq >= 4) %>% print


####################################################################################################
# 3. 신규단어 등록: NIADic 형태소사전에 신규단어 등록을 통한 전처리  
####################################################################################################

# --------------------------------------------------
# 3.1 신규단어 후보파악
# --------------------------------------------------

# 3.1.1 한글명사 추출내용 재조회
# --------------------------------------------------
txt # - 전처리 적용한 텍스트
txt_wd # - 원문순서대로 추출된 단어조회
# - txt객체와 txt_wd객체 간 비교를 통해서 등록대상 신규단어 후보를 파악함


# 3.1.2 신규단어 등록대상 후보파악
# --------------------------------------------------
# - 다음과 같이 3가지 관점에서 NIADic형태소사전에 신규단어를 
#   순차적으로 또는 한꺼번에 등록해 나가면서 반복적으로 한글단어를 추출할 수 있음
# - NIADic에 신규단어가 등록되어 재추출하다보면, 최초로 파악된 신규단어 등록대상 후보들 중에서 
#   의도대로 잘 추출되는 단어도 있고, 부분적으로 추출되거나 아예 누락되는 경우에 변화가 올 수 있음

# (1) 결합할 신규단어 등록후보
#      ==> NIADic사전에서 추출/분리했으나 결합단어로 만들어야 의미가 분명해지는 단어
# - "전화" "조사" ==> 전화조사
# - "대표" "성" ==> 대표성
# - "휴대" "전화" ==> 휴대전화
# - "문제" "점" ==> 문제점
# - "성향" "가중" "방법" ==> 성향가중방법
# - "반복" "비례" "가중" "법" ==> 반복비례가중법
# - "여론조사" "방법" ==> 여론조사방법 
# - "보정" "방법" ==> 보정방법 
# - "연구" "설계" ==> 연구설계

# (2) 변경할 신규단어 등록후보
#     ==> NIADIC사전에서 제대로 추출/분리하지 못해 수정/변경해서 만들어야 하는 단어 
# - "문제" "점들" ==> 문제점
# - "연구" "설계가" ==> 연구설계
# - "표본" "편파" ==> 표본편차 
# - "확률" "적" "표집방법에" ==> 비확률적표집방법
# - "인구" "통계학적" ==> 인구통계학

# (3) 누락된 신규단어 등록후보
#     ==> NIADic사전에서 추출하지 못해 누락되어 직접 추가삽입 해야하는 단어
# - "대안적"6 ==> 대안 6
# - "감에"2 "감을"1 "감도"1 ==> 감 4
#    (왼쪽 숫자는 누락된 갯수, 오른쪽 숫자는 해당단어로 지정해 삽입해야하는 갯수)

# --------------------------------------------------
# 3.2 NIADic 형태소사전에 신규단어 1개 직접 등록
# --------------------------------------------------

# 신규단어 1개를 NIA사전에 입력
buildDictionary(ext_dic = c('sejong', 'worimalsam', 'insighter'),
                user_dic = data.frame(term = '전화조사', tag = 'ncn'),
                category_dic_nms = c('general'),
                replace_usr_dic = TRUE)
# - ext_dic: 신규단어를 등록할 NIADic 세부 사전명칭 입력
# - user_dic: 신규단어명와 품사유형을 데이터프레임 형식으로
# - category_dic_nms: 신규단어가 소속될 카테고리명칭
# - replace_usr_dic:
#   => FALSE 신규단어를 추가할 때마다 기존 사용자 신규단어목록에 추가됨
#   => TRUE  신규단어를 추가할 때마다 기존 사용자 신규단어목록은 없애고 새롭게 등록됨

# 등록된 신규단어목록 확인
get_dictionary('user_dic')

# 업데이트된 NIADic활용 한글명사 재추출
sprintf('신규단어 등록한 NIADic활용 개별 텍스트자료에서 한글명사 추출내용 파악:')
txt_wd_new <- extractNoun(txt) %>% print
# - "전화" "조사" ==> "전화조사"로 추출됨

# --------------------------------------------------
# 3.3 NIADic 형태소사전에 신규단어 여러개 직접 등록
# --------------------------------------------------

# 신규단어 여러개를 직접 NIA사전에 입력
buildDictionary(ext_dic = c('sejong', 'worimalsam', 'insighter'),
                user_dic = data.frame(term = c('전화조사', '성향가중방법'), tag = c('ncn', 'ncn')),
                category_dic_nms = c('general'),
                replace_usr_dic = TRUE)

# 등록된 신규단어목록 확인
get_dictionary('user_dic')

# 업데이트된 NIADic활용 한글명사 재추출
sprintf('신규단어 등록한 NIADic활용 개별 텍스트자료에서 한글명사 추출내용 파악:')
txt_wd_new <- extractNoun(txt) %>% print
# - "전화" "조사" ==> "전화조사"로 추출됨
# - "성향" "가중" "방법" ==> "성향가중방법"으로 추출됨

# --------------------------------------------------
# 3.4 별도 벡터객체 내용을 NIADic 형태소사전에 일괄등록
# --------------------------------------------------

# 3.4.1 신규단어 여러개를 별도의 벡터객체로 마련
# --------------------------------------------------
new_wd <- c('전화조사', '성향가중방법', '반복비례가중법', '대표성', '감') %>% print
new_tg <- rep('ncn', length(new_wd)) %>% print
# - 신규단어 갯수만큼 명사형품사태그를 자동으로 반복생성함

# 벡터객체로 마련된 신규단어목록과 품사태그를 데이터프레임(티블) 객체로 작성
new_wd_df <- tibble(term = new_wd, tag = new_tg) %>% print


# 3.4.2 준비된 신규단어 목록과 품사태그를 NIADic형태소 사전에 업데이트
# --------------------------------------------------
buildDictionary(ext_dic = c('sejong', 'worimalsam', 'insighter'),
                user_dic = data.frame(new_wd_df),
                category_dic_nms = c('general'),
                replace_usr_dic = TRUE)

# 등록된 신규단어목록 확인
get_dictionary('user_dic')


# 3.4.3 업데이트된 NIADic활용 한글명사 재추출
# --------------------------------------------------
sprintf('신규단어 등록한 NIADic활용 개별 텍스트자료에서 한글명사 추출내용 파악:')
txt_wd_new <- extractNoun(txt) %>% print

# 신규단어 중 추출이 제대로 나온 경우
# - "전화" "조사" ==> "전화조사"로 추출됨
# - "성향" "가중" "방법" ==> "성향가중방법"으로 추출됨
# - "반복" "비례" "가중" "법" ==> "반복비례가중법"으로 추출됨

# 신규단어 중 추출이 안된 경우
# - "대표" "성" ==> (그대로)
# - "감" ==> (누락됨)

# ==> NIADic 형태소 사전은 기존 SejongDic 형태소사전에 비해 
#     기본 단어와 카테고리분류체계가 대폭 확충되었음
# ==> 그러나, 신규단어 추가시 제대로 반영이 되는 경우와 그렇지 않은 경우가 있어
#     신규단어 추가기능은 불확실성이 있음

# --------------------------------------------------
# 3.5 별도 플레인텍스트파일(plain-text file)을 이용한 신규단어 관리
# --------------------------------------------------

# 3.5.1 신규단어를 별로 플레인텍스트 파일로 저장
# --------------------------------------------------

# 필요시 신규단어 여러 개를 벡터객체에 추가
new_wd <- c('휴대전화', '대안') %>% print
new_tg <- rep('ncn', length(new_wd)) %>% print
# - 신규단어 갯수만큼 명사형품사태그를 자동으로 반복생성함

# 메모리상의 데이터프레임에 신규단어 추가
new_wd_df <- add_row(new_wd_df, term = new_wd, tag = new_tg) %>% print

# 신규단어 목록중 중복단어 제거한 고유단어로 정제
new_wd_df %<>% distinct %>% print

# 메모리상에 신규단어 추가된 데이터프레임을 플레인텍스트파일로 저장 
write_delim(x = new_wd_df, path = './datatm/my_new_word.csv', delim = '\t')

# 플레인텍스트파일 직접오픈해서 추가된 신규단어 등록여부 확인
browseURL('./datatm/my_new_word.csv')

# 플레인텍스트 파일에서 신규단어 목록 재로딩
new_wd_df <- read_delim(file = './datatm/my_new_word.csv', 
                        col_names = TRUE, delim = '\t') %>% print
# - readr패키지 read_delim()함수를 이용해 일반 플레인텍스트파일에 입력된 신규단어 목록을 
#   데이터프레임(티블) 형식으로 메모리로 로딩함
# - 추가된 '휴대전화' 신규단어가 등록되었음을 알 수 있음


# 3.5.2 메모리로 로딩한 플레인텍스트파일 내용을 NIADic 형태소사전에 일괄등록
# --------------------------------------------------
# 데이터프레임 객체에 준비된 신규단어 목록과 품사태그를 NIADic형태소 사전에 업데이트
buildDictionary(ext_dic = c('sejong', 'worimalsam', 'insighter'),
                user_dic = data.frame(new_wd_df),
                category_dic_nms = c('general'),
                replace_usr_dic = TRUE)

# 등록된 신규단어목록 확인
get_dictionary('user_dic')

# 3.5.3 업데이트된 NIADic활용 한글명사 재추출
# --------------------------------------------------
sprintf('신규단어 등록한 NIADic활용 개별 텍스트자료에서 한글명사 추출내용 파악:')
txt_wd_new <- extractNoun(txt) %>% print

# 신규단어 중 추출이 제대로 나온 경우
# - "전화" "조사" ==> "전화조사"로 추출됨
# - "성향" "가중" "방법" ==> "성향가중방법"으로 추출됨
# - "반복" "비례" "가중" "법" ==> "반복비례가중법"으로 추출됨

# 신규단어 중 추출이 안된 경우
# - "대표" "성" ==> (그대로)
# - "휴대" "전화" ==> (그대로)
# - "대안" ==> (원문에 여러 개 있으나 1개만 추출됨)
# - "감" ==> (누락됨)

# ==> NIADic 형태소 사전은 기존 SejongDic 형태소사전에 비해 
#     기본 단어와 카테고리분류체계가 대폭 확충되었음
# ==> 그러나, 신규단어 추가시 제대로 반영이 되는 경우와 그렇지 않은 경우가 있어
#     신규단어 추가기능은 불확실성이 있음

# --------------------------------------------------
# 3.6 별도 엑셀파일을 이용한 신규단어 관리
# --------------------------------------------------

# 3.6.1 메모리 상에 엑셀파일 객체 생성
# --------------------------------------------------
# 신규 워크북(엑셀파일)객체를 메모리상에 생성
wb <- openxlsx::createWorkbook() %>% print

# 신규워크북에 신규단어를 저장할 워크시트를 생성
openxlsx::addWorksheet(wb, sheetName = 'newword')

# 3.6.2 신규단어 목록 저장
# --------------------------------------------------

# 필요시 신규단어 여러 개를 벡터객체에 추가
new_wd <- c('문제점', '연구설계') %>% print
new_tg <- rep('ncn', length(new_wd)) %>% print
# - 신규단어 갯수만큼 명사형품사태그를 자동으로 반복생성함

# 메모리상의 데이터프레임에 신규단어 추가
new_wd_df <- add_row(new_wd_df, term = new_wd, tag = new_tg) %>% print

# 신규단어 목록중 중복단어 제거한 고유단어로 정제
new_wd_df %<>% distinct %>% print

# 워크시트에 메모리상의 신규단어목록을 저장함
openxlsx::writeData(wb, sheet = 1, new_wd_df)

# 메모리상의 엑셀파일객체를 작업경로에 물리적으로 저장
openxlsx::saveWorkbook(wb, './datatm/my_new_word.xlsx', overwrite = TRUE)

# 엑셀파일 직접오픈해서 추가된 신규단어 등록여부 확인
r2excel::xlsx.openFile("./datatm/my_new_word.xlsx")

# 3.6.3 신규단어 엑셀파일 로딩
# --------------------------------------------------
# 엑셀파일 시트명 확인
excel_sheets('./datatm/my_new_word.xlsx')

# 엑셀파일에서 신규단어 시트내용 로딩
new_wd_df <- read_excel(path = './datatm/my_new_word.xlsx', 
                        sheet = 'newword', col_names = TRUE) %>% print
# - readr패키지 read_delim()함수를 이용해 일반 플레인텍스트파일에 입력된 신규단어 목록을 
#   데이터프레임(티블) 형식으로 메모리로 로딩함

# 3.6.2 NIADic형태소 사전에 업데이트
# --------------------------------------------------
# 데이터프레임객체에 준비된 신규단어 목록과 품사태그를 NIADic형태소 사전에 업데이트
buildDictionary(ext_dic = c('sejong', 'worimalsam', 'insighter'),
                user_dic = data.frame(new_wd_df),
                category_dic_nms = c('general'),
                replace_usr_dic = TRUE)

# 등록된 신규단어목록 확인
get_dictionary('user_dic')

# 3.6.4 업데이트된 NIADic활용 한글명사 재추출
# --------------------------------------------------
sprintf('신규단어 등록한 NIADic활용 개별 텍스트자료에서 한글명사 추출내용 파악:')
txt_wd_new <- extractNoun(txt) %>% print

# 신규단어 중 추출이 제대로 나온 경우
# - "전화" "조사" ==> "전화조사"로 추출됨
# - "성향" "가중" "방법" ==> "성향가중방법"으로 추출됨
# - "반복" "비례" "가중" "법" ==> "반복비례가중법"으로 추출됨
# - "연구" "설계" ==> "반복비례가중법"으로 추출됨

# 신규단어 중 추출이 안된 경우
# - "대표" "성" ==> (그대로)
# - "휴대" "전화" ==> (그대로)
# - "문제" "점" ==> (그대로)
# - "대안" ==> (원문에 여러 개 있으나 1개만 추출됨)
# - "감" ==> (누락됨)

# ==> NIADic 형태소 사전은 기존 SejongDic 형태소사전에 비해 
#     기본 단어와 카테고리분류체계가 대폭 확충되었음
# ==> 그러나, 신규단어 추가시 제대로 반영이 되는 경우와 그렇지 않은 경우가 있어
#     신규단어 추가기능은 불확실성이 있음


####################################################################################################
# 4. 신규단어 등록: 텍스트셋에 필요한 신규단어를 결합/변경/삽입 방식으로 직접 업데이트 하기
####################################################################################################

# --------------------------------------------------
# 4.1 텍스트셋에 직접 업데이트할 신규단어 후보파악
# --------------------------------------------------

# 4.1.1 신규단어 등록대상 후보파악
# --------------------------------------------------
# - 원본 txt객체와 NIADic 신규단어등록을 통해 단어를 추출한 txt_wd객체간 
#   비교를 통해서등록대상 신규단어 후보를 파악함
txt
txt_wd_new
# - NIADic형태소 사전에 필요한 신규단어 등록을 해나가면서 한글명사단어를 추출했는데,
#   의도대로 잘 된 것도 있고 그렇지 않는 경우도 있으므로
#   텍스트셋에 필요한 신규단어를 직접 결합/변경/삽입이라는 3가지 방향으로 업데이트 할 수 있음


# 4.1.2 NIADic 사용대비 직접처리해야할 신규단어 등록대상 후보도출
# --------------------------------------------------
# (1) 결합할 신규단어 등록후보
#      ==> NIADic사전에서 추출/분리했으나 결합단어로 만들어야 의미가 분명해지는 단어

# NIADic에서 처리된 경우
# - "전화" "조사" ==> 전화조사
# - "성향" "가중" "방법" ==> 성향가중방법 
# - "반복" "비례" "가중" "법" ==> 반복비례가중법
# - "연구" "설계" ==> 연구설계

# 직접 업데이트 필요
# - "대표" "성" ==> 대표성
# - "휴대" "전화" ==> 휴대전화
# - "문제" "점" ==> 문제점
# - "여론조사" "방법" ==> 여론조사방법
# - "보정" "방법" ==> 보정방법

# (2) 변경할 신규단어 등록후보
#     ==> NIADIC사전에서 제대로 추출/분리하지 못해 수정/변경해서 만들어야 하는 단어 

# NIADic에서 처리된 경우
# - "연구" "설계가" ==> 연구설계

# 직접 업데이트 필요
# - "문제" "점들" ==> 문제점
# - "표본" "편파" ==> 표본편차 
# - "인구" "통계학적" ==> 인구통계학
# - "확률" "적" "표집" "방법" ==> 비확률적표집방법

# (3) 누락된 신규단어 등록후보
#     ==> NIADic사전에서 추출하지 못해 누락되어 직접 추가삽입 해야하는 단어

# NIADic에서 처리된 경우
# - 없음

# 직접 업데이트 필요
# - "대안적"6 ==> 대안 6
# - "감에"2 "감을"1 "감도"1 ==> 감 4
#    (왼쪽 숫자는 누락된 갯수, 오른쪽 숫자는 해당단어로 지정해 삽입해야하는 갯수)


# 4.1.3 추출된 한글단어 단일 문자열로 변환
# --------------------------------------------------
txt_one <- str_c(txt_wd_new, collapse = ' ') %>% print
# - 텍스트셋에 들어 있는 개별단어요소들을 하나의 문자열데이터로 만들어서 
#   정규표현식을 통한 필요 신규단어의 결합/변경/삽입을 쉽게 하기 위함임 


# --------------------------------------------------
# 4.2 결합방식으로 신규단어 직접 텍스트셋에 업데이트
# --------------------------------------------------

# 4.2.1 결합방식으로 업데이트할 후보단어 재확인
# --------------------------------------------------
# 결합할 신규단어 등록후보
# ==> NIADic사전에서 추출/분리했으나 결합단어로 만들어야 의미가 분명해지는 단어

# NIADic에서 처리된 경우
# - "전화" "조사" ==> 전화조사
# - "성향" "가중" "방법" ==> 성향가중방법 
# - "반복" "비례" "가중" "법" ==> 반복비례가중법
# - "연구" "설계" ==> 연구설계

# 직접 업데이트 필요
# - "대표" "성" ==> 대표성
# - "휴대" "전화" ==> 휴대전화
# - "문제" "점" ==> 문제점
# - "여론조사" "방법" ==> 여론조사방법
# - "보정" "방법" ==> 보정방법


# 4.2.2 결합방식으로 신규단어 직접 텍스트셋에 업데이트 
# --------------------------------------------------

# 4.2.2.1
# 결합대상 원본단어와 신규단어 목록 정의
new_combine_src <- c('전화 조사', '성향 가중 방법', '반복 비례 가중  법', 
                     '연구 설계', '대표 성', '휴대 전화', '문제 점', 
                     '여론조사 방법', '보정 방법') %>% print
# - NIADic에서 이미 처리된 것과 직접 업데이트가 필요한 목록을 모두 사용해도 됨 

new_combine_tgt <- str_replace_all(new_combine_src, pattern = ' ', replacement = '') %>% print
# - 분리되어 있는 결합대상 원본단어를 결합시켜 신규단어 목록으로 만들어 줌

# 4.2.2.2
# 결합방식으로 업데이트할 대상단어를 정규표현식으로 준비
new_combine_src_exp <- str_c('\\b', new_combine_src, '\\b') %>% print
# - 정규표현식에 단어경계설정방식을 통해서 결합한 단어들만 정확하게 추출할 수 있도록 함

# 분리되어 있는 단어들을 결합방식으로 신규단어 텍스트셋에 직접업데이트 실시 
for (i in 1:length(new_combine_src)) {
  txt_one <- str_replace_all(txt_one, 
                             pattern = new_combine_src_exp[i], 
                             replacement = new_combine_tgt[i])
}


# 4.2.3 결합방식으로 업데이트된 단일문자열을 명사형 단어단위로 추출(X) 분리(O)
# --------------------------------------------------
# 단일문자열 내용확인
txt_one 

# 단일문자열을 명사형 단어단위로 분리
txt_wd_new <- str_extract_all(txt_one, pattern = boundary('word')) %>% print
# - KoNLP::extractNoun() 함수를 이용하면 단어추출이 잘된 경우와 불안정한 경우가 공존하므로
#   이 방식으로 다시 단어 추출/분리를 하면 안됨
# - stringr::str_extract_all()함수의 boundary('word')옵션을 통해서 
#   공백을 기준으로 단어단위로 직접분리하는 방법을 사용해아먄 
#   결합방식으로 신규단어를 업데이트 처리한 한글단어들을 만날 수 있음

# NIADic에서 처리된 경우 --> 처리되었음
# - "전화" "조사" ==> 전화조사 
# - "성향" "가중" "방법" ==> 성향가중방법 
# - "반복" "비례" "가중" "법" ==> 반복비례가중법
# - "연구" "설계" ==> 연구설계

# 직접 업데이트 필요 --> 처리되었음
# - "대표" "성" ==> 대표성
# - "휴대" "전화" ==> 휴대전화
# - "문제" "점" ==> 문제점
# - "여론조사" "방법" ==> 여론조사방법
# - "보정" "방법" ==> 보정방법


# --------------------------------------------------
# 4.3 변경방식으로 신규단어 직접 텍스트셋에 업데이트
# --------------------------------------------------

# 4.3.1 변경방식으로 업데이트할 후보단어 재확인
# --------------------------------------------------
# 변경할 신규단어 등록후보
# ==> NIADIC사전에서 제대로 추출/분리하지 못해 수정/변경해서 만들어야 하는 단어 

# NIADic에서 처리된 경우
# - "연구" "설계가" ==> 연구설계

# 직접 업데이트 필요
# - "문제" "점들" ==> 문제점
# - "표본" "편파" ==> 표본편차 
# - "인구" "통계학적" ==> 인구통계학
# - "확률" "적" "표집" "방법" ==> 비확률적표집방법


# 4.3.2 변경방식으로 신규단어 직접 텍스트셋에 업데이트 
# --------------------------------------------------

# 4.3.2.1
# 변경대상 원본단어와 신규단어 목록 정의
new_change_src <- c('연구 설계가', '문제 점들', '표본 편파', '인구 통계학적', '확률 적 표집 방법') %>% print
# - NIADic에서 이미 처리된 것과 직접 업데이트가 필요한 목록을 모두 사용해도 됨 

new_change_tgt <- c('연구설계','문제점', '표본편차', '인구통계학', '비확률적표집방법') %>% print
# - 변경대상 원본단어를 수정해 신규단어 목록으로 만들어 줌

# 4.2.2.2
# 변경방식으로 업데이트할 대상단어를 정규표현식으로 준비
new_change_src_exp <- str_c('\\b', new_change_src, '\\b') %>% print
# - 정규표현식에 단어경계설정방식을 통해서 변경할 단어들만 정확하게 추출할 수 있도록 함

# 분리되어 있는 단어들을 변경방식으로 신규단어 텍스트셋에 직접업데이트 실시 
for (i in 1:length(new_change_src)) {
  txt_one <- str_replace_all(txt_one, 
                             pattern = new_change_src_exp[i], 
                             replacement = new_change_tgt[i])
}


# 4.3.3 변경방식으로 업데이트된 단일문자열을 명사형 단어단위로 추출(X) 분리(O)
# --------------------------------------------------
# 단일문자열 내용확인
txt_one 

# 단일문자열을 명사형 단어단위로 분리
txt_wd_new <- str_extract_all(txt_one, pattern = boundary('word')) %>% print
# - KoNLP::extractNoun() 함수를 이용하면 단어추출이 잘된 경우와 불안정한 경우가 공존하므로
#   이 방식으로 다시 단어 추출/분리를 하면 안됨
# - stringr::str_extract_all()함수의 boundary('word')옵션을 통해서 
#   공백을 기준으로 단어단위로 직접분리하는 방법을 사용해아먄 
#   변경방식으로 신규단어를 업데이트 처리한 한글단어들을 만날 수 있음

# NIADic에서 처리된 경우 --> 처리되었음
# - "연구" "설계가" ==> 연구설계

# 직접 업데이트 필요 --> 처리되었음
# - "문제" "점들" ==> 문제점
# - "표본" "편파" ==> 표본편차 
# - "인구" "통계학적" ==> 인구통계학
# - "확률" "적" "표집" "방법" ==> 비확률적표집방법


# --------------------------------------------------
# 4.4 삽입방식으로 누락된 신규단어 직접 텍스트셋에 업데이트
# -------------------------------------------------

# 4.4.1 삽입방식으로 업데이트할 후보단어 재확인
# --------------------------------------------------
# 누락된 신규단어 등록후보
# ==> NIADic사전에서 추출하지 못해 누락되어 직접 추가삽입 해야하는 단어

# NIADic에서 처리된 경우
# - 없음

# 직접 업데이트 필요
# - "대안적"6 ==> 대안 6
# - "감에"2 "감을"1 "감도"1 ==> 감 4
#    (왼쪽 숫자는 누락된 갯수, 오른쪽 숫자는 해당단어로 지정해 삽입해야하는 갯수)


# 4.3.2 삽입방식으로 누락된 신규단어 직접 텍스트셋에 업데이트 
# --------------------------------------------------

# 4.3.2.1
# 삽입대상 원본단어와 신규단어 목록 정의
new_insert_src <- c('대안', '감') %>% print
# - NIADic에서 이미 처리된 것과 직접 업데이트가 필요한 목록을 모두 사용해도 됨 

new_insert_tgt <- c(6, 4) %>% print
# - 누락되어 있는 변경대상 원본단어를 몇개 정도 반복해서 신규단어 목록으로 만들어 줄지 결정함

# 4.2.2.2
# 삽입방식으로 업데이트할 대상단어를 자동으로 반복생성 준비
new_insert_src_exp <- rep(new_insert_src, new_insert_tgt) %>% str_c(collapse = ' ') %>% print
# - 데이터를 필요한 갯수만큼 반복생성해주는 rep()함수통 통해서 삽입할 단어들을 준비함

# 누락되어 있는 단어들을 삽입방식으로 신규단어 텍스트셋에 직접업데이트 실시 
txt_one %<>% str_c(new_insert_src_exp, sep = ' ') %>% print


# 4.3.3 삽입방식으로 업데이트된 단일문자열을 명사형 단어단위로 추출(X) 분리(O)
# --------------------------------------------------
# 단일문자열 내용확인
txt_one 

# 단일문자열을 명사형 단어단위로 분리
txt_wd_new <- str_extract_all(txt_one, pattern = boundary('word')) %>% print
# - KoNLP::extractNoun() 함수를 이용하면 단어추출이 잘된 경우와 불안정한 경우가 공존하므로
#   이 방식으로 다시 단어 추출/분리를 하면 안됨
# - stringr::str_extract_all()함수의 boundary('word')옵션을 통해서 
#   공백을 기준으로 단어단위로 직접분리하는 방법을 사용해아먄 
#   삽입방식으로 신규단어를 업데이트 처리한 한글단어들을 만날 수 있음

# NIADic에서 처리된 경우 --> 해당사항없음
# - 없음

# 직접 업데이트 필요 --> 처리되었음
# - "대안적"6 ==> 대안 6
# - "감에"2 "감을"1 "감도"1 ==> 감 4
#    (왼쪽 숫자는 누락된 갯수, 오른쪽 숫자는 해당단어로 지정해 삽입해야하는 갯수)


####################################################################################################
# 5. 불용어(stopword) 처리를 통한 전처리  
####################################################################################################

# --------------------------------------------------
# 5.1 불용어 후보파악
# --------------------------------------------------

# 5.1.1 한글명사 추출내용 재조회
# --------------------------------------------------
# 단일문자열 내용확인
txt_one 

# 앞서 신규단어 등록을 통해 전처리하여 원문순서대로 추출한 한글단어 
txt_wd_new 


# 5.1.2 불용어 처리 방향
# --------------------------------------------------

# (1) 글자수를 기준으로 불필요 단어 제거
# - 글자수1개부터 시작해 글자사n개까지 레벨별로 해당단어목록을 확인해
#   불용어로 판단되는 단어들을 직접 삭제처리함

# - 특정글자수에 해당하는 단어들을 추출해서 유지대상단어와 삭제대상단어가 몇 개가 있는지 비교함
# - 삭제대상 단어가 적으면, 
#   ==> 직접 삭제대상 단어를 지정해 텍스트셋에서 삭제처리함
# - 유지대상 단어가 적으면, 
#   ==> 특정글자수 단어들에서 유지대상 단어를 차집합으로 제외시켜 
#       삭제대상 단어만을 추출해 텍스트셋에서 삭제처리함 

# (2) 발현빈도수를 기준으로 불필요 단어 제거

# - 단어별 발현한 빈도수를 카운팅하여 내림차순으로 정렬한 다음에
#   일정한 빈도수이하를 기록한 단어들을 불용어로 판단하여 삭제처리함
# - 분석대상 텍스트셋에 따라 기준으로 설정할 발현빈도수는 다를 수 있음


# (3) 직접 1:1 삭제방식으로 불필요 단어 제거
# - 글자수나 발현빈도수를 기준으로 불용어처리를 한 다음에 남아 있는 텍스트셋을 구성하는 단어들 중에서 
#   분석대상으로서 가치가 낮은 단어를 불용어로 추가판단하여 직접 1:1로 삭제처리함

# (4)  불용어 목록 파일저장
# - 글자수나 발현빈도수 기준으로 선정한 불용어목록과 직접 1:1로 삭제처리한 불용어목록들을 
#   별도의 파일(플레인텍스트/엑셀)에 저장해서 언제든지 메모리로 로딩해 재활용 할 수 있음


# 5.1.3 한글명사단어들의 컬럼수기준 분포파악
# --------------------------------------------------
txt_wd_new[[1]] %>% str_length() %>% table %>% print %>% sum
# - 전체 한글단어개수는 141개이며, 
#   컬럼수 유형은 글자수1개 ~ 글자수8개까지 존재함
# - 글자수1개 ~ 글자수4개까지는 각 레벨별로 불용어를 파악하며,
#   글자수5개 ~ 글자수8개까지는 묶어서 불용어를 파악해 전처리함


# --------------------------------------------------
# 5.2 컬럼수 1개를 기준으로 불필요 단어 제거
# --------------------------------------------------

# 5.2.1 글자수 1개인 한글명사 파악
# --------------------------------------------------

# 추출된 한글명사단어의 글자수가 1개인 내용 조회
stwd1 <- Filter(function(x) {str_length(x) == 1} , txt_wd_new[[1]]) %>% print %>% sort %>% print 
# - stw1: stopword one column
# - 앞서 신규단어를 추가해 한글단어를 추출한 txt_wd_new객체를 투입함
# - 추출된 순서대로 또는 가나다라 순서대로 조회함

# 글자수1개 단어별 빈도수 계산
stwd1 %>% table %>% print
# - 철자 순서대로 빈도수를 보여줌

# 글자수1개 단어들의 고유이름 추출
stwd1_uq <- stwd1 %>% table %>% names %>% print


# 5.2.2 글자수 1개인 한글명사 전처리 방향
# --------------------------------------------------
# - 대부분의 글자수 1개인 단어들은 삭제대상이며, 
#   "감"이라는 단어 정도가 유지해야할 단어로 판단됨
# - 따라서 삭제대상 단어가 많고 유지대상 단어가 적으므로 
#   글자수 1개인 한글명사에서 유지대상 단어를 제외하는 차집합개념으로 삭제대상 단어를 추출해 처리함 

stwd1_uq %>% print
# - 추출된 글자수1개 한글명사 고유이름 준비

stwd1_keep <- c('감') %>% print
# - 추출된 글자수1개 한글명사 중에서 유지해야할 단어

stwd1_del <- setdiff(stwd1_uq, stwd1_keep) %>% print
# - 삭제해야할 글자수1개 한글명사단어

# 추출된 삭제대상 글자수1개 한글명사단어 내용을 탐색하는 정규표현식 작성
stwd1_del_exp <- str_c('\\b', stwd1_del, '\\b') %>% print %>% str_c(collapse = '|') %>% print
# - 추출된 삭제대상 글자수1개 단어만을 단어경계로 설정한 다음에 
#   OR조건인 수직파이프라인(|)기호를 이용해서 하나하나 탐색할 수 있도록 정규표현식 작성


# 5.2.3 글자수 1개인 한글명사 중 삭제 전처리
# --------------------------------------------------
# 추출된 한글단어 단일 문자열로 변환
txt_one <- str_c(txt_wd_new[[1]], collapse = ' ') %>% print

# 단일문자열 내용에서 삭제대상 글자수1개 한글명사 전처리 
txt_one <- str_replace_all(txt_one, pattern = stwd1_del_exp, replacement = '') %>% print


# 5.2.4 글자수1개 불용어를 삭제한 단일문자열을 명사형 단어단위로 추출(X) 분리(O)
# --------------------------------------------------
txt_wd_stwd <- str_extract_all(txt_one, pattern = boundary('word')) %>% print
# - KoNLP::extractNoun() 함수를 이용하면 단어추출이 잘된 경우와 불안정한 경우가 공존하므로
#   이 방식으로 다시 단어 추출/분리를 하면 안됨
# - stringr::str_extract_all()함수의 boundary('word')옵션을 통해서 
#   공백을 기준으로 단어단위로 직접분리하는 방법을 사용해아먄 
#   글자수를 기준으로 불필요 단어를 삭제처리한 한글단어들을 만날 수 있음

# (유지대상) "감" --> 유지되었음 
# (삭제대상) 대부분의 글자수1개 한글단어 --> 삭제처리 되었음

# --------------------------------------------------
# 5.3 컬럼수 2개를 기준으로 불필요 단어 제거
# --------------------------------------------------

# 5.3.1 글자수 2개인 한글명사 파악
# --------------------------------------------------
# 추출된 한글명사단어의 글자수가 2개인 내용 조회
stwd2 <- Filter(function(x) {str_length(x) == 2} , txt_wd_stwd[[1]]) %>% print %>% sort %>% print 
# - 앞서 글자수1개 불용어를 처리한 txt_wd_stwd객체를 투입함
# - 추출된 순서대로 또는 가나다라 순서대로 조회함

# 글자수2개 단어별 빈도수 계산
stwd2 %>% table %>% print
# - 철자 순서대로 빈도수를 보여줌

# 글자수2개 단어들의 고유이름 추출
stwd2_uq <- stwd2 %>% table %>% names %>% print


# 5.3.2 글자수 2개인 한글명사 전처리 방향
# --------------------------------------------------
# - 대부분의 글자수 2개인 단어들은 유지대상이며, 
#   '고려' '기존' '추후' '최큰' '토대' '하거' '하기' '치명' '제기' '실제' '해당' '실정'
#   이라는 단어 정도가 삭제 해야할 단어로 판단됨
# - 따라서 유지대상 단어가 많고 삭제대상 단어가 적으므로 
#   글자수 2개인 한글명사에서 삭제대상 단어를 직접지정해 지워서 처리함 

stwd2_uq %>% print
# - 추출된 글자수2개 한글명사 고유이름 준비

stwd2_del <- c('고려', '기존', '추후', '최큰', '토대', '하거', 
               '하기', '치명', '제기', '실제', '해당', '실정') %>% print
# - 추출된 글자수2개 한글명사 중에서 삭제해야 할 단어

# 추출된 삭제대상 글자수2개 한글명사단어 내용을 탐색하는 정규표현식 작성
stwd2_del_exp <- str_c('\\b', stwd2_del, '\\b') %>% print %>% str_c(collapse = '|') %>% print
# - 추출된 삭제대상 글자수2개 단어만을 단어경계로 설정한 다음에 
#   OR조건인 수직파이프라인(|)기호를 이용해서 하나하나 탐색할 수 있도록 정규표현식 작성


# 5.3.3 글자수 2개인 한글명사 중 삭제 전처리
# --------------------------------------------------
# 추출된 한글단어 단일 문자열로 변환
txt_one <- str_c(txt_wd_stwd[[1]], collapse = ' ') %>% print

# 단일문자열 내용에서 삭제대상 글자수2개 한글명사 전처리 
txt_one <- str_replace_all(txt_one, pattern = stwd2_del_exp, replacement = '') %>% print


# 5.3.4 전처리된 단일 문자열을 명사형 단어단위로 추출(X) 분리(O)
# --------------------------------------------------
txt_wd_stwd <- str_extract_all(txt_one, pattern = boundary('word')) %>% print
# - KoNLP::extractNoun() 함수를 이용하면 단어추출이 잘된 경우와 불안정한 경우가 공존하므로
#   이 방식으로 다시 단어 추출/분리를 하면 안됨
# - stringr::str_extract_all()함수의 boundary('word')옵션을 통해서 
#   공백을 기준으로 단어단위로 직접분리하는 방법을 사용해아먄 
#   글자수를 기준으로 불필요 단어를 삭제처리한 한글단어들을 만날 수 있음

# (삭제대상) '고려' '기존' '추후' '최큰' '토대' '하거' '하기' 
#            '치명' '제기' '실제' '해당' '실정' --> 삭제처리 되었음
# (유지대상) 나머지 글자수2개 단어 --> 유지되었음 


# --------------------------------------------------
# 5.4 컬럼수 3개를 기준으로 불필요 단어 제거
# --------------------------------------------------

# 5.4.1 글자수 3개인 한글명사 파악
# --------------------------------------------------

# 추출된 한글명사단어의 글자수가 3개인 내용 조회
stwd3 <- Filter(function(x) {str_length(x) == 3} , txt_wd_stwd[[1]]) %>% print %>% sort %>% print 
# - 앞서 글자수2개 불용어를 처리한 txt_wd_stwd객체를 투입함
# - 추출된 순서대로 또는 가나다라 순서대로 조회함

# 글자수3개 단어별 빈도수 계산
stwd3 %>% table %>% print
# - 철자 순서대로 빈도수를 보여줌

# 글자수3개 단어들의 고유이름 추출
stwd3_uq <- stwd3 %>% table %>% names %>% print


# 5.4.2 글자수 3개인 한글명사 전처리 방향
# --------------------------------------------------
# - 대부분의 글자수 3개인 단어들은 유지대상이며, 
#   '지속적'이라는 단어 정도가 삭제해야할 단어로 판단됨
# - 따라서 유지대상 단어가 많고 삭제대상 단어가 적으므로 
#   글자수 3개인 한글명사에서 삭제대상 단어를 직접지정해 지워서 처리함 

stwd3_uq %>% print
# - 추출된 글자수3개 한글명사 고유이름 준비

stwd3_del <- c('지속적') %>% print
# - 추출된 글자수3개 한글명사 중에서 삭제해야 할 단어

# 추출된 삭제대상 글자수3개 한글명사단어 내용을 탐색하는 정규표현식 작성
stwd3_del_exp <- str_c('\\b', stwd3_del, '\\b') %>% print %>% str_c(collapse = '|') %>% print
# - 추출된 삭제대상 글자수3개 단어만을 단어경계로 설정한 다음에 
#   OR조건인 수직파이프라인(|)기호를 이용해서 하나하나 탐색할 수 있도록 정규표현식 작성


# 5.4.3 글자수 3개인 한글명사 중 삭제 전처리
# --------------------------------------------------
# 추출된 한글단어 단일 문자열로 변환
txt_one <- str_c(txt_wd_stwd[[1]], collapse = ' ') %>% print

# 단일문자열 내용에서 삭제대상 글자수3개 한글명사 전처리 
txt_one <- str_replace_all(txt_one, pattern = stwd3_del_exp, replacement = '') %>% print


# 5.4.4 전처리된 단일 문자열을 명사형 단어단위로 추출(X) 분리(O)
# --------------------------------------------------
txt_wd_stwd <- str_extract_all(txt_one, pattern = boundary('word')) %>% print
# - KoNLP::extractNoun() 함수를 이용하면 단어추출이 잘된 경우와 불안정한 경우가 공존하므로
#   이 방식으로 다시 단어 추출/분리를 하면 안됨
# - stringr::str_extract_all()함수의 boundary('word')옵션을 통해서 
#   공백을 기준으로 단어단위로 직접분리하는 방법을 사용해아먄 
#   글자수를 기준으로 불필요 단어를 삭제처리한 한글단어들을 만날 수 있음

# (삭제대상) '지속적' --> 삭제처리 되었음
# (유지대상) 나머지 글자수3개 단어 --> 유지되었음 


# --------------------------------------------------
# 5.5 컬럼수 4개를 기준으로 불필요 단어 제거
# --------------------------------------------------

# 5.5.1 글자수 4개인 한글명사 파악
# --------------------------------------------------

# 추출된 한글명사단어의 글자수가 4개인 내용 조회
stwd4 <- Filter(function(x) {str_length(x) == 4} , txt_wd_stwd[[1]]) %>% print %>% sort %>% print 
# - 앞서 글자수3개 불용어를 처리한 txt_wd_stwd객체를 투입함
# - 추출된 순서대로 또는 가나다라 순서대로 조회함

# 글자수4개 단어별 빈도수 계산
stwd4 %>% table %>% print
# - 철자 순서대로 빈도수를 보여줌

# 글자수4개 단어들의 고유이름 추출
stwd4_uq <- stwd4 %>% table %>% names %>% print


# 5.5.2 글자수 4개인 한글명사 전처리 방향
# --------------------------------------------------
# - 전체적으로 글자수 4개인 단어들은 모두 유지대상임이며,
#   별도로 전처리할 내용은 없음


# --------------------------------------------------
# 5.6 컬럼수 5개 이상를 기준으로 불필요 단어 제거
# --------------------------------------------------

# 5.6.1 글자수 5개 이상인 한글명사 파악
# --------------------------------------------------

# 추출된 한글명사단어의 글자수가 5개 이상인 내용 조회
stwd5more <- Filter(function(x) {str_length(x) >= 5} , txt_wd_stwd[[1]]) %>% print %>% sort %>% print 
# - 앞서 글자수4개 불용어를 처리한 txt_wd_stwd객체를 투입함
# - 추출된 순서대로 또는 가나다라 순서대로 조회함

# 글자수5개이상 단어별 빈도수 계산
stwd5more %>% table %>% print
# - 철자 순서대로 빈도수를 보여줌

# 글자수5개이상 단어들의 고유이름 추출
stwd5more_uq <- stwd5more %>% table %>% names %>% print


# 5.6.2 글자수 5개이상인 한글명사 전처리 방향
# --------------------------------------------------
# - 전체적으로 글자수 5개이상인 단어들은 모두 유지대상임이며,
#   별도로 전처리할 내용은 없음


# --------------------------------------------------
# 5.7 발현빈도수를 기준으로 불필요 단어 제거
# --------------------------------------------------

# 5.7.1 한글명사 추출내용 재조회
# --------------------------------------------------
# 단일문자열 내용확인
txt_one 

# 앞서 글자수를 기준으로 불용어를 파악해 전처리하여 원문순서대로 추출한 한글단어 
txt_wd_stwd


# 5.7.2 한글명사단어별 빈도수 파악
# --------------------------------------------------
# 단어철자를 기준으로 빈도수 파악
txt_wd_fq <- txt_wd_stwd[[1]] %>% table %>% print 


# 한글명사들의 발현빈도수를 별도의 데이터프레임 객체로 저장
txt_wd_fq_df <- data.frame(term = names(txt_wd_fq), freq = c(txt_wd_fq), 
                           row.names = NULL,
                           stringsAsFactors = FALSE)

# 단어철자를 기준으로 빈도수 정렬
txt_wd_fq_df %>% print

# 한글명사들을 발현 빈도수를 기준으로 내림차순 정렬
txt_wd_fq_df %>% dplyr::arrange(desc(freq), term) %>% print

# 발현빈도수가 현저하게 적은 한글명사단어 준비
stwd_lwfq <- txt_wd_fq_df %>% filter(freq <= 1) %>% select(term) %>% pull(term) %>% print
# - 분석대상 텍스트셋에 따라 기준으로 설정할 발현빈도수는 다를 수 있으며,
#   이번 분석에서는 임계치빈도수(freq) = 1이하로 설정함


# 5.7.3 발현빈도수 적은 한글명사 삭제 전처리
# --------------------------------------------------
# 한글명사단어 중 발현빈도수가 현저하게 적은 내용을 탐색하는 정규표현식 작성
stwd_lwfq_exp <- str_c('\\b', stwd_lwfq, '\\b') %>% print %>% str_c(collapse = '|') %>% print
# - 추출된 삭제대상 빈도수가 현저하게 작은 단어만을 단어경계로 설정한 다음에 
#   OR조건인 수직파이프라인(|)기호를 이용해서 하나하나 탐색할 수 있도록 정규표현식 작성

# 발현빈도가 적은 한글명사단어 직접 전처리방식으로 지우기
txt_one <- str_replace_all(txt_one, pattern = stwd_lwfq_exp, replacement = '') %>% print


# 5.7.4  발현빈도가 적은 단어를 삭제처리한 단일문자열을 명사형 단어단위로 추출(X) 분리(O)
# --------------------------------------------------
txt_wd_stwd <- str_extract_all(txt_one, pattern = boundary('word')) %>% print
# - KoNLP::extractNoun() 함수를 이용하면 단어추출이 잘된 경우와 불안정한 경우가 공존하므로
#   이 방식으로 다시 단어 추출/분리를 하면 안됨
# - stringr::str_extract_all()함수의 boundary('word')옵션을 통해서 
#   공백을 기준으로 단어단위로 직접분리하는 방법을 사용해아먄 
#   발현빈도를 기준으로 불필요 단어를 삭제처리한 한글단어들을 만날 수 있음


# --------------------------------------------------
# 5.8 직접 1:1 삭제방식으로 불필요 단어 제거
# --------------------------------------------------

# 5.8.1 한글명사 추출내용 재조회
# --------------------------------------------------
# 단일문자열 내용확인
txt_one 

# 앞서 발현빈도수를 기준으로 불용어를 파악해 전처리하여 원문순서대로 추출한 한글단어 
txt_wd_stwd


# 5.8.2 불필요 단어목록 파악
# --------------------------------------------------
# 단어철자를 기준으로 빈도수 파악
txt_wd_fq <- txt_wd_stwd[[1]] %>% table %>% print 


# 한글명사들의 발현빈도수를 별도의 데이터프레임 객체로 저장
txt_wd_fq_df <- data.frame(term = names(txt_wd_fq), freq = c(txt_wd_fq), 
                           row.names = NULL,
                           stringsAsFactors = FALSE)

# 단어철자를 기준으로 빈도수 정렬
txt_wd_fq_df %>% print

# 한글명사들을 발현 빈도수를 기준으로 내림차순 정렬
txt_wd_fq_df %>% dplyr::arrange(desc(freq), term) %>% print

# 직접 1:1로 삭제처리할 불필요 단어목록 선정
stwd_1to1 <- c('극복', '부여') %>% print


# 5.8.3 불필요 한글명사 삭제 전처리
# --------------------------------------------------
# 불필요 단어로 판단한 단어를 탐색하는 정규표현식 작성
stwd_1to1_exp <- str_c('\\b', stwd_1to1, '\\b') %>% print %>% str_c(collapse = '|') %>% print
# - 추출된 삭제대상 빈도수가 현저하게 작은 단어만을 단어경계로 설정한 다음에 
#   OR조건인 수직파이프라인(|)기호를 이용해서 하나하나 탐색할 수 있도록 정규표현식 작성

# 불필요 한글명사 직접 전처리방식으로 지우기
txt_one <- str_replace_all(txt_one, pattern = stwd_1to1_exp, replacement = '') %>% print


# 5.8.4  불필요 단어를 삭제처리한 단일문자열을 명사형 단어단위로 추출(X) 분리(O)
# --------------------------------------------------
txt_wd_stwd <- str_extract_all(txt_one, pattern = boundary('word')) %>% print
# - KoNLP::extractNoun() 함수를 이용하면 단어추출이 잘된 경우와 불안정한 경우가 공존하므로
#   이 방식으로 다시 단어 추출/분리를 하면 안됨
# - stringr::str_extract_all()함수의 boundary('word')옵션을 통해서 
#   공백을 기준으로 단어단위로 직접분리하는 방법을 사용해아먄 
#   불필요 단어를 1:1로 직접 삭제처리한 한글단어들을 만날 수 있음

# --------------------------------------------------
# 5.9 불용어 목록 파일저장
# --------------------------------------------------

# 5.9.1 불용어 목록 준비
# --------------------------------------------------
# 앞서 글자수를 기준으로 불용어로 판단한 한글단어 목록 재조회
stwd1_del # - 글자수1개 불용어
stwd2_del # - 글자수2개 불용어
stwd3_del # - 글자수3개 불용어
stwd_1to1 # - 1:1 직접삭제용 불용어
# - 글자수4개(stwd4)와 글자수5개이상(stwd5more) 인 경우에는 불용어가 없었음

# 불용어 목록 벡터객체로 결합
my_stwd <- c(stwd1_del, stwd2_del, stwd3_del, stwd_1to1) %>% print

# 메모리상의 벡터객체에 불용어 추가
my_stwd <- c(my_stwd, c('것', '어떤')) %>% print
# - 언제든지 판단에 따라 불용어를 추가할 수 있음

# 불용어 목록중 중복단어 제거한 고유단어로 정제
my_stwd %<>% unique %>% print


# 5.9.2 불용어 목록을 플레인텍스트 파일에 추가
# --------------------------------------------------
# 메모리상에 존재하는 불용어 목록을 플레인텍스트파일로 저장 
write.csv(my_stwd, file = './datatm/my_stop_word.csv', 
          row.names = FALSE, quote = FALSE)

# 플레인텍스트파일 직접오픈해서 추가된 신규단어 등록여부 확인
browseURL('./datatm/my_stop_word.csv')

# 플레인텍스트 파일에서 불용어 목록 재로딩
my_stwd_pool <- read_delim(file = './datatm/my_stop_word.csv', 
                           col_names = TRUE, delim = ',',
                           locale = locale(encoding = "EUC-KR")) %>% print

# 불용어 목록이 재로딩 되었음을 알 수 있음
# ==> 이 불용어 목록에 단어경계 정규표현식을 별도로 반영하여 전처리가 될 수 있도록 해야함


# 5.9.3 불용어 목록을 엑셀 파일에 추가
# --------------------------------------------------
# 신규 워크북(엑셀파일)객체를 메모리상에 생성
wb <- openxlsx::createWorkbook() %>% print

# 신규워크북에 불용어를 저장할 워크시트를 생성
openxlsx::addWorksheet(wb, sheetName = 'stopword')

# 메모리상의 벡터객체에 불용어 추가
my_stwd <- c(my_stwd, c('그', '그것', '그런', '어떤', '등')) %>% print
# - 언제든지 판단에 따라 불용어를 추가할 수 있음

# 불용어 목록중 중복단어 제거한 고유단어로 정제
my_stwd %<>% unique %>% print

# 워크시트에 메모리상의 불용어목록을 저장함
openxlsx::writeData(wb, sheet = 1, my_stwd)

# 메모리상의 엑셀파일객체를 작업경로에 물리적으로 저장
openxlsx::saveWorkbook(wb, './datatm/my_stop_word.xlsx', overwrite = TRUE)

# 엑셀파일 직접오픈해서 추가된 신규단어 등록여부 확인
r2excel::xlsx.openFile("./datatm/my_stop_word.xlsx")

# 엑셀파일에서 불용어 시트내용 재로딩
my_stwd_pool <- read_excel(path = './datatm/my_stop_word.xlsx', 
                           sheet = 'stopword', col_names = FALSE) %>% print

# 불용어 목록이 재로딩 되었음을 알 수 있음
# ==> 이 불용어 목록에 단어경계 정규표현식을 별도로 반영하여 전처리가 될 수 있도록 해야함


####################################################################################################
# 6. 어근동일화를 통한 전처리  
####################################################################################################

# --------------------------------------------------
# 6.1 어근동일화 데이터준비
# --------------------------------------------------

# 6.1.1 어근동일화 방향성
# --------------------------------------------------
# - 단어의미(어간)은 동일/유사한데 어미에서 다양한 변화가 주어지는 경우
#   일정한 패턴이 있는 동의/유사어들은 정규표현식을 통해서 일괄 특정단어로 동일화함
# - 다양한 철자로 인해서 일정한 패턴으로 검색하기 어려운 경우에는
#   직접 1:1변환방식으로 특정단어로 변경을 해나가면서 전처리 가능함 
# - 별도의 동의/유사어사전(소스코드상에 벡터객체로, 또는 csv/엑셀파일로)을 만들어 놓고 
#   해당하는 신규단어 하나하나를 등록해서 일괄 동일화함

# ==> 사실, 어근동일화를 적용하기전에 KoNLP::extractNoun()을 적용하면, 
#     명사형어간(ncn)중심으로 추출을 해주므로 어근동일화가 자동으로 적용된 것으로 보면됨


# 6.1.2 특정한 문서 1개를 별도로 로딩해 분석대상 문자열로 만듬
# --------------------------------------------------
# base::readLines()함수를 이용한 일반문자열 데이터로딩
raw <- readLines('./datatm/stem_rawtext.csv', encoding = 'UTF-8')
raw
# - csv/txt 파일이 데이터프레임 형식의 정형데이터 내용이 아니라 
#   일반 문자열 데이터가 들어 있는 경우 간단하게 로딩하는 방법

class(raw)
# - 이 객체는 주어진 한글텍스트셋 안에 있는 문단이나 문장 구조는 상관없이
#   하나의 문자열데이터로 고려하여 분석하는 것임

# --------------------------------------------------
# 6.2 어근동일화 후보파악
# --------------------------------------------------

# 6.2.1 텍스트raw데이터 단어단위 분석
# --------------------------------------------------
# 단어단위로 분리
raw_wd <- str_extract_all(raw, pattern = boundary('word')) %>% print

# 철자순서대로 빈도수 분석
raw_wd[[1]] %>% sort %>% table %>% print

# 어근동일화 필요성 단어
# - 커뮤니케이션과, 커뮤니케이션에, 커뮤니케이션을, 
#   커뮤니케이션의,  커뮤니케이션이란, 커뮤니케이션적 ==> 커뮤니케이션으로 통일
# - 포퓰리즘간의, 포퓰리즘과, 포퓰리즘에, 포퓰리즘이 ==> 포퓰리즘으로 통일
# - 현상과, 현상들을, 현상을, 현상의, 현상이 ==> 현상으로 통일
# - 관점에서, 관점을, 관점이 ==> 관점으로 통일

# --------------------------------------------------
# 6.3 어근동일화 대상단어 전처리
# --------------------------------------------------

# 6.3.1 언근동일화 대상단어 직접 전처리
# --------------------------------------------------
# 추출된 한글단어 단일 문자열로 변환
raw_one <- str_c(raw_wd[[1]], collapse = ' ')
raw_one

# 어근동일화 대상단어 목록 정의
stem_pool <- c('커뮤니케이션', '포퓰리즘', '현상', '관점')

# 어근동일화 대상단어와 해당표현들 조회
str_extract_all(raw_one, pattern = str_c('\\b', stem_pool, '[:alpha:]{1,}'))

# 어근동일화 대상단어 직접 전처리방식으로 정리
for (i in 1:length(stem_pool)) {
  stem_exp <- str_c('\\b', stem_pool[i], '[:alpha:]{1,}')
  raw_one <- str_replace_all(raw_one, pattern = stem_exp, replacement = stem_pool[i])
}
  
# 6.3.2 전처리된 단일 문자열을 명사형 단어단위로 추출(X) 분리(O)
# --------------------------------------------------
raw_wd_stem <- str_extract_all(raw_one, pattern = boundary('word')) %>% print
# - KoNLP::extractNoun() 함수를 이용하면, 신규단어가 제대로 추출되는 경우도 있고
#   기존대로 분리된 채로 추출되는 경우도 있으므로 이 방식을 사용하면 안됨
# - stringr::str_extract_all()함수의 boundary('word')옵션을 통해서 공백을 기준으로
#   단어단위로 직접분리하는 방법을 사용해 언근동일화를 적용한 전처리된 한글단어들을 만날 수 있음

raw_wd_stem[[1]] %>% table %>% sort
# - 커뮤니케이션, 포퓰리즘, 현상, 관점이라는 단어들이 모두 어근동일화처리가 적용되어
#   어미적 표현이 포함된 단어없이 순수한 어근/어간 단어로 변환되어 있음

# --------------------------------------------------
# 6.4 어근동일화 목록 파일저장
# --------------------------------------------------

# 6.4.1 어근동일화 대상단어 목록 준비
# --------------------------------------------------
# 앞서 어근동일화 단어로판단한 한글단어 목록 재조회
stem_pool 

# 메모리상의 벡터객체에 어근동일화 대상단어 추가
stem_pool <- c(stem_pool, c('감시', '연구')) %>% print

# 불용어 목록중 중복단어 제거한 고유단어로 정제
stem_pool %<>% unique %>% print

# 어근동일화 대상단어와 해당표현들 조회
str_extract_all(raw_one, pattern = str_c('\\b', stem_pool, '[:alpha:]{1,}'))

# 6.4.2 어근동일화 대상단어 플레인텍스트 파일에 추가
# --------------------------------------------------
# 메모리상에 존재하는 어근동일화 대상단어 목록을 플레인텍스트파일로 저장 
write.csv(stem_pool, file = './datatm/my_stem_word.csv', 
          row.names = FALSE, quote = FALSE)

# 플레인텍스트파일 직접오픈해서 추가된 어근동일화 대상단어 등록여부 확인
browseURL('./datatm/my_stem_word.csv')

# 플레인텍스트 파일에서 어근동일화 대상단어 재로딩
my_stem_pool <- read_delim(file = './datatm/my_stem_word.csv', 
                           col_names = TRUE, delim = ',',
                           locale = locale(encoding = "EUC-KR")) %>% print

# 어근동일화 대상단어 목록이 재로딩 되었음을 알 수 있음
# ==> 이 대상단어 목록에 단어경계 정규표현식을 별도로 반영하여 전처리가 될 수 있도록 해야함

# 6.4.3 어근동일화 대상단어 목록을 엑셀 파일에 추가
# --------------------------------------------------

# 신규 워크북(엑셀파일)객체를 메모리상에 생성
wb <- openxlsx::createWorkbook() %>% print

# 신규워크북에 불용어를 저장할 워크시트를 생성
openxlsx::addWorksheet(wb, sheetName = 'stemword')

# 메모리상의 벡터객체에 어근동일화 대상단어 추가
stem_pool <- c(stem_pool, c('관계', '분과')) %>% print

# 불용어 목록중 중복단어 제거한 고유단어로 정제
stem_pool %<>% unique %>% print

# 어근동일화 대상단어와 해당표현들 조회
str_extract_all(raw_one, pattern = str_c('\\b', stem_pool, '[:alpha:]{1,}'))

# 워크시트에 메모리상의 불용어목록을 저장함
openxlsx::writeData(wb, sheet = 1, stem_pool)

# 메모리상의 엑셀파일객체를 작업경로에 물리적으로 저장
openxlsx::saveWorkbook(wb, './datatm/my_stem_word.xlsx', overwrite = TRUE)

# 엑셀파일 직접오픈해서 추가된 신규단어 등록여부 확인
r2excel::xlsx.openFile("./datatm/my_stem_word.xlsx")

# 엑셀파일에서 신규단어 시트내용 재로딩
my_stem_pool <- read_excel(path = './datatm/my_stem_word.xlsx', 
                           sheet = 'stemword', col_names = FALSE) %>% print

# 어근동일화 대상단어 목록이 재로딩 되었음을 알 수 있음
# ==> 이 대상단어 목록에 단어경계 정규표현식을 별도로 반영하여 전처리가 될 수 있도록 해야함


### End of Documents ###############################################################################
