{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color = 'color'>[R텍마08] 2부4장_텍스트 데이터 전처리</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 필요 패키지 인스톨 및 라이브러리 로딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading required package: stringr\n",
      "Loading required package: magrittr\n",
      "Loading required package: purrr\n",
      "\n",
      "Attaching package: 'purrr'\n",
      "\n",
      "The following object is masked from 'package:magrittr':\n",
      "\n",
      "    set_names\n",
      "\n",
      "The following object is masked from 'package:jsonlite':\n",
      "\n",
      "    flatten\n",
      "\n",
      "Loading required package: tm\n",
      "Warning message:\n",
      "\"package 'tm' was built under R version 3.5.2\"Loading required package: NLP\n",
      "Warning message:\n",
      "\"package 'NLP' was built under R version 3.5.2\""
     ]
    },
    {
     "data": {
      "text/html": [
       "<dl class=dl-horizontal>\n",
       "\t<dt>stringr</dt>\n",
       "\t\t<dd>TRUE</dd>\n",
       "\t<dt>magrittr</dt>\n",
       "\t\t<dd>TRUE</dd>\n",
       "\t<dt>purrr</dt>\n",
       "\t\t<dd>TRUE</dd>\n",
       "\t<dt>tm</dt>\n",
       "\t\t<dd>TRUE</dd>\n",
       "</dl>\n"
      ],
      "text/latex": [
       "\\begin{description*}\n",
       "\\item[stringr] TRUE\n",
       "\\item[magrittr] TRUE\n",
       "\\item[purrr] TRUE\n",
       "\\item[tm] TRUE\n",
       "\\end{description*}\n"
      ],
      "text/markdown": [
       "stringr\n",
       ":   TRUEmagrittr\n",
       ":   TRUEpurrr\n",
       ":   TRUEtm\n",
       ":   TRUE\n",
       "\n"
      ],
      "text/plain": [
       " stringr magrittr    purrr       tm \n",
       "    TRUE     TRUE     TRUE     TRUE "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 필요한 패키지 목록 생성\n",
    "pkg <- c('stringr', 'magrittr', 'purrr', 'tm')\n",
    "\n",
    "# 필요패키지 설지여부를 체크해 미설치패키지 목록을 저장\n",
    "new_pkg <- pkg[!(pkg %in% rownames(installed.packages()))]\n",
    "\n",
    "# 미설치 패키지 목록이 1개라도 있으면, 일괄 인스톨 실시\n",
    "if (length(new_pkg)) install.packages(new_pkg, dependencies = TRUE, repos=\"http://R-Forge.R-project.org\")\n",
    "\n",
    "# 필요패키지를 일괄 로딩실시\n",
    "sapply(pkg, require, character.only = TRUE)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color = 'blue'>공란 처리(stripping white space) (p.97)</font>\n",
    "* 공란(white space)은 단어와 단어를 구분하는 기능을 수행\n",
    "* 문자 입력의 실수, 또는 온라인 텍스트 수집과정의 오류로 공란이 2개 이상 발견되는데, 이를 1개로 축약시키는 과정임"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 간단 텍스트셋 준비\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "'검색대상 객체내용:'"
      ],
      "text/latex": [
       "'검색대상 객체내용:'"
      ],
      "text/markdown": [
       "'검색대상 객체내용:'"
      ],
      "text/plain": [
       "[1] \"검색대상 객체내용:\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"software environment\"  \"software  environment\" \"softwear\\tenvironment\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "'검색대상 객체유형: character객체'"
      ],
      "text/latex": [
       "'검색대상 객체유형: character객체'"
      ],
      "text/markdown": [
       "'검색대상 객체유형: character객체'"
      ],
      "text/plain": [
       "[1] \"검색대상 객체유형: character객체\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "'검색대상 객체길이: 3요소'"
      ],
      "text/latex": [
       "'검색대상 객체길이: 3요소'"
      ],
      "text/markdown": [
       "'검색대상 객체길이: 3요소'"
      ],
      "text/plain": [
       "[1] \"검색대상 객체길이: 3요소\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "'검색대상 객체글자수:'"
      ],
      "text/latex": [
       "'검색대상 객체글자수:'"
      ],
      "text/markdown": [
       "'검색대상 객체글자수:'"
      ],
      "text/plain": [
       "[1] \"검색대상 객체글자수:\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] 20 21 20\n"
     ]
    }
   ],
   "source": [
    "# 간단 텍스트셋 준비\n",
    "my <- c('software environment', 'software  environment', 'softwear\tenvironment')\n",
    "# - my <- c('software environment', 'software  environment', 'softwear\\tenvironment')\n",
    "# - 실제 텍스트셋 내용 중 탭(tab)키가 들어 있는 경우 주피터노트북상에서 \t 기호로 입력됨\n",
    "# - 이 탭(tab)키를 의미하는 \t 기호를 확장문자열(탈출문자) 방식인 \\t로 직접 표시해도 됨\n",
    "\n",
    "sprintf('검색대상 객체내용:')\n",
    "print(my)\n",
    "\n",
    "sprintf('검색대상 객체유형: %s객체', class(my))\n",
    "sprintf('검색대상 객체길이: %d요소', length(my))\n",
    "\n",
    "sprintf('검색대상 객체글자수:')\n",
    "nchar(my) %>% print\n",
    "# - sprintf('검색대상 객체글자수: %d글자', str_length(my))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 검색패턴으로 분해문자 없이 분해하는 경우"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 검색패턴 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 아무런 분해문자가 없는 상태로 검색패턴 설정\n",
    "src = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 검색패턴을 이용한 텍스트셋 분해"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1]]\n",
      " [1] \"s\" \"o\" \"f\" \"t\" \"w\" \"a\" \"r\" \"e\" \" \" \"e\" \"n\" \"v\" \"i\" \"r\" \"o\" \"n\" \"m\" \"e\" \"n\"\n",
      "[20] \"t\"\n",
      "\n",
      "[[2]]\n",
      " [1] \"s\" \"o\" \"f\" \"t\" \"w\" \"a\" \"r\" \"e\" \" \" \" \" \"e\" \"n\" \"v\" \"i\" \"r\" \"o\" \"n\" \"m\" \"e\"\n",
      "[20] \"n\" \"t\"\n",
      "\n",
      "[[3]]\n",
      " [1] \"s\"  \"o\"  \"f\"  \"t\"  \"w\"  \"e\"  \"a\"  \"r\"  \"\\t\" \"e\"  \"n\"  \"v\"  \"i\"  \"r\"  \"o\" \n",
      "[16] \"n\"  \"m\"  \"e\"  \"n\"  \"t\" \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 아무런 분해문자가 없는 상태로 분해 \n",
    "str_split(string = my, pattern = src) %>% print\n",
    "\n",
    "# - 분해된 결과가 각 단어별 철자(letter) 단위로 분해되어 원하는 결과가 아님"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 검색패턴으로 단순 공란 1칸을 설정해 분해하는 경우"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 검색패턴 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 공란 한 칸을 단순히 검색패턴으로 설정\n",
    "src = ' '"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 검색패턴을 이용한 텍스트셋 탐색"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "'원본문장:'"
      ],
      "text/latex": [
       "'원본문장:'"
      ],
      "text/markdown": [
       "'원본문장:'"
      ],
      "text/plain": [
       "[1] \"원본문장:\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"software environment\"  \"software  environment\" \"softwear\\tenvironment\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style=white-space:pre-wrap>'정규표현식 검색패턴:  '</span>"
      ],
      "text/latex": [
       "'정규표현식 검색패턴:  '"
      ],
      "text/markdown": [
       "<span style=white-space:pre-wrap>'정규표현식 검색패턴:  '</span>"
      ],
      "text/plain": [
       "[1] \"정규표현식 검색패턴:  \""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#----------------------------------------"
     ]
    },
    {
     "data": {
      "text/html": [
       "'검색패턴 포함여부를 요소별 논리값으로:'"
      ],
      "text/latex": [
       "'검색패턴 포함여부를 요소별 논리값으로:'"
      ],
      "text/markdown": [
       "'검색패턴 포함여부를 요소별 논리값으로:'"
      ],
      "text/plain": [
       "[1] \"검색패턴 포함여부를 요소별 논리값으로:\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]  TRUE  TRUE FALSE\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "'검색패턴 포함여부를 요소별 인덱스번호로:'"
      ],
      "text/latex": [
       "'검색패턴 포함여부를 요소별 인덱스번호로:'"
      ],
      "text/markdown": [
       "'검색패턴 포함여부를 요소별 인덱스번호로:'"
      ],
      "text/plain": [
       "[1] \"검색패턴 포함여부를 요소별 인덱스번호로:\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] 1 2\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "'검색패턴 포함여부를 요소별 내용으로:'"
      ],
      "text/latex": [
       "'검색패턴 포함여부를 요소별 내용으로:'"
      ],
      "text/markdown": [
       "'검색패턴 포함여부를 요소별 내용으로:'"
      ],
      "text/plain": [
       "[1] \"검색패턴 포함여부를 요소별 내용으로:\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"software environment\"  \"software  environment\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "'검색패턴 포함여부를 요소별 출현위치로:'"
      ],
      "text/latex": [
       "'검색패턴 포함여부를 요소별 출현위치로:'"
      ],
      "text/markdown": [
       "'검색패턴 포함여부를 요소별 출현위치로:'"
      ],
      "text/plain": [
       "[1] \"검색패턴 포함여부를 요소별 출현위치로:\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1]]\n",
      "     start end\n",
      "[1,]     9   9\n",
      "\n",
      "[[2]]\n",
      "     start end\n",
      "[1,]     9   9\n",
      "[2,]    10  10\n",
      "\n",
      "[[3]]\n",
      "     start end\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "'검색패턴 포함여부를 요소별 출현내용으로:'"
      ],
      "text/latex": [
       "'검색패턴 포함여부를 요소별 출현내용으로:'"
      ],
      "text/markdown": [
       "'검색패턴 포함여부를 요소별 출현내용으로:'"
      ],
      "text/plain": [
       "[1] \"검색패턴 포함여부를 요소별 출현내용으로:\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1]]\n",
      "[1] \" \"\n",
      "\n",
      "[[2]]\n",
      "[1] \" \" \" \"\n",
      "\n",
      "[[3]]\n",
      "character(0)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "'검색패턴 포함여부를 요소별 출현횟수로:'"
      ],
      "text/latex": [
       "'검색패턴 포함여부를 요소별 출현횟수로:'"
      ],
      "text/markdown": [
       "'검색패턴 포함여부를 요소별 출현횟수로:'"
      ],
      "text/plain": [
       "[1] \"검색패턴 포함여부를 요소별 출현횟수로:\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] 1 2 0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "'검색패턴 포함여부를 요소별 기술통계로:'"
      ],
      "text/latex": [
       "'검색패턴 포함여부를 요소별 기술통계로:'"
      ],
      "text/markdown": [
       "'검색패턴 포함여부를 요소별 기술통계로:'"
      ],
      "text/plain": [
       "[1] \"검색패턴 포함여부를 요소별 기술통계로:\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1]]\n",
      "  \n",
      "1 \n",
      "\n",
      "[[2]]\n",
      "  \n",
      "2 \n",
      "\n",
      "[[3]]\n",
      "integer(0)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 해당 검색패턴으로 탐색\n",
    "\n",
    "sprintf('원본문장:')\n",
    "print(my)\n",
    "sprintf('정규표현식 검색패턴: %s', src)\n",
    "cat('#----------------------------------------')\n",
    "\n",
    "sprintf('검색패턴 포함여부를 요소별 논리값으로:')\n",
    "str_detect(string = my, pattern = src) %>% print \n",
    "# - grepl(x = my, pattern = src) %>% print 동일결과\n",
    "# - 검색패턴이 들어 있으면 TRUE, 없으면 FALSE 출력\n",
    "\n",
    "sprintf('검색패턴 포함여부를 요소별 인덱스번호로:')\n",
    "str_which(string = my, pattern = src) %>% print\n",
    "# - grep(x = my, pattern = src) %>% print 동일결과\n",
    "# - 검색패턴이 들어 있으면 해당 요소의 인덱스번호 출력, 없으면 해당 요소 인덱스번호 미출력\n",
    "\n",
    "sprintf('검색패턴 포함여부를 요소별 내용으로:')\n",
    "str_subset(string = my, pattern = src) %>% print \n",
    "# - grep(x = my, pattern = src , value = TRUE) %>% print 동일결과\n",
    "# - 검색패턴이 들어 있으면 해당 요소의 내용 출력, 없으면 해당 요소의 내용 미출력\n",
    "\n",
    "sprintf('검색패턴 포함여부를 요소별 출현위치로:')\n",
    "str_locate_all(string = my, pattern = src) %>% print\n",
    "# - gregexpr(text = my, pattern = src) %>% print 동일결과\n",
    "# - 검색패턴이 해당 텍스트요소에 들어 있지 않으면 base::gregexpr()은 -1을, stringr::str_locate_all()은 NA를 출력함\n",
    "\n",
    "sprintf('검색패턴 포함여부를 요소별 출현내용으로:')\n",
    "str_extract_all(string = my, pattern = src) %>% print\n",
    "# - gregexpr(text = my, pattern = src) %>% regmatches(x = my) %>% print 동일결과\n",
    "# - 검색패턴이 해당 텍스트요소에 들어 있지 않으면 character(0)을 출력함\n",
    "\n",
    "sprintf('검색패턴 포함여부를 요소별 출현횟수로:')\n",
    "str_count(string = my, pattern = src) %>% print\n",
    "# - gregexpr(text = my, pattern = src) %>% regmatches(x = my) %>% lengths %>% print 동일결과\n",
    "# - 검색패턴이 해당 텍스트요소에 들어 있는 갯수를 카운팅해 출력함\n",
    "\n",
    "sprintf('검색패턴 포함여부를 요소별 기술통계로:')\n",
    "str_extract_all(string = my, pattern = src) %>% \n",
    "    map(table) %>% map(sort, decreasing = TRUE) %>% print\n",
    "\n",
    "# 검색패턴 기준으로 탐색결과\n",
    "# - 첫번째 문자열에서는 공백 1칸이 파악되어, 이를 기준으로 단어 2개로 정상분해 가능성 있음\n",
    "# - 두번째 문자열에서는 공백이 연속으로 2칸 파악되어, 공백 1칸이 분해결과로 남게되는 문제점 있음\n",
    "# - 세번째 문자열에서는 탭(tab)키를 의미하는 \\t 표시가 공백 1칸으로 매칭되지 못해서 분해자체가 안될 문제점 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 검색패턴을 이용한 텍스트셋 분해"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "'원본문장:'"
      ],
      "text/latex": [
       "'원본문장:'"
      ],
      "text/markdown": [
       "'원본문장:'"
      ],
      "text/plain": [
       "[1] \"원본문장:\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"software environment\"  \"software  environment\" \"softwear\\tenvironment\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style=white-space:pre-wrap>'정규표현식 검색패턴:  '</span>"
      ],
      "text/latex": [
       "'정규표현식 검색패턴:  '"
      ],
      "text/markdown": [
       "<span style=white-space:pre-wrap>'정규표현식 검색패턴:  '</span>"
      ],
      "text/plain": [
       "[1] \"정규표현식 검색패턴:  \""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#----------------------------------------"
     ]
    },
    {
     "data": {
      "text/html": [
       "'검색패턴을 기준으로 요소별로 분해:'"
      ],
      "text/latex": [
       "'검색패턴을 기준으로 요소별로 분해:'"
      ],
      "text/markdown": [
       "'검색패턴을 기준으로 요소별로 분해:'"
      ],
      "text/plain": [
       "[1] \"검색패턴을 기준으로 요소별로 분해:\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1]]\n",
      "[1] \"software\"    \"environment\"\n",
      "\n",
      "[[2]]\n",
      "[1] \"software\"    \"\"            \"environment\"\n",
      "\n",
      "[[3]]\n",
      "[1] \"softwear\\tenvironment\"\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "'요소별로 분해된 세부요소 갯수:'"
      ],
      "text/latex": [
       "'요소별로 분해된 세부요소 갯수:'"
      ],
      "text/markdown": [
       "'요소별로 분해된 세부요소 갯수:'"
      ],
      "text/plain": [
       "[1] \"요소별로 분해된 세부요소 갯수:\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1]]\n",
      "[1] 2\n",
      "\n",
      "[[2]]\n",
      "[1] 3\n",
      "\n",
      "[[3]]\n",
      "[1] 1\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "'요소별로 분해된 세부요소들의 글자 갯수:'"
      ],
      "text/latex": [
       "'요소별로 분해된 세부요소들의 글자 갯수:'"
      ],
      "text/markdown": [
       "'요소별로 분해된 세부요소들의 글자 갯수:'"
      ],
      "text/plain": [
       "[1] \"요소별로 분해된 세부요소들의 글자 갯수:\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1]]\n",
      "[1]  8 11\n",
      "\n",
      "[[2]]\n",
      "[1]  8  0 11\n",
      "\n",
      "[[3]]\n",
      "[1] 20\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 해당 검색패턴으로 분해\n",
    "\n",
    "sprintf('원본문장:')\n",
    "print(my)\n",
    "sprintf('정규표현식 검색패턴: %s', src)\n",
    "cat('#----------------------------------------')\n",
    "\n",
    "sprintf('검색패턴을 기준으로 요소별로 분해:')\n",
    "str_split(string = my, pattern = src) %>% print\n",
    "# - strsplit(x = my, split = src) 동일결과\n",
    "\n",
    "sprintf('요소별로 분해된 세부요소 갯수:')\n",
    "str_split(string = my, pattern = src) %>% map(length) %>% print\n",
    "\n",
    "sprintf('요소별로 분해된 세부요소들의 글자 갯수:')\n",
    "str_split(string = my, pattern = src) %>% map(str_length) %>% print\n",
    "\n",
    "# 검색패턴 기준으로 분해결과\n",
    "# - 첫번째 문자열에서는 공백 1칸을 기준으로 단어 2개로 정상분해 되었음\n",
    "# - 두번째 문자열에서는 공백이 연속으로 2칸 파악되어 이를 기준으로 인접한 단어들이 각가 분해되었으나,\n",
    "#   공백 2칸도 분해되어 공백 1칸이 분해결과로 남게되는 문제점 발생\n",
    "# - 세번째 문자열에서는 탭(tab)키를 의미하는 \\t 표시가 공백 1칸으로 매칭되지 못해서 분해자체가 안된 문제점 발생"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 스페이스 문자를 한 칸의 공란으로 전처리해서 분해\n",
    "\n",
    "* 텍스트셋에 제어, 간격, 공백 문자 포함여부 검색\n",
    "<br><font color = 'red'>[:blank:]</font> --> 간격문자 - 스페이스바(spacebar) 또는 탭키(Tab) 등 --> pattern = [[:blank:]]\n",
    "<br><font color = 'red'>[:cntrl:]</font> --> 제어문자 - 캐리지리턴(CR), 라인피드(LF) --> pattern = [[:cntrl:]]\n",
    "    <br><font color = 'red'>[:space:]</font> --> 공백문자 - 스페이스바(spacebar), 탭키(Tab), 캐리지리턴(CR), 라인피드(LF), 라인끝(EOL) 등 --> pattern = [[:space:]] \n",
    "<br>- CR(Carraiage Return): (\\r)는 커서를 다음 줄로 진행하지 않고 행의 시작 부분으로 이동 \n",
    "<br>- LF(Line Feed): (\\n)는 행의 시작 부분으로 돌아 가지 않고 커서를 다음 행으로 이동 \n",
    "<br>- EOL(End of Line): (\\r\\n)는 두개의 ASCII 문자이며 CR과 LF 의 조합"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 검색패턴 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 원본패턴\n",
    "src = '[[:space:]]{1,}'\n",
    "# - 스페이스 공백문자에 해당하는 요소가 1개 이상 들어 있는 패턴을 모두 찾도록 설정\n",
    "\n",
    "# 목표패턴\n",
    "tgt = ' '\n",
    "# - 스페이스 공백문자 모두를 1칸짜리 공란으로 변경할 수 있도록 설정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 검색패턴을 이용한 텍스트셋 탐색"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "'원본문장:'"
      ],
      "text/latex": [
       "'원본문장:'"
      ],
      "text/markdown": [
       "'원본문장:'"
      ],
      "text/plain": [
       "[1] \"원본문장:\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"software environment\"  \"software  environment\" \"softwear\\tenvironment\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "'정규표현식 검색패턴: [[:space:]]{1,}'"
      ],
      "text/latex": [
       "'정규표현식 검색패턴: {[}{[}:space:{]}{]}\\{1,\\}'"
      ],
      "text/markdown": [
       "'정규표현식 검색패턴: [[:space:]]{1,}'"
      ],
      "text/plain": [
       "[1] \"정규표현식 검색패턴: [[:space:]]{1,}\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#----------------------------------------"
     ]
    },
    {
     "data": {
      "text/html": [
       "'검색패턴 포함여부를 요소별 논리값으로:'"
      ],
      "text/latex": [
       "'검색패턴 포함여부를 요소별 논리값으로:'"
      ],
      "text/markdown": [
       "'검색패턴 포함여부를 요소별 논리값으로:'"
      ],
      "text/plain": [
       "[1] \"검색패턴 포함여부를 요소별 논리값으로:\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] TRUE TRUE TRUE\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "'검색패턴 포함여부를 요소별 인덱스번호로:'"
      ],
      "text/latex": [
       "'검색패턴 포함여부를 요소별 인덱스번호로:'"
      ],
      "text/markdown": [
       "'검색패턴 포함여부를 요소별 인덱스번호로:'"
      ],
      "text/plain": [
       "[1] \"검색패턴 포함여부를 요소별 인덱스번호로:\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] 1 2 3\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "'검색패턴 포함여부를 요소별 내용으로:'"
      ],
      "text/latex": [
       "'검색패턴 포함여부를 요소별 내용으로:'"
      ],
      "text/markdown": [
       "'검색패턴 포함여부를 요소별 내용으로:'"
      ],
      "text/plain": [
       "[1] \"검색패턴 포함여부를 요소별 내용으로:\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"software environment\"  \"software  environment\" \"softwear\\tenvironment\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "'검색패턴 포함여부를 요소별 출현위치로:'"
      ],
      "text/latex": [
       "'검색패턴 포함여부를 요소별 출현위치로:'"
      ],
      "text/markdown": [
       "'검색패턴 포함여부를 요소별 출현위치로:'"
      ],
      "text/plain": [
       "[1] \"검색패턴 포함여부를 요소별 출현위치로:\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1]]\n",
      "     start end\n",
      "[1,]     9   9\n",
      "\n",
      "[[2]]\n",
      "     start end\n",
      "[1,]     9  10\n",
      "\n",
      "[[3]]\n",
      "     start end\n",
      "[1,]     9   9\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "'검색패턴 포함여부를 요소별 출현내용으로:'"
      ],
      "text/latex": [
       "'검색패턴 포함여부를 요소별 출현내용으로:'"
      ],
      "text/markdown": [
       "'검색패턴 포함여부를 요소별 출현내용으로:'"
      ],
      "text/plain": [
       "[1] \"검색패턴 포함여부를 요소별 출현내용으로:\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1]]\n",
      "[1] \" \"\n",
      "\n",
      "[[2]]\n",
      "[1] \"  \"\n",
      "\n",
      "[[3]]\n",
      "[1] \"\\t\"\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "'검색패턴 포함여부를 요소별 출현횟수로:'"
      ],
      "text/latex": [
       "'검색패턴 포함여부를 요소별 출현횟수로:'"
      ],
      "text/markdown": [
       "'검색패턴 포함여부를 요소별 출현횟수로:'"
      ],
      "text/plain": [
       "[1] \"검색패턴 포함여부를 요소별 출현횟수로:\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] 1 1 1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "'검색패턴 포함여부를 요소별 기술통계로:'"
      ],
      "text/latex": [
       "'검색패턴 포함여부를 요소별 기술통계로:'"
      ],
      "text/markdown": [
       "'검색패턴 포함여부를 요소별 기술통계로:'"
      ],
      "text/plain": [
       "[1] \"검색패턴 포함여부를 요소별 기술통계로:\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1]]\n",
      "  \n",
      "1 \n",
      "\n",
      "[[2]]\n",
      "   \n",
      " 1 \n",
      "\n",
      "[[3]]\n",
      "\\t \n",
      " 1 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 해당 검색패턴으로 탐색\n",
    "\n",
    "sprintf('원본문장:')\n",
    "print(my)\n",
    "sprintf('정규표현식 검색패턴: %s', src)\n",
    "cat('#----------------------------------------')\n",
    "\n",
    "sprintf('검색패턴 포함여부를 요소별 논리값으로:')\n",
    "str_detect(string = my, pattern = src) %>% print \n",
    "# - grepl(x = my, pattern = src) %>% print 동일결과\n",
    "# - 검색패턴이 들어 있으면 TRUE, 없으면 FALSE 출력\n",
    "\n",
    "sprintf('검색패턴 포함여부를 요소별 인덱스번호로:')\n",
    "str_which(string = my, pattern = src) %>% print\n",
    "# - grep(x = my, pattern = src) %>% print 동일결과\n",
    "# - 검색패턴이 들어 있으면 해당 요소의 인덱스번호 출력, 없으면 해당 요소 인덱스번호 미출력\n",
    "\n",
    "sprintf('검색패턴 포함여부를 요소별 내용으로:')\n",
    "str_subset(string = my, pattern = src) %>% print \n",
    "# - grep(x = my, pattern = src , value = TRUE) %>% print 동일결과\n",
    "# - 검색패턴이 들어 있으면 해당 요소의 내용 출력, 없으면 해당 요소의 내용 미출력\n",
    "\n",
    "sprintf('검색패턴 포함여부를 요소별 출현위치로:')\n",
    "str_locate_all(string = my, pattern = src) %>% print\n",
    "# - gregexpr(text = my, pattern = src) %>% print 동일결과\n",
    "# - 검색패턴이 해당 텍스트요소에 들어 있지 않으면 base::gregexpr()은 -1을, stringr::str_locate_all()은 NA를 출력함\n",
    "\n",
    "sprintf('검색패턴 포함여부를 요소별 출현내용으로:')\n",
    "str_extract_all(string = my, pattern = src) %>% print\n",
    "# - gregexpr(text = my, pattern = src) %>% regmatches(x = my) %>% print 동일결과\n",
    "# - 검색패턴이 해당 텍스트요소에 들어 있지 않으면 character(0)을 출력함\n",
    "\n",
    "sprintf('검색패턴 포함여부를 요소별 출현횟수로:')\n",
    "str_count(string = my, pattern = src) %>% print\n",
    "# - gregexpr(text = my, pattern = src) %>% regmatches(x = my) %>% lengths %>% print 동일결과\n",
    "# - 검색패턴이 해당 텍스트요소에 들어 있는 갯수를 카운팅해 출력함\n",
    "\n",
    "sprintf('검색패턴 포함여부를 요소별 기술통계로:')\n",
    "str_extract_all(string = my, pattern = src) %>% \n",
    "    map(table) %>% map(sort, decreasing = TRUE) %>% print\n",
    "\n",
    "# 검색패턴 기준으로 탐색결과\n",
    "# - 첫번째 문자열에서는 공백 1칸이 파악되어, 이를 그대로 공백 1칸으로 변경할 예정\n",
    "# - 두번째 문자열에서는 공백이 연속으로 2칸 파악되어, 이를 공백 1칸으로 변경할 예정\n",
    "# - 세번째 문자열에서는 탭(tab)키를 의미하는 \\t 표시가 파악되어, 이를 공백 1칸으로 변경할 예정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 검색패턴을 이용한 텍스트셋 목표패턴으로 변경"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "'원본문장:'"
      ],
      "text/latex": [
       "'원본문장:'"
      ],
      "text/markdown": [
       "'원본문장:'"
      ],
      "text/plain": [
       "[1] \"원본문장:\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"software environment\"  \"software  environment\" \"softwear\\tenvironment\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "'정규표현식 검색패턴: [[:space:]]{1,}'"
      ],
      "text/latex": [
       "'정규표현식 검색패턴: {[}{[}:space:{]}{]}\\{1,\\}'"
      ],
      "text/markdown": [
       "'정규표현식 검색패턴: [[:space:]]{1,}'"
      ],
      "text/plain": [
       "[1] \"정규표현식 검색패턴: [[:space:]]{1,}\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=white-space:pre-wrap>'정규표현식 목표패턴:  '</span>"
      ],
      "text/latex": [
       "'정규표현식 목표패턴:  '"
      ],
      "text/markdown": [
       "<span style=white-space:pre-wrap>'정규표현식 목표패턴:  '</span>"
      ],
      "text/plain": [
       "[1] \"정규표현식 목표패턴:  \""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#----------------------------------------"
     ]
    },
    {
     "data": {
      "text/html": [
       "'검색패턴을 기준으로 요소별로 원본패턴을 목표패턴으로 변경:'"
      ],
      "text/latex": [
       "'검색패턴을 기준으로 요소별로 원본패턴을 목표패턴으로 변경:'"
      ],
      "text/markdown": [
       "'검색패턴을 기준으로 요소별로 원본패턴을 목표패턴으로 변경:'"
      ],
      "text/plain": [
       "[1] \"검색패턴을 기준으로 요소별로 원본패턴을 목표패턴으로 변경:\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "'전처리된 객체내용:'"
      ],
      "text/latex": [
       "'전처리된 객체내용:'"
      ],
      "text/markdown": [
       "'전처리된 객체내용:'"
      ],
      "text/plain": [
       "[1] \"전처리된 객체내용:\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"software environment\" \"software environment\" \"softwear environment\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "'전처리된 객체유형: character객체'"
      ],
      "text/latex": [
       "'전처리된 객체유형: character객체'"
      ],
      "text/markdown": [
       "'전처리된 객체유형: character객체'"
      ],
      "text/plain": [
       "[1] \"전처리된 객체유형: character객체\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "'전처리된 객체길이: 3요소'"
      ],
      "text/latex": [
       "'전처리된 객체길이: 3요소'"
      ],
      "text/markdown": [
       "'전처리된 객체길이: 3요소'"
      ],
      "text/plain": [
       "[1] \"전처리된 객체길이: 3요소\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "'전처리된 객체글자수:'"
      ],
      "text/latex": [
       "'전처리된 객체글자수:'"
      ],
      "text/markdown": [
       "'전처리된 객체글자수:'"
      ],
      "text/plain": [
       "[1] \"전처리된 객체글자수:\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] 20 20 20\n"
     ]
    }
   ],
   "source": [
    "# 해당 검색패턴을 목표패턴으로 변경\n",
    "\n",
    "sprintf('원본문장:')\n",
    "print(my)\n",
    "sprintf('정규표현식 검색패턴: %s', src)\n",
    "sprintf('정규표현식 목표패턴: %s', tgt)\n",
    "cat('#----------------------------------------')\n",
    "\n",
    "sprintf('검색패턴을 기준으로 요소별로 원본패턴을 목표패턴으로 변경:')\n",
    "my_rpc <- str_replace_all(string = my, pattern = src, replacement = tgt)\n",
    "# - gsub(x = my, pattern = src, replacement = tgt) 동일결과\n",
    "\n",
    "sprintf('전처리된 객체내용:')\n",
    "print(my_rpc)\n",
    "\n",
    "sprintf('전처리된 객체유형: %s객체', class(my_rpc))\n",
    "sprintf('전처리된 객체길이: %d요소', length(my_rpc))\n",
    "\n",
    "sprintf('전처리된 객체글자수:')\n",
    "nchar(my_rpc) %>% print\n",
    "# - str_length(my_rpc) %>% print 동일결과\n",
    "\n",
    "# 검색패턴을 목표패턴으로 변경결과\n",
    "# - 첫번째 문자열에서는 공백 1칸을 그대로 공백 1칸으로 변경함\n",
    "# - 두번째 문자열에서는 연속적인 공백 2칸을 공백 1칸으로 변경함\n",
    "# - 세번째 문자열에서는 탭(tab)키를 의미하는 \\t 표시를 공백 1칸으로 변경함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 목표패턴을 이용한 텍스트셋 분해"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "'전처리된 문장:'"
      ],
      "text/latex": [
       "'전처리된 문장:'"
      ],
      "text/markdown": [
       "'전처리된 문장:'"
      ],
      "text/plain": [
       "[1] \"전처리된 문장:\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"software environment\" \"software environment\" \"softwear environment\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style=white-space:pre-wrap>'정규표현식 검색패턴:  '</span>"
      ],
      "text/latex": [
       "'정규표현식 검색패턴:  '"
      ],
      "text/markdown": [
       "<span style=white-space:pre-wrap>'정규표현식 검색패턴:  '</span>"
      ],
      "text/plain": [
       "[1] \"정규표현식 검색패턴:  \""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#----------------------------------------"
     ]
    },
    {
     "data": {
      "text/html": [
       "'검색패턴을 기준으로 요소별로 분해:'"
      ],
      "text/latex": [
       "'검색패턴을 기준으로 요소별로 분해:'"
      ],
      "text/markdown": [
       "'검색패턴을 기준으로 요소별로 분해:'"
      ],
      "text/plain": [
       "[1] \"검색패턴을 기준으로 요소별로 분해:\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1]]\n",
      "[1] \"software\"    \"environment\"\n",
      "\n",
      "[[2]]\n",
      "[1] \"software\"    \"environment\"\n",
      "\n",
      "[[3]]\n",
      "[1] \"softwear\"    \"environment\"\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "'요소별로 분해된 세부요소 갯수:'"
      ],
      "text/latex": [
       "'요소별로 분해된 세부요소 갯수:'"
      ],
      "text/markdown": [
       "'요소별로 분해된 세부요소 갯수:'"
      ],
      "text/plain": [
       "[1] \"요소별로 분해된 세부요소 갯수:\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1]]\n",
      "[1] 2\n",
      "\n",
      "[[2]]\n",
      "[1] 2\n",
      "\n",
      "[[3]]\n",
      "[1] 2\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "'요소별로 분해된 세부요소들의 글자 갯수:'"
      ],
      "text/latex": [
       "'요소별로 분해된 세부요소들의 글자 갯수:'"
      ],
      "text/markdown": [
       "'요소별로 분해된 세부요소들의 글자 갯수:'"
      ],
      "text/plain": [
       "[1] \"요소별로 분해된 세부요소들의 글자 갯수:\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1]]\n",
      "[1]  8 11\n",
      "\n",
      "[[2]]\n",
      "[1]  8 11\n",
      "\n",
      "[[3]]\n",
      "[1]  8 11\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 해당 목표패턴으로 분해\n",
    "\n",
    "sprintf('전처리된 문장:')\n",
    "print(my_rpc)\n",
    "sprintf('정규표현식 검색패턴: %s', tgt)\n",
    "cat('#----------------------------------------')\n",
    "\n",
    "sprintf('검색패턴을 기준으로 요소별로 분해:')\n",
    "str_split(string = my_rpc, pattern = tgt) %>% print\n",
    "\n",
    "sprintf('요소별로 분해된 세부요소 갯수:')\n",
    "str_split(string = my_rpc, pattern = tgt) %>% map(length) %>% print\n",
    "\n",
    "sprintf('요소별로 분해된 세부요소들의 글자 갯수:')\n",
    "str_split(string = my_rpc, pattern = tgt) %>% map(str_length) %>% print\n",
    "\n",
    "# 검색패턴 기준으로 분해결과\n",
    "# - 3개 문자열 모두 공백 1칸을 기준으로 단어 2개로 정상분해 되었음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color = 'blue'>대소문자 통일(Case-insensitive) (p.100)</font>\n",
    "* 영문 텍스트셋은 대소문자를 구별해서 사용하므로, 이에 대한 적절한 처리가 필요함\n",
    "* 일률적인 소문자로 통일하는 방법 이외에, 여건에 따라 특정한 문구는 대문자나 대소문자 혼용을 유지해야하는 경우도 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 간단 텍스트셋 준비\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "'검색대상 객체내용:'"
      ],
      "text/latex": [
       "'검색대상 객체내용:'"
      ],
      "text/markdown": [
       "'검색대상 객체내용:'"
      ],
      "text/plain": [
       "[1] \"검색대상 객체내용:\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"The 45th President of the United States, Donald Trump, states that he knows how to play trump with the former president\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "'검색대상 객체유형: character객체'"
      ],
      "text/latex": [
       "'검색대상 객체유형: character객체'"
      ],
      "text/markdown": [
       "'검색대상 객체유형: character객체'"
      ],
      "text/plain": [
       "[1] \"검색대상 객체유형: character객체\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "'검색대상 객체길이: 1요소'"
      ],
      "text/latex": [
       "'검색대상 객체길이: 1요소'"
      ],
      "text/markdown": [
       "'검색대상 객체길이: 1요소'"
      ],
      "text/plain": [
       "[1] \"검색대상 객체길이: 1요소\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "'검색대상 객체글자수:'"
      ],
      "text/latex": [
       "'검색대상 객체글자수:'"
      ],
      "text/markdown": [
       "'검색대상 객체글자수:'"
      ],
      "text/plain": [
       "[1] \"검색대상 객체글자수:\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] 119\n"
     ]
    }
   ],
   "source": [
    "# 간단 텍스트셋 준비\n",
    "my <- 'The 45th President of the United States, Donald Trump, states that he knows how to play trump with the former president'\n",
    "\n",
    "sprintf('검색대상 객체내용:')\n",
    "print(my)\n",
    "\n",
    "sprintf('검색대상 객체유형: %s객체', class(my))\n",
    "sprintf('검색대상 객체길이: %d요소', length(my))\n",
    "\n",
    "sprintf('검색대상 객체글자수:')\n",
    "nchar(my) %>% print\n",
    "# - sprintf('검색대상 객체글자수: %d글자', str_length(my))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 단어단위로 직접 분해하는 경우\n",
    "* 보통은 <font color = 'blue'>str_split(string = 텍스트셋, pattern = <font color = 'red'>검색패턴</font>)</font>을 통해서 텍스트셋을 세부요소로 점진적으로 분해할 수 있음\n",
    "* 현재 주어진 텍스트셋에는 단어 이외에 문장부호나 특수문자 등이 섞여있는 구조라서, 이들에 대한 전처리가 필요하지만 \n",
    "<br>단어경계 옵션인 pattern = boundary('word')를 통해서 바로 텍스트셋을 단어단위로 분해할 수 있음\n",
    "* ==> <font color = 'blue'>str_extract_all(string = my, pattern = <font color = 'red'>boundary('word')</font>)</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 검색패턴이 아닌 단어경계 옵션을 이용한 단어 직접추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "'원본문장:'"
      ],
      "text/latex": [
       "'원본문장:'"
      ],
      "text/markdown": [
       "'원본문장:'"
      ],
      "text/plain": [
       "[1] \"원본문장:\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"The 45th President of the United States, Donald Trump, states that he knows how to play trump with the former president\"\n",
      "#----------------------------------------"
     ]
    },
    {
     "data": {
      "text/html": [
       "'단어경계를 기준으로 요소별로 분해:'"
      ],
      "text/latex": [
       "'단어경계를 기준으로 요소별로 분해:'"
      ],
      "text/markdown": [
       "'단어경계를 기준으로 요소별로 분해:'"
      ],
      "text/plain": [
       "[1] \"단어경계를 기준으로 요소별로 분해:\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1]]\n",
      " [1] \"The\"       \"45th\"      \"President\" \"of\"        \"the\"       \"United\"   \n",
      " [7] \"States\"    \"Donald\"    \"Trump\"     \"states\"    \"that\"      \"he\"       \n",
      "[13] \"knows\"     \"how\"       \"to\"        \"play\"      \"trump\"     \"with\"     \n",
      "[19] \"the\"       \"former\"    \"president\"\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "'요소별로 분해된 세부요소 갯수:'"
      ],
      "text/latex": [
       "'요소별로 분해된 세부요소 갯수:'"
      ],
      "text/markdown": [
       "'요소별로 분해된 세부요소 갯수:'"
      ],
      "text/plain": [
       "[1] \"요소별로 분해된 세부요소 갯수:\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1]]\n",
      "[1] 21\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "'요소별로 분해된 세부요소들의 글자 갯수:'"
      ],
      "text/latex": [
       "'요소별로 분해된 세부요소들의 글자 갯수:'"
      ],
      "text/markdown": [
       "'요소별로 분해된 세부요소들의 글자 갯수:'"
      ],
      "text/plain": [
       "[1] \"요소별로 분해된 세부요소들의 글자 갯수:\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1]]\n",
      " [1] 3 4 9 2 3 6 6 6 5 6 4 2 5 3 2 4 5 4 3 6 9\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "'요소별로 분해된 세부요소들의 기술통계:'"
      ],
      "text/latex": [
       "'요소별로 분해된 세부요소들의 기술통계:'"
      ],
      "text/markdown": [
       "'요소별로 분해된 세부요소들의 기술통계:'"
      ],
      "text/plain": [
       "[1] \"요소별로 분해된 세부요소들의 기술통계:\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1]]\n",
      "\n",
      "      the      45th    Donald    former        he       how     knows        of \n",
      "        2         1         1         1         1         1         1         1 \n",
      "     play president President    states    States      that       The        to \n",
      "        1         1         1         1         1         1         1         1 \n",
      "    trump     Trump    United      with \n",
      "        1         1         1         1 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 해당 검색패턴으로 분해\n",
    "\n",
    "sprintf('원본문장:')\n",
    "print(my)\n",
    "cat('#----------------------------------------')\n",
    "\n",
    "sprintf('단어경계를 기준으로 요소별로 분해:')\n",
    "str_extract_all(string = my, pattern = boundary('word')) %>% print\n",
    "\n",
    "sprintf('요소별로 분해된 세부요소 갯수:')\n",
    "str_extract_all(string = my, pattern = boundary('word')) %>% map(length) %>% print\n",
    "\n",
    "sprintf('요소별로 분해된 세부요소들의 글자 갯수:')\n",
    "str_extract_all(string = my, pattern = boundary('word')) %>% map(str_length) %>% print\n",
    "\n",
    "sprintf('요소별로 분해된 세부요소들의 기술통계:')\n",
    "str_extract_all(string = my, pattern = boundary('word')) %>% \n",
    "    map(table) %>% map(sort, decreasing = TRUE) %>% print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 텍스트셋 분해결과에 따른 전처리 방향설정\n",
    "* <font color = 'red'>the</font>와 <font color = 'red'>The</font> --> 같은 단어로 간주해 소문자로 통일처리\n",
    "* <font color = 'red'>president</font>와 <font color = 'red'>President</font> --> 같은 단어로 간주해 소문자로 통일처리\n",
    "* <font color = 'red'>states</font>와 <font color = 'red'>States</font> \n",
    "<br>--> state는 말하다와 미국주라는 상이한 의미로 사용되고 있어 통일 불필요. 미국주에는 고유명사인 States_uq로 처리\n",
    "* <font color = 'red'>trump</font>와 <font color = 'red'>Trump</font>\n",
    "<br>--> trump는 카드놀이와 대통령성이라는 상이한 의미로 사용되고 있어 통일 불필요. 대통령성에는 고유명사인 Trump_uq로 처리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 전처리 방향별 정제 작업실시 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color = 'red'>the</font>와 <font color = 'red'>The</font> --> 같은 단어로 간주해 소문자로 통일처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "'단어경계를 기준으로 요소별로 분해:'"
      ],
      "text/latex": [
       "'단어경계를 기준으로 요소별로 분해:'"
      ],
      "text/markdown": [
       "'단어경계를 기준으로 요소별로 분해:'"
      ],
      "text/plain": [
       "[1] \"단어경계를 기준으로 요소별로 분해:\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1]]\n",
      " [1] \"the\"       \"45th\"      \"President\" \"of\"        \"the\"       \"United\"   \n",
      " [7] \"States\"    \"Donald\"    \"Trump\"     \"states\"    \"that\"      \"he\"       \n",
      "[13] \"knows\"     \"how\"       \"to\"        \"play\"      \"trump\"     \"with\"     \n",
      "[19] \"the\"       \"former\"    \"president\"\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "'요소별로 분해된 세부요소 갯수:'"
      ],
      "text/latex": [
       "'요소별로 분해된 세부요소 갯수:'"
      ],
      "text/markdown": [
       "'요소별로 분해된 세부요소 갯수:'"
      ],
      "text/plain": [
       "[1] \"요소별로 분해된 세부요소 갯수:\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1]]\n",
      "[1] 21\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "'요소별로 분해된 세부요소들의 글자 갯수:'"
      ],
      "text/latex": [
       "'요소별로 분해된 세부요소들의 글자 갯수:'"
      ],
      "text/markdown": [
       "'요소별로 분해된 세부요소들의 글자 갯수:'"
      ],
      "text/plain": [
       "[1] \"요소별로 분해된 세부요소들의 글자 갯수:\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1]]\n",
      " [1] 3 4 9 2 3 6 6 6 5 6 4 2 5 3 2 4 5 4 3 6 9\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "'요소별로 분해된 세부요소들의 기술통계:'"
      ],
      "text/latex": [
       "'요소별로 분해된 세부요소들의 기술통계:'"
      ],
      "text/markdown": [
       "'요소별로 분해된 세부요소들의 기술통계:'"
      ],
      "text/plain": [
       "[1] \"요소별로 분해된 세부요소들의 기술통계:\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1]]\n",
      "\n",
      "      the      45th    Donald    former        he       how     knows        of \n",
      "        3         1         1         1         1         1         1         1 \n",
      "     play president President    states    States      that        to     trump \n",
      "        1         1         1         1         1         1         1         1 \n",
      "    Trump    United      with \n",
      "        1         1         1 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 전처리 방향별 정규표현식 설정\n",
    "org <- '[T|t][H|h][E|e]' # 원본패턴: 대소문자에 상관없이 the라는 3글자를 검색하는 패턴설정\n",
    "tgt <- 'the' # 목표패턴: 모두 the라는 소문자로 통일하는 패턴설정 \n",
    "\n",
    "# 특정패턴에 대한 전처리 실시\n",
    "my_pp <- str_replace_all(string = my, pattern = org, replacement = tgt)\n",
    "\n",
    "sprintf('단어경계를 기준으로 요소별로 분해:')\n",
    "str_extract_all(string = my_pp, pattern = boundary('word')) %>% print\n",
    "\n",
    "sprintf('요소별로 분해된 세부요소 갯수:')\n",
    "str_extract_all(string = my_pp, pattern = boundary('word')) %>% map(length) %>% print\n",
    "\n",
    "sprintf('요소별로 분해된 세부요소들의 글자 갯수:')\n",
    "str_extract_all(string = my_pp, pattern = boundary('word')) %>% map(str_length) %>% print\n",
    "\n",
    "sprintf('요소별로 분해된 세부요소들의 기술통계:')\n",
    "str_extract_all(string = my_pp, pattern = boundary('word')) %>% \n",
    "    map(table) %>% map(sort, decreasing = TRUE) %>% print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color = 'red'>president</font>와 <font color = 'red'>President</font> --> 같은 단어로 간주해 소문자로 통일처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "'단어경계를 기준으로 요소별로 분해:'"
      ],
      "text/latex": [
       "'단어경계를 기준으로 요소별로 분해:'"
      ],
      "text/markdown": [
       "'단어경계를 기준으로 요소별로 분해:'"
      ],
      "text/plain": [
       "[1] \"단어경계를 기준으로 요소별로 분해:\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1]]\n",
      " [1] \"the\"       \"45th\"      \"president\" \"of\"        \"the\"       \"United\"   \n",
      " [7] \"States\"    \"Donald\"    \"Trump\"     \"states\"    \"that\"      \"he\"       \n",
      "[13] \"knows\"     \"how\"       \"to\"        \"play\"      \"trump\"     \"with\"     \n",
      "[19] \"the\"       \"former\"    \"president\"\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "'요소별로 분해된 세부요소 갯수:'"
      ],
      "text/latex": [
       "'요소별로 분해된 세부요소 갯수:'"
      ],
      "text/markdown": [
       "'요소별로 분해된 세부요소 갯수:'"
      ],
      "text/plain": [
       "[1] \"요소별로 분해된 세부요소 갯수:\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1]]\n",
      "[1] 21\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "'요소별로 분해된 세부요소들의 글자 갯수:'"
      ],
      "text/latex": [
       "'요소별로 분해된 세부요소들의 글자 갯수:'"
      ],
      "text/markdown": [
       "'요소별로 분해된 세부요소들의 글자 갯수:'"
      ],
      "text/plain": [
       "[1] \"요소별로 분해된 세부요소들의 글자 갯수:\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1]]\n",
      " [1] 3 4 9 2 3 6 6 6 5 6 4 2 5 3 2 4 5 4 3 6 9\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "'요소별로 분해된 세부요소들의 기술통계:'"
      ],
      "text/latex": [
       "'요소별로 분해된 세부요소들의 기술통계:'"
      ],
      "text/markdown": [
       "'요소별로 분해된 세부요소들의 기술통계:'"
      ],
      "text/plain": [
       "[1] \"요소별로 분해된 세부요소들의 기술통계:\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1]]\n",
      "\n",
      "      the president      45th    Donald    former        he       how     knows \n",
      "        3         2         1         1         1         1         1         1 \n",
      "       of      play    states    States      that        to     trump     Trump \n",
      "        1         1         1         1         1         1         1         1 \n",
      "   United      with \n",
      "        1         1 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 전처리 방향별 정규표현식 설정\n",
    "org <- '[P|p][R|r][E|e][S|s][I|i][D|d][E|e][N|n][T|t]' \n",
    "# - 원본패턴: 대소문자에 상관없이 president라는 9글자를 검색하는 패턴설정 \n",
    "tgt <- 'president' \n",
    "# - 목표패턴: 모두 president라는 소문자로 통일하는 패턴설정 \n",
    "\n",
    "# 특정패턴에 대한 전처리 실시\n",
    "my_pp <- str_replace_all(string = my_pp, pattern = org, replacement = tgt)\n",
    "\n",
    "sprintf('단어경계를 기준으로 요소별로 분해:')\n",
    "str_extract_all(string = my_pp, pattern = boundary('word')) %>% print\n",
    "\n",
    "sprintf('요소별로 분해된 세부요소 갯수:')\n",
    "str_extract_all(string = my_pp, pattern = boundary('word')) %>% map(length) %>% print\n",
    "\n",
    "sprintf('요소별로 분해된 세부요소들의 글자 갯수:')\n",
    "str_extract_all(string = my_pp, pattern = boundary('word')) %>% map(str_length) %>% print\n",
    "\n",
    "sprintf('요소별로 분해된 세부요소들의 기술통계:')\n",
    "str_extract_all(string = my_pp, pattern = boundary('word')) %>% \n",
    "    map(table) %>% map(sort, decreasing = TRUE) %>% print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color = 'red'>states</font>와 <font color = 'red'>States</font> \n",
    "<br>--> state는 말하다와 미국주라는 상이한 의미로 사용되고 있어 통일 불필요. 미국주에는 고유명사인 States_uq로 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "'단어경계를 기준으로 요소별로 분해:'"
      ],
      "text/latex": [
       "'단어경계를 기준으로 요소별로 분해:'"
      ],
      "text/markdown": [
       "'단어경계를 기준으로 요소별로 분해:'"
      ],
      "text/plain": [
       "[1] \"단어경계를 기준으로 요소별로 분해:\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1]]\n",
      " [1] \"the\"       \"45th\"      \"president\" \"of\"        \"the\"       \"United\"   \n",
      " [7] \"States_uq\" \"Donald\"    \"Trump\"     \"states\"    \"that\"      \"he\"       \n",
      "[13] \"knows\"     \"how\"       \"to\"        \"play\"      \"trump\"     \"with\"     \n",
      "[19] \"the\"       \"former\"    \"president\"\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "'요소별로 분해된 세부요소 갯수:'"
      ],
      "text/latex": [
       "'요소별로 분해된 세부요소 갯수:'"
      ],
      "text/markdown": [
       "'요소별로 분해된 세부요소 갯수:'"
      ],
      "text/plain": [
       "[1] \"요소별로 분해된 세부요소 갯수:\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1]]\n",
      "[1] 21\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "'요소별로 분해된 세부요소들의 글자 갯수:'"
      ],
      "text/latex": [
       "'요소별로 분해된 세부요소들의 글자 갯수:'"
      ],
      "text/markdown": [
       "'요소별로 분해된 세부요소들의 글자 갯수:'"
      ],
      "text/plain": [
       "[1] \"요소별로 분해된 세부요소들의 글자 갯수:\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1]]\n",
      " [1] 3 4 9 2 3 6 9 6 5 6 4 2 5 3 2 4 5 4 3 6 9\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "'요소별로 분해된 세부요소들의 기술통계:'"
      ],
      "text/latex": [
       "'요소별로 분해된 세부요소들의 기술통계:'"
      ],
      "text/markdown": [
       "'요소별로 분해된 세부요소들의 기술통계:'"
      ],
      "text/plain": [
       "[1] \"요소별로 분해된 세부요소들의 기술통계:\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1]]\n",
      "\n",
      "      the president      45th    Donald    former        he       how     knows \n",
      "        3         2         1         1         1         1         1         1 \n",
      "       of      play    states States_uq      that        to     trump     Trump \n",
      "        1         1         1         1         1         1         1         1 \n",
      "   United      with \n",
      "        1         1 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 전처리 방향별 정규표현식 설정\n",
    "org <- 'States' \n",
    "# - 원본패턴: 미국주를 의미하는 단어를 찾는 검색패턴 설정 \n",
    "tgt <- 'States_uq' \n",
    "# - 목표패턴: 미국주를 의미하는 단어에 고유명사라는 뜻의 uq(unique) 확장자를 붙여줌 \n",
    "\n",
    "# 특정패턴에 대한 전처리 실시\n",
    "my_pp <- str_replace_all(string = my_pp, pattern = org, replacement = tgt)\n",
    "\n",
    "sprintf('단어경계를 기준으로 요소별로 분해:')\n",
    "str_extract_all(string = my_pp, pattern = boundary('word')) %>% print\n",
    "\n",
    "sprintf('요소별로 분해된 세부요소 갯수:')\n",
    "str_extract_all(string = my_pp, pattern = boundary('word')) %>% map(length) %>% print\n",
    "\n",
    "sprintf('요소별로 분해된 세부요소들의 글자 갯수:')\n",
    "str_extract_all(string = my_pp, pattern = boundary('word')) %>% map(str_length) %>% print\n",
    "\n",
    "sprintf('요소별로 분해된 세부요소들의 기술통계:')\n",
    "str_extract_all(string = my_pp, pattern = boundary('word')) %>% \n",
    "    map(table) %>% map(sort, decreasing = TRUE) %>% print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color = 'red'>trump</font>와 <font color = 'red'>Trump</font>\n",
    "<br>--> trump는 카드놀이와 대통령성이라는 상이한 의미로 사용되고 있어 통일 불필요. 대통령성에는 고유명사인 Trump_uq로 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "'단어경계를 기준으로 요소별로 분해:'"
      ],
      "text/latex": [
       "'단어경계를 기준으로 요소별로 분해:'"
      ],
      "text/markdown": [
       "'단어경계를 기준으로 요소별로 분해:'"
      ],
      "text/plain": [
       "[1] \"단어경계를 기준으로 요소별로 분해:\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1]]\n",
      " [1] \"the\"       \"45th\"      \"president\" \"of\"        \"the\"       \"United\"   \n",
      " [7] \"States_uq\" \"Donald\"    \"Trump_uq\"  \"states\"    \"that\"      \"he\"       \n",
      "[13] \"knows\"     \"how\"       \"to\"        \"play\"      \"trump\"     \"with\"     \n",
      "[19] \"the\"       \"former\"    \"president\"\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "'요소별로 분해된 세부요소 갯수:'"
      ],
      "text/latex": [
       "'요소별로 분해된 세부요소 갯수:'"
      ],
      "text/markdown": [
       "'요소별로 분해된 세부요소 갯수:'"
      ],
      "text/plain": [
       "[1] \"요소별로 분해된 세부요소 갯수:\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1]]\n",
      "[1] 21\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "'요소별로 분해된 세부요소들의 글자 갯수:'"
      ],
      "text/latex": [
       "'요소별로 분해된 세부요소들의 글자 갯수:'"
      ],
      "text/markdown": [
       "'요소별로 분해된 세부요소들의 글자 갯수:'"
      ],
      "text/plain": [
       "[1] \"요소별로 분해된 세부요소들의 글자 갯수:\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1]]\n",
      " [1] 3 4 9 2 3 6 9 6 8 6 4 2 5 3 2 4 5 4 3 6 9\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "'요소별로 분해된 세부요소들의 기술통계:'"
      ],
      "text/latex": [
       "'요소별로 분해된 세부요소들의 기술통계:'"
      ],
      "text/markdown": [
       "'요소별로 분해된 세부요소들의 기술통계:'"
      ],
      "text/plain": [
       "[1] \"요소별로 분해된 세부요소들의 기술통계:\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1]]\n",
      "\n",
      "      the president      45th    Donald    former        he       how     knows \n",
      "        3         2         1         1         1         1         1         1 \n",
      "       of      play    states States_uq      that        to     trump  Trump_uq \n",
      "        1         1         1         1         1         1         1         1 \n",
      "   United      with \n",
      "        1         1 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 전처리 방향별 정규표현식 설정\n",
    "org <- 'Trump' \n",
    "# - 원본패턴: 미국대통령 성을 찾는 검색패턴 설정 \n",
    "tgt <- 'Trump_uq' \n",
    "# - 목표패턴: 미국대통령 성에 고유명사라는 뜻의 uq(unique) 확장자를 붙여줌 \n",
    "\n",
    "# 특정패턴에 대한 전처리 실시\n",
    "my_pp <- str_replace_all(string = my_pp, pattern = org, replacement = tgt)\n",
    "\n",
    "sprintf('단어경계를 기준으로 요소별로 분해:')\n",
    "str_extract_all(string = my_pp, pattern = boundary('word')) %>% print\n",
    "\n",
    "sprintf('요소별로 분해된 세부요소 갯수:')\n",
    "str_extract_all(string = my_pp, pattern = boundary('word')) %>% map(length) %>% print\n",
    "\n",
    "sprintf('요소별로 분해된 세부요소들의 글자 갯수:')\n",
    "str_extract_all(string = my_pp, pattern = boundary('word')) %>% map(str_length) %>% print\n",
    "\n",
    "sprintf('요소별로 분해된 세부요소들의 기술통계:')\n",
    "str_extract_all(string = my_pp, pattern = boundary('word')) %>% \n",
    "    map(table) %>% map(sort, decreasing = TRUE) %>% print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color = 'blue'>숫자표현(numeric data) 제거 (p.102)</font>\n",
    "* 텍스트셋에는 양적인 크기나 서열/순서의 의미, 범주의 의미, 변화의 의미로서 숫자를 사용하므로 이에 대한 적절한 처리가 필요함\n",
    "* 일률적인 숫자텍스트 삭제 이외에, 여건에 따라 특정한 수치적 표현은 고유명사로 유지해야하는 경우도 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 간단 텍스트셋 준비\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "'검색대상 객체내용:'"
      ],
      "text/latex": [
       "'검색대상 객체내용:'"
      ],
      "text/markdown": [
       "'검색대상 객체내용:'"
      ],
      "text/plain": [
       "[1] \"검색대상 객체내용:\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"In 2016, the number of smartphones sold to consumers stood at around 1.5 billion units, a significant increase from the 680 million units sold in 2012. This means that over 28 percent of the world’s total population owned a smart device in 2016.\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "'검색대상 객체유형: character객체'"
      ],
      "text/latex": [
       "'검색대상 객체유형: character객체'"
      ],
      "text/markdown": [
       "'검색대상 객체유형: character객체'"
      ],
      "text/plain": [
       "[1] \"검색대상 객체유형: character객체\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "'검색대상 객체길이: 1요소'"
      ],
      "text/latex": [
       "'검색대상 객체길이: 1요소'"
      ],
      "text/markdown": [
       "'검색대상 객체길이: 1요소'"
      ],
      "text/plain": [
       "[1] \"검색대상 객체길이: 1요소\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "'검색대상 객체글자수:'"
      ],
      "text/latex": [
       "'검색대상 객체글자수:'"
      ],
      "text/markdown": [
       "'검색대상 객체글자수:'"
      ],
      "text/plain": [
       "[1] \"검색대상 객체글자수:\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] 245\n"
     ]
    }
   ],
   "source": [
    "# 간단 텍스트셋 준비\n",
    "my <- 'In 2016, the number of smartphones sold to consumers stood at around 1.5 billion units, a significant increase from the 680 million units sold in 2012. This means that over 28 percent of the world’s total population owned a smart device in 2016.'\n",
    "\n",
    "sprintf('검색대상 객체내용:')\n",
    "print(my)\n",
    "\n",
    "sprintf('검색대상 객체유형: %s객체', class(my))\n",
    "sprintf('검색대상 객체길이: %d요소', length(my))\n",
    "\n",
    "sprintf('검색대상 객체글자수:')\n",
    "nchar(my) %>% print\n",
    "# - sprintf('검색대상 객체글자수: %d글자', str_length(my))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 단어단위로 직접 분해하는 경우\n",
    "* 보통은 <font color = 'blue'>str_split(string = 텍스트셋, pattern = <font color = 'red'>검색패턴</font>)</font>을 통해서 텍스트셋을 세부요소로 점진적으로 분해할 수 있음\n",
    "* 현재 주어진 텍스트셋에는 단어 이외에 문장부호나 특수문자 등이 섞여있는 구조라서, 이들에 대한 전처리가 필요하지만 \n",
    "<br>단어경계 옵션인 pattern = boundary('word')를 통해서 바로 텍스트셋을 단어단위로 분해할 수 있음\n",
    "* ==> <font color = 'blue'>str_extract_all(string = my, pattern = <font color = 'red'>boundary('word')</font>)</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 검색패턴이 아닌 단어경계 옵션을 이용한 텍스트셋 직접분해"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "'원본문장:'"
      ],
      "text/latex": [
       "'원본문장:'"
      ],
      "text/markdown": [
       "'원본문장:'"
      ],
      "text/plain": [
       "[1] \"원본문장:\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"In 2016, the number of smartphones sold to consumers stood at around 1.5 billion units, a significant increase from the 680 million units sold in 2012. This means that over 28 percent of the world’s total population owned a smart device in 2016.\"\n",
      "#----------------------------------------"
     ]
    },
    {
     "data": {
      "text/html": [
       "'단어경계를 기준으로 요소별로 분해:'"
      ],
      "text/latex": [
       "'단어경계를 기준으로 요소별로 분해:'"
      ],
      "text/markdown": [
       "'단어경계를 기준으로 요소별로 분해:'"
      ],
      "text/plain": [
       "[1] \"단어경계를 기준으로 요소별로 분해:\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1]]\n",
      " [1] \"In\"          \"2016\"        \"the\"         \"number\"      \"of\"         \n",
      " [6] \"smartphones\" \"sold\"        \"to\"          \"consumers\"   \"stood\"      \n",
      "[11] \"at\"          \"around\"      \"1.5\"         \"billion\"     \"units\"      \n",
      "[16] \"a\"           \"significant\" \"increase\"    \"from\"        \"the\"        \n",
      "[21] \"680\"         \"million\"     \"units\"       \"sold\"        \"in\"         \n",
      "[26] \"2012\"        \"This\"        \"means\"       \"that\"        \"over\"       \n",
      "[31] \"28\"          \"percent\"     \"of\"          \"the\"         \"world’s\"   \n",
      "[36] \"total\"       \"population\"  \"owned\"       \"a\"           \"smart\"      \n",
      "[41] \"device\"      \"in\"          \"2016\"       \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "'요소별로 분해된 세부요소 갯수:'"
      ],
      "text/latex": [
       "'요소별로 분해된 세부요소 갯수:'"
      ],
      "text/markdown": [
       "'요소별로 분해된 세부요소 갯수:'"
      ],
      "text/plain": [
       "[1] \"요소별로 분해된 세부요소 갯수:\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1]]\n",
      "[1] 43\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "'요소별로 분해된 세부요소들의 글자 갯수:'"
      ],
      "text/latex": [
       "'요소별로 분해된 세부요소들의 글자 갯수:'"
      ],
      "text/markdown": [
       "'요소별로 분해된 세부요소들의 글자 갯수:'"
      ],
      "text/plain": [
       "[1] \"요소별로 분해된 세부요소들의 글자 갯수:\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1]]\n",
      " [1]  2  4  3  6  2 11  4  2  9  5  2  6  3  7  5  1 11  8  4  3  3  7  5  4  2\n",
      "[26]  4  4  5  4  4  2  7  2  3  7  5 10  5  1  5  6  2  4\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "'요소별로 분해된 세부요소들의 기술통계:'"
      ],
      "text/latex": [
       "'요소별로 분해된 세부요소들의 기술통계:'"
      ],
      "text/markdown": [
       "'요소별로 분해된 세부요소들의 기술통계:'"
      ],
      "text/plain": [
       "[1] \"요소별로 분해된 세부요소들의 기술통계:\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1]]\n",
      "\n",
      "        the        2016           a          in          of        sold \n",
      "          3           2           2           2           2           2 \n",
      "      units         1.5        2012          28         680      around \n",
      "          2           1           1           1           1           1 \n",
      "         at     billion   consumers      device        from          In \n",
      "          1           1           1           1           1           1 \n",
      "   increase       means     million      number        over       owned \n",
      "          1           1           1           1           1           1 \n",
      "    percent  population significant       smart smartphones       stood \n",
      "          1           1           1           1           1           1 \n",
      "       that        This          to       total    world’s \n",
      "          1           1           1           1           1 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 해당 검색패턴으로 분해\n",
    "\n",
    "sprintf('원본문장:')\n",
    "print(my)\n",
    "cat('#----------------------------------------')\n",
    "\n",
    "sprintf('단어경계를 기준으로 요소별로 분해:')\n",
    "str_extract_all(string = my, pattern = boundary('word')) %>% print\n",
    "\n",
    "sprintf('요소별로 분해된 세부요소 갯수:')\n",
    "str_extract_all(string = my, pattern = boundary('word')) %>% map(length) %>% print\n",
    "\n",
    "sprintf('요소별로 분해된 세부요소들의 글자 갯수:')\n",
    "str_extract_all(string = my, pattern = boundary('word')) %>% map(str_length) %>% print\n",
    "\n",
    "sprintf('요소별로 분해된 세부요소들의 기술통계:')\n",
    "str_extract_all(string = my, pattern = boundary('word')) %>% \n",
    "    map(table) %>% map(sort, decreasing = TRUE) %>% print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 텍스트셋 분해결과에 따른 전처리 방향설정\n",
    "* 분해된 단어 중에서 <font color = 'blue'>1.5</font>처럼 <font color = 'red'>소수점이 포함된 숫자</font>를 제거함\n",
    "* 분해된 단어 중에서 <font color = 'blue'>2016, 2012, 680, 28</font> 등의 <font color = 'red'>정수</font>를 제거함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 전처리 방향별 정제 작업실시 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 분해된 단어 중에서 <font color = 'blue'>1.5</font>처럼 <font color = 'red'>소수점이 포함된 숫자</font>를 제거함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "'단어경계를 기준으로 요소별로 분해:'"
      ],
      "text/latex": [
       "'단어경계를 기준으로 요소별로 분해:'"
      ],
      "text/markdown": [
       "'단어경계를 기준으로 요소별로 분해:'"
      ],
      "text/plain": [
       "[1] \"단어경계를 기준으로 요소별로 분해:\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1]]\n",
      " [1] \"In\"          \"2016\"        \"the\"         \"number\"      \"of\"         \n",
      " [6] \"smartphones\" \"sold\"        \"to\"          \"consumers\"   \"stood\"      \n",
      "[11] \"at\"          \"around\"      \"billion\"     \"units\"       \"a\"          \n",
      "[16] \"significant\" \"increase\"    \"from\"        \"the\"         \"680\"        \n",
      "[21] \"million\"     \"units\"       \"sold\"        \"in\"          \"2012\"       \n",
      "[26] \"This\"        \"means\"       \"that\"        \"over\"        \"28\"         \n",
      "[31] \"percent\"     \"of\"          \"the\"         \"world’s\"    \"total\"      \n",
      "[36] \"population\"  \"owned\"       \"a\"           \"smart\"       \"device\"     \n",
      "[41] \"in\"          \"2016\"       \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "'요소별로 분해된 세부요소 갯수:'"
      ],
      "text/latex": [
       "'요소별로 분해된 세부요소 갯수:'"
      ],
      "text/markdown": [
       "'요소별로 분해된 세부요소 갯수:'"
      ],
      "text/plain": [
       "[1] \"요소별로 분해된 세부요소 갯수:\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1]]\n",
      "[1] 42\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "'요소별로 분해된 세부요소들의 글자 갯수:'"
      ],
      "text/latex": [
       "'요소별로 분해된 세부요소들의 글자 갯수:'"
      ],
      "text/markdown": [
       "'요소별로 분해된 세부요소들의 글자 갯수:'"
      ],
      "text/plain": [
       "[1] \"요소별로 분해된 세부요소들의 글자 갯수:\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1]]\n",
      " [1]  2  4  3  6  2 11  4  2  9  5  2  6  7  5  1 11  8  4  3  3  7  5  4  2  4\n",
      "[26]  4  5  4  4  2  7  2  3  7  5 10  5  1  5  6  2  4\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "'요소별로 분해된 세부요소들의 기술통계:'"
      ],
      "text/latex": [
       "'요소별로 분해된 세부요소들의 기술통계:'"
      ],
      "text/markdown": [
       "'요소별로 분해된 세부요소들의 기술통계:'"
      ],
      "text/plain": [
       "[1] \"요소별로 분해된 세부요소들의 기술통계:\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1]]\n",
      "\n",
      "        the        2016           a          in          of        sold \n",
      "          3           2           2           2           2           2 \n",
      "      units        2012          28         680      around          at \n",
      "          2           1           1           1           1           1 \n",
      "    billion   consumers      device        from          In    increase \n",
      "          1           1           1           1           1           1 \n",
      "      means     million      number        over       owned     percent \n",
      "          1           1           1           1           1           1 \n",
      " population significant       smart smartphones       stood        that \n",
      "          1           1           1           1           1           1 \n",
      "       This          to       total    world’s \n",
      "          1           1           1           1 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 전처리 방향별 정규표현식 설정\n",
    "org <- '[[:digit:]]+[.][[:digit:]]+' # 원본패턴: 대소문자에 상관없이 the라는 3글자를 검색하는 패턴설정\n",
    "tgt <- ' ' # 목표패턴: 모두 the라는 소문자로 통일하는 패턴설정 \n",
    "\n",
    "# 특정패턴에 대한 전처리 실시\n",
    "my_pp <- str_replace_all(string = my, pattern = org, replacement = tgt)\n",
    "\n",
    "sprintf('단어경계를 기준으로 요소별로 분해:')\n",
    "str_extract_all(string = my_pp, pattern = boundary('word')) %>% print\n",
    "\n",
    "sprintf('요소별로 분해된 세부요소 갯수:')\n",
    "str_extract_all(string = my_pp, pattern = boundary('word')) %>% map(length) %>% print\n",
    "\n",
    "sprintf('요소별로 분해된 세부요소들의 글자 갯수:')\n",
    "str_extract_all(string = my_pp, pattern = boundary('word')) %>% map(str_length) %>% print\n",
    "\n",
    "sprintf('요소별로 분해된 세부요소들의 기술통계:')\n",
    "str_extract_all(string = my_pp, pattern = boundary('word')) %>% \n",
    "    map(table) %>% map(sort, decreasing = TRUE) %>% print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 분해된 단어 중에서 <font color = 'blue'>2016, 2012, 680, 28</font> 등의 <font color = 'red'>정수</font>를 제거함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "'단어경계를 기준으로 요소별로 분해:'"
      ],
      "text/latex": [
       "'단어경계를 기준으로 요소별로 분해:'"
      ],
      "text/markdown": [
       "'단어경계를 기준으로 요소별로 분해:'"
      ],
      "text/plain": [
       "[1] \"단어경계를 기준으로 요소별로 분해:\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1]]\n",
      " [1] \"In\"          \"the\"         \"number\"      \"of\"          \"smartphones\"\n",
      " [6] \"sold\"        \"to\"          \"consumers\"   \"stood\"       \"at\"         \n",
      "[11] \"around\"      \"billion\"     \"units\"       \"a\"           \"significant\"\n",
      "[16] \"increase\"    \"from\"        \"the\"         \"million\"     \"units\"      \n",
      "[21] \"sold\"        \"in\"          \"This\"        \"means\"       \"that\"       \n",
      "[26] \"over\"        \"percent\"     \"of\"          \"the\"         \"world’s\"   \n",
      "[31] \"total\"       \"population\"  \"owned\"       \"a\"           \"smart\"      \n",
      "[36] \"device\"      \"in\"         \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "'요소별로 분해된 세부요소 갯수:'"
      ],
      "text/latex": [
       "'요소별로 분해된 세부요소 갯수:'"
      ],
      "text/markdown": [
       "'요소별로 분해된 세부요소 갯수:'"
      ],
      "text/plain": [
       "[1] \"요소별로 분해된 세부요소 갯수:\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1]]\n",
      "[1] 37\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "'요소별로 분해된 세부요소들의 글자 갯수:'"
      ],
      "text/latex": [
       "'요소별로 분해된 세부요소들의 글자 갯수:'"
      ],
      "text/markdown": [
       "'요소별로 분해된 세부요소들의 글자 갯수:'"
      ],
      "text/plain": [
       "[1] \"요소별로 분해된 세부요소들의 글자 갯수:\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1]]\n",
      " [1]  2  3  6  2 11  4  2  9  5  2  6  7  5  1 11  8  4  3  7  5  4  2  4  5  4\n",
      "[26]  4  7  2  3  7  5 10  5  1  5  6  2\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "'요소별로 분해된 세부요소들의 기술통계:'"
      ],
      "text/latex": [
       "'요소별로 분해된 세부요소들의 기술통계:'"
      ],
      "text/markdown": [
       "'요소별로 분해된 세부요소들의 기술통계:'"
      ],
      "text/plain": [
       "[1] \"요소별로 분해된 세부요소들의 기술통계:\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1]]\n",
      "\n",
      "        the           a          in          of        sold       units \n",
      "          3           2           2           2           2           2 \n",
      "     around          at     billion   consumers      device        from \n",
      "          1           1           1           1           1           1 \n",
      "         In    increase       means     million      number        over \n",
      "          1           1           1           1           1           1 \n",
      "      owned     percent  population significant       smart smartphones \n",
      "          1           1           1           1           1           1 \n",
      "      stood        that        This          to       total    world’s \n",
      "          1           1           1           1           1           1 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 전처리 방향별 정규표현식 설정\n",
    "org <- '[[:digit:]]+' # 원본패턴: 대소문자에 상관없이 the라는 3글자를 검색하는 패턴설정\n",
    "tgt <- ' ' # 목표패턴: 모두 the라는 소문자로 통일하는 패턴설정 \n",
    "\n",
    "# 특정패턴에 대한 전처리 실시\n",
    "my_pp <- str_replace_all(string = my, pattern = org, replacement = tgt)\n",
    "\n",
    "sprintf('단어경계를 기준으로 요소별로 분해:')\n",
    "str_extract_all(string = my_pp, pattern = boundary('word')) %>% print\n",
    "\n",
    "sprintf('요소별로 분해된 세부요소 갯수:')\n",
    "str_extract_all(string = my_pp, pattern = boundary('word')) %>% map(length) %>% print\n",
    "\n",
    "sprintf('요소별로 분해된 세부요소들의 글자 갯수:')\n",
    "str_extract_all(string = my_pp, pattern = boundary('word')) %>% map(str_length) %>% print\n",
    "\n",
    "sprintf('요소별로 분해된 세부요소들의 기술통계:')\n",
    "str_extract_all(string = my_pp, pattern = boundary('word')) %>% \n",
    "    map(table) %>% map(sort, decreasing = TRUE) %>% print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color = 'blue'>문장부호(punctuation marks)와 특수문자(special characters) 제거 (p.105)</font>\n",
    "* 텍스트셋에는 많은 문장부호들이 사용되며, 각 문장부호는 고유한 문법적인 기능과 의미를 담고 있음\n",
    "* 문장부호인 마침표(.), 콤마(,), 콜론(:), 세미콜론(;), 특수문자인 !@#$%^&*()-_+=/<> 등을 상황에 따라 제거필요"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 간단 텍스트셋 준비\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "'검색대상 객체내용:'"
      ],
      "text/latex": [
       "'검색대상 객체내용:'"
      ],
      "text/markdown": [
       "'검색대상 객체내용:'"
      ],
      "text/plain": [
       "[1] \"검색대상 객체내용:\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"Baek et al. (2014) argued that the state of default-setting is critical for people to protect their own personal privacy on the Internet.\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "'검색대상 객체유형: character객체'"
      ],
      "text/latex": [
       "'검색대상 객체유형: character객체'"
      ],
      "text/markdown": [
       "'검색대상 객체유형: character객체'"
      ],
      "text/plain": [
       "[1] \"검색대상 객체유형: character객체\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "'검색대상 객체길이: 1요소'"
      ],
      "text/latex": [
       "'검색대상 객체길이: 1요소'"
      ],
      "text/markdown": [
       "'검색대상 객체길이: 1요소'"
      ],
      "text/plain": [
       "[1] \"검색대상 객체길이: 1요소\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "'검색대상 객체글자수:'"
      ],
      "text/latex": [
       "'검색대상 객체글자수:'"
      ],
      "text/markdown": [
       "'검색대상 객체글자수:'"
      ],
      "text/plain": [
       "[1] \"검색대상 객체글자수:\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] 137\n"
     ]
    }
   ],
   "source": [
    "# 간단 텍스트셋 준비\n",
    "my <- 'Baek et al. (2014) argued that the state of default-setting is critical for people to protect their own personal privacy on the Internet.'\n",
    "\n",
    "sprintf('검색대상 객체내용:')\n",
    "print(my)\n",
    "\n",
    "sprintf('검색대상 객체유형: %s객체', class(my))\n",
    "sprintf('검색대상 객체길이: %d요소', length(my))\n",
    "\n",
    "sprintf('검색대상 객체글자수:')\n",
    "nchar(my) %>% print\n",
    "# - sprintf('검색대상 객체글자수: %d글자', str_length(my))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 검색패턴으로 단순 공란 1칸을 설정해 분해하는 경우"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 검색패턴 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 공란 한 칸을 단순히 검색패턴으로 설정\n",
    "src = ' '"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 검색패턴을 이용한 텍스트셋 탐색"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "'원본문장:'"
      ],
      "text/latex": [
       "'원본문장:'"
      ],
      "text/markdown": [
       "'원본문장:'"
      ],
      "text/plain": [
       "[1] \"원본문장:\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"Baek et al. (2014) argued that the state of default-setting is critical for people to protect their own personal privacy on the Internet.\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style=white-space:pre-wrap>'정규표현식 검색패턴:  '</span>"
      ],
      "text/latex": [
       "'정규표현식 검색패턴:  '"
      ],
      "text/markdown": [
       "<span style=white-space:pre-wrap>'정규표현식 검색패턴:  '</span>"
      ],
      "text/plain": [
       "[1] \"정규표현식 검색패턴:  \""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#----------------------------------------"
     ]
    },
    {
     "data": {
      "text/html": [
       "'검색패턴 포함여부를 요소별 논리값으로:'"
      ],
      "text/latex": [
       "'검색패턴 포함여부를 요소별 논리값으로:'"
      ],
      "text/markdown": [
       "'검색패턴 포함여부를 요소별 논리값으로:'"
      ],
      "text/plain": [
       "[1] \"검색패턴 포함여부를 요소별 논리값으로:\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] TRUE\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "'검색패턴 포함여부를 요소별 인덱스번호로:'"
      ],
      "text/latex": [
       "'검색패턴 포함여부를 요소별 인덱스번호로:'"
      ],
      "text/markdown": [
       "'검색패턴 포함여부를 요소별 인덱스번호로:'"
      ],
      "text/plain": [
       "[1] \"검색패턴 포함여부를 요소별 인덱스번호로:\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] 1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "'검색패턴 포함여부를 요소별 내용으로:'"
      ],
      "text/latex": [
       "'검색패턴 포함여부를 요소별 내용으로:'"
      ],
      "text/markdown": [
       "'검색패턴 포함여부를 요소별 내용으로:'"
      ],
      "text/plain": [
       "[1] \"검색패턴 포함여부를 요소별 내용으로:\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"Baek et al. (2014) argued that the state of default-setting is critical for people to protect their own personal privacy on the Internet.\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "'검색패턴 포함여부를 요소별 출현위치로:'"
      ],
      "text/latex": [
       "'검색패턴 포함여부를 요소별 출현위치로:'"
      ],
      "text/markdown": [
       "'검색패턴 포함여부를 요소별 출현위치로:'"
      ],
      "text/plain": [
       "[1] \"검색패턴 포함여부를 요소별 출현위치로:\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1]]\n",
      "      start end\n",
      " [1,]     5   5\n",
      " [2,]     8   8\n",
      " [3,]    12  12\n",
      " [4,]    19  19\n",
      " [5,]    26  26\n",
      " [6,]    31  31\n",
      " [7,]    35  35\n",
      " [8,]    41  41\n",
      " [9,]    44  44\n",
      "[10,]    60  60\n",
      "[11,]    63  63\n",
      "[12,]    72  72\n",
      "[13,]    76  76\n",
      "[14,]    83  83\n",
      "[15,]    86  86\n",
      "[16,]    94  94\n",
      "[17,]   100 100\n",
      "[18,]   104 104\n",
      "[19,]   113 113\n",
      "[20,]   121 121\n",
      "[21,]   124 124\n",
      "[22,]   128 128\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "'검색패턴 포함여부를 요소별 출현내용으로:'"
      ],
      "text/latex": [
       "'검색패턴 포함여부를 요소별 출현내용으로:'"
      ],
      "text/markdown": [
       "'검색패턴 포함여부를 요소별 출현내용으로:'"
      ],
      "text/plain": [
       "[1] \"검색패턴 포함여부를 요소별 출현내용으로:\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1]]\n",
      " [1] \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \"\n",
      "[20] \" \" \" \" \" \"\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "'검색패턴 포함여부를 요소별 출현횟수로:'"
      ],
      "text/latex": [
       "'검색패턴 포함여부를 요소별 출현횟수로:'"
      ],
      "text/markdown": [
       "'검색패턴 포함여부를 요소별 출현횟수로:'"
      ],
      "text/plain": [
       "[1] \"검색패턴 포함여부를 요소별 출현횟수로:\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] 22\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "'검색패턴 포함여부를 요소별 기술통계로:'"
      ],
      "text/latex": [
       "'검색패턴 포함여부를 요소별 기술통계로:'"
      ],
      "text/markdown": [
       "'검색패턴 포함여부를 요소별 기술통계로:'"
      ],
      "text/plain": [
       "[1] \"검색패턴 포함여부를 요소별 기술통계로:\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1]]\n",
      "   \n",
      "22 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 해당 검색패턴으로 탐색\n",
    "\n",
    "sprintf('원본문장:')\n",
    "print(my)\n",
    "sprintf('정규표현식 검색패턴: %s', src)\n",
    "cat('#----------------------------------------')\n",
    "\n",
    "sprintf('검색패턴 포함여부를 요소별 논리값으로:')\n",
    "str_detect(string = my, pattern = src) %>% print \n",
    "# - grepl(x = my, pattern = src) %>% print 동일결과\n",
    "# - 검색패턴이 들어 있으면 TRUE, 없으면 FALSE 출력\n",
    "\n",
    "sprintf('검색패턴 포함여부를 요소별 인덱스번호로:')\n",
    "str_which(string = my, pattern = src) %>% print\n",
    "# - grep(x = my, pattern = src) %>% print 동일결과\n",
    "# - 검색패턴이 들어 있으면 해당 요소의 인덱스번호 출력, 없으면 해당 요소 인덱스번호 미출력\n",
    "\n",
    "sprintf('검색패턴 포함여부를 요소별 내용으로:')\n",
    "str_subset(string = my, pattern = src) %>% print \n",
    "# - grep(x = my, pattern = src , value = TRUE) %>% print 동일결과\n",
    "# - 검색패턴이 들어 있으면 해당 요소의 내용 출력, 없으면 해당 요소의 내용 미출력\n",
    "\n",
    "sprintf('검색패턴 포함여부를 요소별 출현위치로:')\n",
    "str_locate_all(string = my, pattern = src) %>% print\n",
    "# - gregexpr(text = my, pattern = src) %>% print 동일결과\n",
    "# - 검색패턴이 해당 텍스트요소에 들어 있지 않으면 base::gregexpr()은 -1을, stringr::str_locate_all()은 NA를 출력함\n",
    "\n",
    "sprintf('검색패턴 포함여부를 요소별 출현내용으로:')\n",
    "str_extract_all(string = my, pattern = src) %>% print\n",
    "# - gregexpr(text = my, pattern = src) %>% regmatches(x = my) %>% print 동일결과\n",
    "# - 검색패턴이 해당 텍스트요소에 들어 있지 않으면 character(0)을 출력함\n",
    "\n",
    "sprintf('검색패턴 포함여부를 요소별 출현횟수로:')\n",
    "str_count(string = my, pattern = src) %>% print\n",
    "# - gregexpr(text = my, pattern = src) %>% regmatches(x = my) %>% lengths %>% print 동일결과\n",
    "# - 검색패턴이 해당 텍스트요소에 들어 있는 갯수를 카운팅해 출력함\n",
    "\n",
    "sprintf('검색패턴 포함여부를 요소별 기술통계로:')\n",
    "str_extract_all(string = my, pattern = src) %>% \n",
    "    map(table) %>% map(sort, decreasing = TRUE) %>% print\n",
    "\n",
    "# 검색패턴 기준으로 탐색결과\n",
    "# - 첫번째 문자열에서는 공백 1칸이 파악되어, 이를 기준으로 단어 2개로 정상분해 가능성 있음\n",
    "# - 두번째 문자열에서는 공백이 연속으로 2칸 파악되어, 공백 1칸이 분해결과로 남게되는 문제점 있음\n",
    "# - 세번째 문자열에서는 탭(tab)키를 의미하는 \\t 표시가 공백 1칸으로 매칭되지 못해서 분해자체가 안될 문제점 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 검색패턴을 이용한 텍스트셋 분해"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "'원본문장:'"
      ],
      "text/latex": [
       "'원본문장:'"
      ],
      "text/markdown": [
       "'원본문장:'"
      ],
      "text/plain": [
       "[1] \"원본문장:\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"Baek et al. (2014) argued that the state of default-setting is critical for people to protect their own personal privacy on the Internet.\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style=white-space:pre-wrap>'정규표현식 검색패턴:  '</span>"
      ],
      "text/latex": [
       "'정규표현식 검색패턴:  '"
      ],
      "text/markdown": [
       "<span style=white-space:pre-wrap>'정규표현식 검색패턴:  '</span>"
      ],
      "text/plain": [
       "[1] \"정규표현식 검색패턴:  \""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#----------------------------------------"
     ]
    },
    {
     "data": {
      "text/html": [
       "'검색패턴을 기준으로 요소별로 분해:'"
      ],
      "text/latex": [
       "'검색패턴을 기준으로 요소별로 분해:'"
      ],
      "text/markdown": [
       "'검색패턴을 기준으로 요소별로 분해:'"
      ],
      "text/plain": [
       "[1] \"검색패턴을 기준으로 요소별로 분해:\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1]]\n",
      " [1] \"Baek\"            \"et\"              \"al.\"             \"(2014)\"         \n",
      " [5] \"argued\"          \"that\"            \"the\"             \"state\"          \n",
      " [9] \"of\"              \"default-setting\" \"is\"              \"critical\"       \n",
      "[13] \"for\"             \"people\"          \"to\"              \"protect\"        \n",
      "[17] \"their\"           \"own\"             \"personal\"        \"privacy\"        \n",
      "[21] \"on\"              \"the\"             \"Internet.\"      \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "'요소별로 분해된 세부요소 갯수:'"
      ],
      "text/latex": [
       "'요소별로 분해된 세부요소 갯수:'"
      ],
      "text/markdown": [
       "'요소별로 분해된 세부요소 갯수:'"
      ],
      "text/plain": [
       "[1] \"요소별로 분해된 세부요소 갯수:\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1]]\n",
      "[1] 23\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "'요소별로 분해된 세부요소들의 글자 갯수:'"
      ],
      "text/latex": [
       "'요소별로 분해된 세부요소들의 글자 갯수:'"
      ],
      "text/markdown": [
       "'요소별로 분해된 세부요소들의 글자 갯수:'"
      ],
      "text/plain": [
       "[1] \"요소별로 분해된 세부요소들의 글자 갯수:\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1]]\n",
      " [1]  4  2  3  6  6  4  3  5  2 15  2  8  3  6  2  7  5  3  8  7  2  3  9\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "'요소별로 분해된 세부요소들의 기술통계:'"
      ],
      "text/latex": [
       "'요소별로 분해된 세부요소들의 기술통계:'"
      ],
      "text/markdown": [
       "'요소별로 분해된 세부요소들의 기술통계:'"
      ],
      "text/plain": [
       "[1] \"요소별로 분해된 세부요소들의 기술통계:\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1]]\n",
      "\n",
      "            the          (2014)             al.          argued            Baek \n",
      "              2               1               1               1               1 \n",
      "       critical default-setting              et             for       Internet. \n",
      "              1               1               1               1               1 \n",
      "             is              of              on             own          people \n",
      "              1               1               1               1               1 \n",
      "       personal         privacy         protect           state            that \n",
      "              1               1               1               1               1 \n",
      "          their              to \n",
      "              1               1 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 해당 검색패턴으로 분해\n",
    "\n",
    "sprintf('원본문장:')\n",
    "print(my)\n",
    "sprintf('정규표현식 검색패턴: %s', src)\n",
    "cat('#----------------------------------------')\n",
    "\n",
    "sprintf('검색패턴을 기준으로 요소별로 분해:')\n",
    "str_split(string = my, pattern = src) %>% print\n",
    "# - strsplit(x = my, split = src) 동일결과\n",
    "\n",
    "sprintf('요소별로 분해된 세부요소 갯수:')\n",
    "str_split(string = my, pattern = src) %>% map(length) %>% print\n",
    "\n",
    "sprintf('요소별로 분해된 세부요소들의 글자 갯수:')\n",
    "str_split(string = my, pattern = src) %>% map(str_length) %>% print\n",
    "\n",
    "sprintf('요소별로 분해된 세부요소들의 기술통계:')\n",
    "str_split(string = my, pattern = src) %>% \n",
    "    map(table) %>% map(sort, decreasing = TRUE) %>% print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 텍스트셋 분해결과에 따른 전처리 방향설정\n",
    "* <font color = 'red'>Baek et al. (2004)</font> --> 일종의 참고문헌 표시문구를 고유명사 _reference_로 처리\n",
    "* <font color = 'red'>default-setting</font> --> 사이에 있는 하이픈기호를 기준으로 default와 setting 2개 단어로 분리\n",
    "* <font color = 'red'>Internet.</font> --> 마침표를 제외한 Internet으로 처리\n",
    "* <font color = 'blue'>(주의)</font> 3번째 항목을 1번째 보다 먼저 전처리해 버리면, \n",
    "<br>1번째 문구인 Baek et al. (2004)에서 마침표가 삭제되어 Baek et al(2004)와 같이 변경되는 결과를 초래함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 전처리 방향별 정제 작업실시 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color = 'red'>default-setting</font> --> 사이에 있는 하이픈기호를 기준으로 default와 setting 2개 단어로 분리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span style=white-space:pre-wrap>'검색패턴  를 기준으로 요소별로 분해'</span>"
      ],
      "text/latex": [
       "'검색패턴  를 기준으로 요소별로 분해'"
      ],
      "text/markdown": [
       "<span style=white-space:pre-wrap>'검색패턴  를 기준으로 요소별로 분해'</span>"
      ],
      "text/plain": [
       "[1] \"검색패턴  를 기준으로 요소별로 분해\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1]]\n",
      " [1] \"Baek\"      \"et\"        \"al.\"       \"(2014)\"    \"argued\"    \"that\"     \n",
      " [7] \"the\"       \"state\"     \"of\"        \"default\"   \"setting\"   \"is\"       \n",
      "[13] \"critical\"  \"for\"       \"people\"    \"to\"        \"protect\"   \"their\"    \n",
      "[19] \"own\"       \"personal\"  \"privacy\"   \"on\"        \"the\"       \"Internet.\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 전처리 방향별 정규표현식 설정\n",
    "org <- '-' # 원본패턴: 단어사이에 있는 하이픈기호 \n",
    "tgt <- ' ' # 목표패턴: 단순 공란 한 칸\n",
    "\n",
    "# 특정패턴에 대한 전처리 실시\n",
    "my_pp <- str_replace_all(string = my, pattern = org, replacement = tgt)\n",
    "\n",
    "sprintf('검색패턴 %s를 기준으로 요소별로 분해', src)\n",
    "str_split(string = my_pp, pattern = src) %>% print\n",
    "# - strsplit(x = my_pp, split = src) 동일결과\n",
    "\n",
    "# 전처리 결과\n",
    "# - default-setting이라는 결합단어가 사이에 있는 하이픈기호를 기준으로 default와 setting 2개 단어로 분리됨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color = 'red'>Baek et al. (2004)</font> --> 일종의 참고문헌 표시문구를 고유명사 _reference_로 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span style=white-space:pre-wrap>'검색패턴  를 기준으로 요소별로 분해'</span>"
      ],
      "text/latex": [
       "'검색패턴  를 기준으로 요소별로 분해'"
      ],
      "text/markdown": [
       "<span style=white-space:pre-wrap>'검색패턴  를 기준으로 요소별로 분해'</span>"
      ],
      "text/plain": [
       "[1] \"검색패턴  를 기준으로 요소별로 분해\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1]]\n",
      " [1] \"_reference_\" \"\"            \"argued\"      \"that\"        \"the\"        \n",
      " [6] \"state\"       \"of\"          \"default\"     \"setting\"     \"is\"         \n",
      "[11] \"critical\"    \"for\"         \"people\"      \"to\"          \"protect\"    \n",
      "[16] \"their\"       \"own\"         \"personal\"    \"privacy\"     \"on\"         \n",
      "[21] \"the\"         \"Internet.\"  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 전처리 방향별 정규표현식 설정\n",
    "org <- '[[:upper:]]{1}[[:alpha:]]{1,}[[:space:]](et al\\\\.)[[:space:]]\\\\([[:digit:]]{4}\\\\)' \n",
    "# - 원본패턴: 저자명 et al. (작성년도) 패턴을 가진 참고문헌 표시문구를 검색하는 패턴 설정 \n",
    "tgt <- '_reference_ ' \n",
    "# 목표패턴: _reference_라는 문구로 변경해 참고문헌 텍스트임을 알 수 있도록 함\n",
    "\n",
    "# 특정패턴에 대한 전처리 실시\n",
    "my_pp <- str_replace_all(string = my_pp, pattern = org, replacement = tgt)\n",
    "\n",
    "sprintf('검색패턴 %s를 기준으로 요소별로 분해', src)\n",
    "str_split(string = my_pp, pattern = src) %>% print\n",
    "# - strsplit(x = my_pp, split = src) 동일결과\n",
    "\n",
    "# 전처리 결과\n",
    "# - default-setting이라는 결합단어가 사이에 있는 하이픈기호를 기준으로 default와 setting 2개 단어로 분리됨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color = 'red'>Internet.</font> --> 마침표를 제외한 Internet으로 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span style=white-space:pre-wrap>'검색패턴  를 기준으로 요소별로 분해'</span>"
      ],
      "text/latex": [
       "'검색패턴  를 기준으로 요소별로 분해'"
      ],
      "text/markdown": [
       "<span style=white-space:pre-wrap>'검색패턴  를 기준으로 요소별로 분해'</span>"
      ],
      "text/plain": [
       "[1] \"검색패턴  를 기준으로 요소별로 분해\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1]]\n",
      " [1] \"_reference_\" \"\"            \"argued\"      \"that\"        \"the\"        \n",
      " [6] \"state\"       \"of\"          \"default\"     \"setting\"     \"is\"         \n",
      "[11] \"critical\"    \"for\"         \"people\"      \"to\"          \"protect\"    \n",
      "[16] \"their\"       \"own\"         \"personal\"    \"privacy\"     \"on\"         \n",
      "[21] \"the\"         \"Internet\"   \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 전처리 방향별 정규표현식 설정\n",
    "\n",
    "org <- '\\\\.[[:space:]]{0,}' \n",
    "# - 원본패턴: 문장과 문장사이를 구별하는 마침표(.)가 있고, 바로 다음에 공백문자가 있으면 같이 삭제하는 패턴 \n",
    "tgt <- '' \n",
    "# - 목표패턴: 아무런 문자없이 설정해 삭제토록 하는 패턴\n",
    "\n",
    "# 특정패턴에 대한 전처리 실시\n",
    "my_pp <- str_replace_all(string = my_pp, pattern = org, replacement = tgt)\n",
    "\n",
    "sprintf('검색패턴 %s를 기준으로 요소별로 분해', src)\n",
    "str_split(string = my_pp, pattern = src) %>% print\n",
    "# - strsplit(x = my_pp, split = src) 동일결과\n",
    "\n",
    "# 전처리 결과\n",
    "# - Internet.이라는 마침표(.)가 붙어 있는 단어에 마침표가 삭제됨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color = 'red'>공란 1칸</font> --> 불필요 하므로 삭제처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span style=white-space:pre-wrap>'검색패턴  를 기준으로 요소별로 분해'</span>"
      ],
      "text/latex": [
       "'검색패턴  를 기준으로 요소별로 분해'"
      ],
      "text/markdown": [
       "<span style=white-space:pre-wrap>'검색패턴  를 기준으로 요소별로 분해'</span>"
      ],
      "text/plain": [
       "[1] \"검색패턴  를 기준으로 요소별로 분해\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1]]\n",
      " [1] \"_reference_\" \"argued\"      \"that\"        \"the\"         \"state\"      \n",
      " [6] \"of\"          \"default\"     \"setting\"     \"is\"          \"critical\"   \n",
      "[11] \"for\"         \"people\"      \"to\"          \"protect\"     \"their\"      \n",
      "[16] \"own\"         \"personal\"    \"privacy\"     \"on\"          \"the\"        \n",
      "[21] \"Internet\"   \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 전처리방향별 정규표현식 설정\n",
    "org = '[[:space:]]{1,}'\n",
    "# - 원본패턴: 스페이스 공백문자에 해당하는 요소가 0개 이상 들어 있는 패턴을 모두 찾도록 설정\n",
    "tgt = ' '\n",
    "# - 목표패턴: 스페이스 공백문자 모두를 1칸짜리 공란으로 변경할 수 있도록 설정\n",
    "\n",
    "# 특정패턴에 대한 전처리 실시\n",
    "my_pp <- str_replace_all(string = my_pp, pattern = org, replacement = tgt)\n",
    "\n",
    "sprintf('검색패턴 %s를 기준으로 요소별로 분해', src)\n",
    "str_split(string = my_pp, pattern = src) %>% print\n",
    "# - strsplit(x = my_pp, split = src) 동일결과\n",
    "\n",
    "# 전처리 결과\n",
    "# - Internet.이라는 마침표(.)가 붙어 있는 단어에 마침표가 삭제됨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color = 'blue'>불용단어(stopword) 제거 (p.107)</font>\n",
    "* 텍스트셋에서 빈번하게 사용되지만 구체적인 의미를 찾기 어려운 단어들을 불용단어라고하며, 보통 삭제를 통해 전처리함\n",
    "* 영어의 경우 a, an, the 등, 한글의 경우 '~다, 것, 이, 그, 저' 등의 예가 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 간단 텍스트셋 준비\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "'원본내용:'"
      ],
      "text/latex": [
       "'원본내용:'"
      ],
      "text/markdown": [
       "'원본내용:'"
      ],
      "text/plain": [
       "[1] \"원본내용:\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"R is a programming language and free software environment for statistical computing and graphics supported by the R Foundation for Statistical Computing. The R language is widely used among statisticians and data miners for developing statistical software and data analysis. R is comparable to popular commercial statistical packages, such as SAS, SPSS, and STATA. \\nPolls, data mining surveys, and studies of scholarly literature databases show substantial increases in popularity in recent years. As of December 2018, R ranks 16th in the TIOBE index, a measure of popularity of programming languages. Choose Stat ABTest, Machine learning, Deep learning, and Text mining(Ref: librestats.com website).\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "'객체유형: character객체'"
      ],
      "text/latex": [
       "'객체유형: character객체'"
      ],
      "text/markdown": [
       "'객체유형: character객체'"
      ],
      "text/plain": [
       "[1] \"객체유형: character객체\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "'객체길이: 1요소'"
      ],
      "text/latex": [
       "'객체길이: 1요소'"
      ],
      "text/markdown": [
       "'객체길이: 1요소'"
      ],
      "text/plain": [
       "[1] \"객체길이: 1요소\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "'원본글자수: 700글자'"
      ],
      "text/latex": [
       "'원본글자수: 700글자'"
      ],
      "text/markdown": [
       "'원본글자수: 700글자'"
      ],
      "text/plain": [
       "[1] \"원본글자수: 700글자\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 분석대상 텍스트 데이터 원본 준비\n",
    "my <- 'R is a programming language and free software environment for statistical computing and graphics supported by the R Foundation for Statistical Computing. The R language is widely used among statisticians and data miners for developing statistical software and data analysis. R is comparable to popular commercial statistical packages, such as SAS, SPSS, and STATA. \n",
    "Polls, data mining surveys, and studies of scholarly literature databases show substantial increases in popularity in recent years. As of December 2018, R ranks 16th in the TIOBE index, a measure of popularity of programming languages. Choose Stat ABTest, Machine learning, Deep learning, and Text mining(Ref: librestats.com website).'\n",
    "\n",
    "sprintf('원본내용:')\n",
    "print(my)\n",
    "\n",
    "sprintf('객체유형: %s객체', class(my))\n",
    "sprintf('객체길이: %d요소', length(my))\n",
    "\n",
    "sprintf('원본글자수: %d글자', nchar(my))\n",
    "# - sprintf('검색대상 객체글자수: %d글자', str_length(my))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 단어단위로 직접 분해하는 경우\n",
    "* 보통은 <font color = 'blue'>str_split(string = 텍스트셋, pattern = <font color = 'red'>검색패턴</font>)</font>을 통해서 텍스트셋을 세부요소로 점진적으로 분해할 수 있음\n",
    "* 현재 주어진 텍스트셋에는 단어 이외에 문장부호나 특수문자 등이 섞여있는 구조라서, 이들에 대한 전처리가 필요하지만 \n",
    "<br>단어경계 옵션인 pattern = boundary('word')를 통해서 바로 텍스트셋을 단어단위로 분해할 수 있음\n",
    "* ==> <font color = 'blue'>str_extract_all(string = my, pattern = <font color = 'red'>boundary('word')</font>)</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 검색패턴이 아닌 단어경계 옵션을 이용한 텍스트셋 직접분해"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "'원본문장:'"
      ],
      "text/latex": [
       "'원본문장:'"
      ],
      "text/markdown": [
       "'원본문장:'"
      ],
      "text/plain": [
       "[1] \"원본문장:\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"R is a programming language and free software environment for statistical computing and graphics supported by the R Foundation for Statistical Computing. The R language is widely used among statisticians and data miners for developing statistical software and data analysis. R is comparable to popular commercial statistical packages, such as SAS, SPSS, and STATA. \\nPolls, data mining surveys, and studies of scholarly literature databases show substantial increases in popularity in recent years. As of December 2018, R ranks 16th in the TIOBE index, a measure of popularity of programming languages. Choose Stat ABTest, Machine learning, Deep learning, and Text mining(Ref: librestats.com website).\"\n",
      "#----------------------------------------"
     ]
    },
    {
     "data": {
      "text/html": [
       "'단어경계를 기준으로 요소별로 분해:'"
      ],
      "text/latex": [
       "'단어경계를 기준으로 요소별로 분해:'"
      ],
      "text/markdown": [
       "'단어경계를 기준으로 요소별로 분해:'"
      ],
      "text/plain": [
       "[1] \"단어경계를 기준으로 요소별로 분해:\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1]]\n",
      "  [1] \"R\"              \"is\"             \"a\"              \"programming\"   \n",
      "  [5] \"language\"       \"and\"            \"free\"           \"software\"      \n",
      "  [9] \"environment\"    \"for\"            \"statistical\"    \"computing\"     \n",
      " [13] \"and\"            \"graphics\"       \"supported\"      \"by\"            \n",
      " [17] \"the\"            \"R\"              \"Foundation\"     \"for\"           \n",
      " [21] \"Statistical\"    \"Computing\"      \"The\"            \"R\"             \n",
      " [25] \"language\"       \"is\"             \"widely\"         \"used\"          \n",
      " [29] \"among\"          \"statisticians\"  \"and\"            \"data\"          \n",
      " [33] \"miners\"         \"for\"            \"developing\"     \"statistical\"   \n",
      " [37] \"software\"       \"and\"            \"data\"           \"analysis\"      \n",
      " [41] \"R\"              \"is\"             \"comparable\"     \"to\"            \n",
      " [45] \"popular\"        \"commercial\"     \"statistical\"    \"packages\"      \n",
      " [49] \"such\"           \"as\"             \"SAS\"            \"SPSS\"          \n",
      " [53] \"and\"            \"STATA\"          \"Polls\"          \"data\"          \n",
      " [57] \"mining\"         \"surveys\"        \"and\"            \"studies\"       \n",
      " [61] \"of\"             \"scholarly\"      \"literature\"     \"databases\"     \n",
      " [65] \"show\"           \"substantial\"    \"increases\"      \"in\"            \n",
      " [69] \"popularity\"     \"in\"             \"recent\"         \"years\"         \n",
      " [73] \"As\"             \"of\"             \"December\"       \"2018\"          \n",
      " [77] \"R\"              \"ranks\"          \"16th\"           \"in\"            \n",
      " [81] \"the\"            \"TIOBE\"          \"index\"          \"a\"             \n",
      " [85] \"measure\"        \"of\"             \"popularity\"     \"of\"            \n",
      " [89] \"programming\"    \"languages\"      \"Choose\"         \"Stat\"          \n",
      " [93] \"ABTest\"         \"Machine\"        \"learning\"       \"Deep\"          \n",
      " [97] \"learning\"       \"and\"            \"Text\"           \"mining\"        \n",
      "[101] \"Ref\"            \"librestats.com\" \"website\"       \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "'요소별로 분해된 세부요소 갯수:'"
      ],
      "text/latex": [
       "'요소별로 분해된 세부요소 갯수:'"
      ],
      "text/markdown": [
       "'요소별로 분해된 세부요소 갯수:'"
      ],
      "text/plain": [
       "[1] \"요소별로 분해된 세부요소 갯수:\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1]]\n",
      "[1] 103\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "'요소별로 분해된 세부요소들의 글자 갯수:'"
      ],
      "text/latex": [
       "'요소별로 분해된 세부요소들의 글자 갯수:'"
      ],
      "text/markdown": [
       "'요소별로 분해된 세부요소들의 글자 갯수:'"
      ],
      "text/plain": [
       "[1] \"요소별로 분해된 세부요소들의 글자 갯수:\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1]]\n",
      "  [1]  1  2  1 11  8  3  4  8 11  3 11  9  3  8  9  2  3  1 10  3 11  9  3  1  8\n",
      " [26]  2  6  4  5 13  3  4  6  3 10 11  8  3  4  8  1  2 10  2  7 10 11  8  4  2\n",
      " [51]  3  4  3  5  5  4  6  7  3  7  2  9 10  9  4 11  9  2 10  2  6  5  2  2  8\n",
      " [76]  4  1  5  4  2  3  5  5  1  7  2 10  2 11  9  6  4  6  7  8  4  8  3  4  6\n",
      "[101]  3 14  7\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "'요소별로 분해된 세부요소들의 기술통계:'"
      ],
      "text/latex": [
       "'요소별로 분해된 세부요소들의 기술통계:'"
      ],
      "text/markdown": [
       "'요소별로 분해된 세부요소들의 기술통계:'"
      ],
      "text/plain": [
       "[1] \"요소별로 분해된 세부요소들의 기술통계:\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1]]\n",
      "\n",
      "           and              R             of           data            for \n",
      "             7              5              4              3              3 \n",
      "            in             is    statistical              a       language \n",
      "             3              3              3              2              2 \n",
      "      learning         mining     popularity    programming       software \n",
      "             2              2              2              2              2 \n",
      "           the           16th           2018         ABTest          among \n",
      "             2              1              1              1              1 \n",
      "      analysis             as             As             by         Choose \n",
      "             1              1              1              1              1 \n",
      "    commercial     comparable      computing      Computing      databases \n",
      "             1              1              1              1              1 \n",
      "      December           Deep     developing    environment     Foundation \n",
      "             1              1              1              1              1 \n",
      "          free       graphics      increases          index      languages \n",
      "             1              1              1              1              1 \n",
      "librestats.com     literature        Machine        measure         miners \n",
      "             1              1              1              1              1 \n",
      "      packages          Polls        popular          ranks         recent \n",
      "             1              1              1              1              1 \n",
      "           Ref            SAS      scholarly           show           SPSS \n",
      "             1              1              1              1              1 \n",
      "          Stat          STATA    Statistical  statisticians        studies \n",
      "             1              1              1              1              1 \n",
      "   substantial           such      supported        surveys           Text \n",
      "             1              1              1              1              1 \n",
      "           The          TIOBE             to           used        website \n",
      "             1              1              1              1              1 \n",
      "        widely          years \n",
      "             1              1 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 해당 검색패턴으로 분해\n",
    "\n",
    "sprintf('원본문장:')\n",
    "print(my)\n",
    "cat('#----------------------------------------')\n",
    "\n",
    "sprintf('단어경계를 기준으로 요소별로 분해:')\n",
    "str_extract_all(string = my, pattern = boundary('word')) %>% print\n",
    "\n",
    "sprintf('요소별로 분해된 세부요소 갯수:')\n",
    "str_extract_all(string = my, pattern = boundary('word')) %>% map(length) %>% print\n",
    "\n",
    "sprintf('요소별로 분해된 세부요소들의 글자 갯수:')\n",
    "str_extract_all(string = my, pattern = boundary('word')) %>% map(str_length) %>% print\n",
    "\n",
    "sprintf('요소별로 분해된 세부요소들의 기술통계:')\n",
    "str_extract_all(string = my, pattern = boundary('word')) %>% \n",
    "    map(table) %>% map(sort, decreasing = TRUE) %>% print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 텍스트셋 분해결과에 따른 전처리 방향설정\n",
    "* 분해된 단어 중에서 <font color = 'blue'>a, the, such</font>처럼 <font color = 'red'>관사</font>를 제거함\n",
    "* 분해된 단어 중에서 <font color = 'blue'>of, for, to, in, by, among</font> 등의 <font color = 'red'>전치사</font>를 제거함\n",
    "* 분해된 단어 중에서 <font color = 'blue'>and, as</font> 등의 <font color = 'red'>접속사</font>를 제거함\n",
    "* 분해된 단어 중에서 <font color = 'blue'>is</font> 등의 <font color = 'red'>동사</font>를 제거함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 전처리 방향별 정제 작업실시 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 분해된 단어 중에서 <font color = 'red'>관사, 전치사, 접속사, 동사</font>를 <font color = 'blue'>불용단어 목록</font>으로 만들어 제거함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전처리 방향별 정규표현식 설정\n",
    "\n",
    "org <- '(a)|(the)|(such)|(of)|(for)|(to)|(in)|(by)|(among)|(and)|(as)|(is)' \n",
    "# - 원본패턴: 불용단어로 간주될만한 단어목록을 검색하는 패턴설정\n",
    "tgt <- '' \n",
    "# - 목표패턴: 불용단어에 대해서 모두 삭제하는 패턴설정\n",
    "\n",
    "# ==> 영어단어 안에 있는 a라는 표현을 무조건 삭제하므로 문제가 발생함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전처리 방향별 정규표현식 설정\n",
    "\n",
    "org <- '(\\\\ba\\\\b)|(\\\\bthe\\\\b)|(\\\\bsuch\\\\b)|(\\\\bof\\\\b)|(\\\\bfor\\\\b)|\n",
    "(\\\\bto\\\\b)|(\\\\bin\\\\b)|(\\\\bby\\\\b)|(\\\\bamong\\\\b)|(\\\\band\\\\b)|(\\\\bas\\\\b)|(\\\\bis\\\\b)' \n",
    "# - 원본패턴: 불용단어로 간주될만한 단어목록을 검색하는 패턴설정\n",
    "tgt <- '' \n",
    "# - 목표패턴: 불용단어에 대해서 모두 삭제하는 패턴설정\n",
    "\n",
    "# ==> 해당 불용어 표현만 정확하게 찾는 단어경계 설정이 되었으나\n",
    "#     대문자로 시작하는 불용어의 경우 체크가 되지 못하는 문제가 발생함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전처리 방향별 정규표현식 설정\n",
    "\n",
    "org <- '(\\\\b[A|a]\\\\b)|(\\\\b[T:t]he\\\\b)|(\\\\b[S:s]uch\\\\b)|(\\\\b[O:o]f\\\\b)|(\\\\b[F:f]or\\\\b)|\n",
    "(\\\\b[T:t]o\\\\b)|(\\\\b[I:i]n\\\\b)|(\\\\b[B:b]y\\\\b)|(\\\\b[A:a]mong\\\\b)|(\\\\b[A:a]nd\\\\b)|(\\\\b[A:a]s\\\\b)|(\\\\b[I:i]s\\\\b)' \n",
    "# - 원본패턴: 불용단어로 간주될만한 단어목록을 검색하는 패턴설정\n",
    "tgt <- '' \n",
    "# - 목표패턴: 불용단어에 대해서 모두 삭제하는 패턴설정\n",
    "\n",
    "# ==> 해당 불용어 표현만 정확하게 찾는 단어경계 설정이 되었으며,\n",
    "#     대문자로 시작하는 불용어의 경우도 검색되는 패턴설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "'단어경계를 기준으로 요소별로 분해:'"
      ],
      "text/latex": [
       "'단어경계를 기준으로 요소별로 분해:'"
      ],
      "text/markdown": [
       "'단어경계를 기준으로 요소별로 분해:'"
      ],
      "text/plain": [
       "[1] \"단어경계를 기준으로 요소별로 분해:\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1]]\n",
      " [1] \"R\"              \"programming\"    \"language\"       \"free\"          \n",
      " [5] \"software\"       \"environment\"    \"statistical\"    \"computing\"     \n",
      " [9] \"graphics\"       \"supported\"      \"R\"              \"Foundation\"    \n",
      "[13] \"Statistical\"    \"Computing\"      \"R\"              \"language\"      \n",
      "[17] \"widely\"         \"used\"           \"statisticians\"  \"data\"          \n",
      "[21] \"miners\"         \"developing\"     \"statistical\"    \"software\"      \n",
      "[25] \"data\"           \"analysis\"       \"R\"              \"comparable\"    \n",
      "[29] \"to\"             \"popular\"        \"commercial\"     \"statistical\"   \n",
      "[33] \"packages\"       \"SAS\"            \"SPSS\"           \"STATA\"         \n",
      "[37] \"Polls\"          \"data\"           \"mining\"         \"surveys\"       \n",
      "[41] \"studies\"        \"scholarly\"      \"literature\"     \"databases\"     \n",
      "[45] \"show\"           \"substantial\"    \"increases\"      \"popularity\"    \n",
      "[49] \"recent\"         \"years\"          \"December\"       \"2018\"          \n",
      "[53] \"R\"              \"ranks\"          \"16th\"           \"TIOBE\"         \n",
      "[57] \"index\"          \"measure\"        \"popularity\"     \"programming\"   \n",
      "[61] \"languages\"      \"Choose\"         \"Stat\"           \"ABTest\"        \n",
      "[65] \"Machine\"        \"learning\"       \"Deep\"           \"learning\"      \n",
      "[69] \"Text\"           \"mining\"         \"Ref\"            \"librestats.com\"\n",
      "[73] \"website\"       \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "'요소별로 분해된 세부요소 갯수:'"
      ],
      "text/latex": [
       "'요소별로 분해된 세부요소 갯수:'"
      ],
      "text/markdown": [
       "'요소별로 분해된 세부요소 갯수:'"
      ],
      "text/plain": [
       "[1] \"요소별로 분해된 세부요소 갯수:\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1]]\n",
      "[1] 73\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "'요소별로 분해된 세부요소들의 글자 갯수:'"
      ],
      "text/latex": [
       "'요소별로 분해된 세부요소들의 글자 갯수:'"
      ],
      "text/markdown": [
       "'요소별로 분해된 세부요소들의 글자 갯수:'"
      ],
      "text/plain": [
       "[1] \"요소별로 분해된 세부요소들의 글자 갯수:\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1]]\n",
      " [1]  1 11  8  4  8 11 11  9  8  9  1 10 11  9  1  8  6  4 13  4  6 10 11  8  4\n",
      "[26]  8  1 10  2  7 10 11  8  3  4  5  5  4  6  7  7  9 10  9  4 11  9 10  6  5\n",
      "[51]  8  4  1  5  4  5  5  7 10 11  9  6  4  6  7  8  4  8  4  6  3 14  7\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "'요소별로 분해된 세부요소들의 기술통계:'"
      ],
      "text/latex": [
       "'요소별로 분해된 세부요소들의 기술통계:'"
      ],
      "text/markdown": [
       "'요소별로 분해된 세부요소들의 기술통계:'"
      ],
      "text/plain": [
       "[1] \"요소별로 분해된 세부요소들의 기술통계:\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1]]\n",
      "\n",
      "             R           data    statistical       language       learning \n",
      "             5              3              3              2              2 \n",
      "        mining     popularity    programming       software           16th \n",
      "             2              2              2              2              1 \n",
      "          2018         ABTest       analysis         Choose     commercial \n",
      "             1              1              1              1              1 \n",
      "    comparable      computing      Computing      databases       December \n",
      "             1              1              1              1              1 \n",
      "          Deep     developing    environment     Foundation           free \n",
      "             1              1              1              1              1 \n",
      "      graphics      increases          index      languages librestats.com \n",
      "             1              1              1              1              1 \n",
      "    literature        Machine        measure         miners       packages \n",
      "             1              1              1              1              1 \n",
      "         Polls        popular          ranks         recent            Ref \n",
      "             1              1              1              1              1 \n",
      "           SAS      scholarly           show           SPSS           Stat \n",
      "             1              1              1              1              1 \n",
      "         STATA    Statistical  statisticians        studies    substantial \n",
      "             1              1              1              1              1 \n",
      "     supported        surveys           Text          TIOBE             to \n",
      "             1              1              1              1              1 \n",
      "          used        website         widely          years \n",
      "             1              1              1              1 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 특정패턴에 대한 전처리 실시\n",
    "my_pp <- str_replace_all(string = my, pattern = org, replacement = tgt)\n",
    "\n",
    "sprintf('단어경계를 기준으로 요소별로 분해:')\n",
    "str_extract_all(string = my_pp, pattern = boundary('word')) %>% print\n",
    "\n",
    "sprintf('요소별로 분해된 세부요소 갯수:')\n",
    "str_extract_all(string = my_pp, pattern = boundary('word')) %>% map(length) %>% print\n",
    "\n",
    "sprintf('요소별로 분해된 세부요소들의 글자 갯수:')\n",
    "str_extract_all(string = my_pp, pattern = boundary('word')) %>% map(str_length) %>% print\n",
    "\n",
    "sprintf('요소별로 분해된 세부요소들의 기술통계:')\n",
    "str_extract_all(string = my_pp, pattern = boundary('word')) %>% \n",
    "    map(table) %>% map(sort, decreasing = TRUE) %>% print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 불용어 라이브러리 확인\n",
    "* 텍스트마이닝을 위한 tm패키지에는 지원가능 불용어목록으로 \n",
    "<br>danish, dutch, english, finnish, french, german, hungarian, italian, norwegian, portuguese, russian, spanish, swedish 등이 가능함\n",
    "* 영어의 경우 <font color = 'red'>en</font>과 <font color = 'red'>SMART</font>라는 이름으로 불용단어목록이 사전에 정의되어 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 텍스트마이닝을 위한 tm패키지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install.packages('tm', dependencies = TRUE, repos = 'http://R-Forge.R-project.org')\n",
    "library(tm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tm패키지의 en불용어 목록"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "'tm패키지의 en불용어 단어수: 174개'"
      ],
      "text/latex": [
       "'tm패키지의 en불용어 단어수: 174개'"
      ],
      "text/markdown": [
       "'tm패키지의 en불용어 단어수: 174개'"
      ],
      "text/plain": [
       "[1] \"tm패키지의 en불용어 단어수: 174개\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "'tm패키지의 en불용어 목록'"
      ],
      "text/latex": [
       "'tm패키지의 en불용어 목록'"
      ],
      "text/markdown": [
       "'tm패키지의 en불용어 목록'"
      ],
      "text/plain": [
       "[1] \"tm패키지의 en불용어 목록\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>'i'</li>\n",
       "\t<li>'me'</li>\n",
       "\t<li>'my'</li>\n",
       "\t<li>'myself'</li>\n",
       "\t<li>'we'</li>\n",
       "\t<li>'our'</li>\n",
       "\t<li>'ours'</li>\n",
       "\t<li>'ourselves'</li>\n",
       "\t<li>'you'</li>\n",
       "\t<li>'your'</li>\n",
       "\t<li>'yours'</li>\n",
       "\t<li>'yourself'</li>\n",
       "\t<li>'yourselves'</li>\n",
       "\t<li>'he'</li>\n",
       "\t<li>'him'</li>\n",
       "\t<li>'his'</li>\n",
       "\t<li>'himself'</li>\n",
       "\t<li>'she'</li>\n",
       "\t<li>'her'</li>\n",
       "\t<li>'hers'</li>\n",
       "\t<li>'herself'</li>\n",
       "\t<li>'it'</li>\n",
       "\t<li>'its'</li>\n",
       "\t<li>'itself'</li>\n",
       "\t<li>'they'</li>\n",
       "\t<li>'them'</li>\n",
       "\t<li>'their'</li>\n",
       "\t<li>'theirs'</li>\n",
       "\t<li>'themselves'</li>\n",
       "\t<li>'what'</li>\n",
       "\t<li>'which'</li>\n",
       "\t<li>'who'</li>\n",
       "\t<li>'whom'</li>\n",
       "\t<li>'this'</li>\n",
       "\t<li>'that'</li>\n",
       "\t<li>'these'</li>\n",
       "\t<li>'those'</li>\n",
       "\t<li>'am'</li>\n",
       "\t<li>'is'</li>\n",
       "\t<li>'are'</li>\n",
       "\t<li>'was'</li>\n",
       "\t<li>'were'</li>\n",
       "\t<li>'be'</li>\n",
       "\t<li>'been'</li>\n",
       "\t<li>'being'</li>\n",
       "\t<li>'have'</li>\n",
       "\t<li>'has'</li>\n",
       "\t<li>'had'</li>\n",
       "\t<li>'having'</li>\n",
       "\t<li>'do'</li>\n",
       "\t<li>'does'</li>\n",
       "\t<li>'did'</li>\n",
       "\t<li>'doing'</li>\n",
       "\t<li>'would'</li>\n",
       "\t<li>'should'</li>\n",
       "\t<li>'could'</li>\n",
       "\t<li>'ought'</li>\n",
       "\t<li>'i\\'m'</li>\n",
       "\t<li>'you\\'re'</li>\n",
       "\t<li>'he\\'s'</li>\n",
       "\t<li>'she\\'s'</li>\n",
       "\t<li>'it\\'s'</li>\n",
       "\t<li>'we\\'re'</li>\n",
       "\t<li>'they\\'re'</li>\n",
       "\t<li>'i\\'ve'</li>\n",
       "\t<li>'you\\'ve'</li>\n",
       "\t<li>'we\\'ve'</li>\n",
       "\t<li>'they\\'ve'</li>\n",
       "\t<li>'i\\'d'</li>\n",
       "\t<li>'you\\'d'</li>\n",
       "\t<li>'he\\'d'</li>\n",
       "\t<li>'she\\'d'</li>\n",
       "\t<li>'we\\'d'</li>\n",
       "\t<li>'they\\'d'</li>\n",
       "\t<li>'i\\'ll'</li>\n",
       "\t<li>'you\\'ll'</li>\n",
       "\t<li>'he\\'ll'</li>\n",
       "\t<li>'she\\'ll'</li>\n",
       "\t<li>'we\\'ll'</li>\n",
       "\t<li>'they\\'ll'</li>\n",
       "\t<li>'isn\\'t'</li>\n",
       "\t<li>'aren\\'t'</li>\n",
       "\t<li>'wasn\\'t'</li>\n",
       "\t<li>'weren\\'t'</li>\n",
       "\t<li>'hasn\\'t'</li>\n",
       "\t<li>'haven\\'t'</li>\n",
       "\t<li>'hadn\\'t'</li>\n",
       "\t<li>'doesn\\'t'</li>\n",
       "\t<li>'don\\'t'</li>\n",
       "\t<li>'didn\\'t'</li>\n",
       "\t<li>'won\\'t'</li>\n",
       "\t<li>'wouldn\\'t'</li>\n",
       "\t<li>'shan\\'t'</li>\n",
       "\t<li>'shouldn\\'t'</li>\n",
       "\t<li>'can\\'t'</li>\n",
       "\t<li>'cannot'</li>\n",
       "\t<li>'couldn\\'t'</li>\n",
       "\t<li>'mustn\\'t'</li>\n",
       "\t<li>'let\\'s'</li>\n",
       "\t<li>'that\\'s'</li>\n",
       "\t<li>'who\\'s'</li>\n",
       "\t<li>'what\\'s'</li>\n",
       "\t<li>'here\\'s'</li>\n",
       "\t<li>'there\\'s'</li>\n",
       "\t<li>'when\\'s'</li>\n",
       "\t<li>'where\\'s'</li>\n",
       "\t<li>'why\\'s'</li>\n",
       "\t<li>'how\\'s'</li>\n",
       "\t<li>'a'</li>\n",
       "\t<li>'an'</li>\n",
       "\t<li>'the'</li>\n",
       "\t<li>'and'</li>\n",
       "\t<li>'but'</li>\n",
       "\t<li>'if'</li>\n",
       "\t<li>'or'</li>\n",
       "\t<li>'because'</li>\n",
       "\t<li>'as'</li>\n",
       "\t<li>'until'</li>\n",
       "\t<li>'while'</li>\n",
       "\t<li>'of'</li>\n",
       "\t<li>'at'</li>\n",
       "\t<li>'by'</li>\n",
       "\t<li>'for'</li>\n",
       "\t<li>'with'</li>\n",
       "\t<li>'about'</li>\n",
       "\t<li>'against'</li>\n",
       "\t<li>'between'</li>\n",
       "\t<li>'into'</li>\n",
       "\t<li>'through'</li>\n",
       "\t<li>'during'</li>\n",
       "\t<li>'before'</li>\n",
       "\t<li>'after'</li>\n",
       "\t<li>'above'</li>\n",
       "\t<li>'below'</li>\n",
       "\t<li>'to'</li>\n",
       "\t<li>'from'</li>\n",
       "\t<li>'up'</li>\n",
       "\t<li>'down'</li>\n",
       "\t<li>'in'</li>\n",
       "\t<li>'out'</li>\n",
       "\t<li>'on'</li>\n",
       "\t<li>'off'</li>\n",
       "\t<li>'over'</li>\n",
       "\t<li>'under'</li>\n",
       "\t<li>'again'</li>\n",
       "\t<li>'further'</li>\n",
       "\t<li>'then'</li>\n",
       "\t<li>'once'</li>\n",
       "\t<li>'here'</li>\n",
       "\t<li>'there'</li>\n",
       "\t<li>'when'</li>\n",
       "\t<li>'where'</li>\n",
       "\t<li>'why'</li>\n",
       "\t<li>'how'</li>\n",
       "\t<li>'all'</li>\n",
       "\t<li>'any'</li>\n",
       "\t<li>'both'</li>\n",
       "\t<li>'each'</li>\n",
       "\t<li>'few'</li>\n",
       "\t<li>'more'</li>\n",
       "\t<li>'most'</li>\n",
       "\t<li>'other'</li>\n",
       "\t<li>'some'</li>\n",
       "\t<li>'such'</li>\n",
       "\t<li>'no'</li>\n",
       "\t<li>'nor'</li>\n",
       "\t<li>'not'</li>\n",
       "\t<li>'only'</li>\n",
       "\t<li>'own'</li>\n",
       "\t<li>'same'</li>\n",
       "\t<li>'so'</li>\n",
       "\t<li>'than'</li>\n",
       "\t<li>'too'</li>\n",
       "\t<li>'very'</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 'i'\n",
       "\\item 'me'\n",
       "\\item 'my'\n",
       "\\item 'myself'\n",
       "\\item 'we'\n",
       "\\item 'our'\n",
       "\\item 'ours'\n",
       "\\item 'ourselves'\n",
       "\\item 'you'\n",
       "\\item 'your'\n",
       "\\item 'yours'\n",
       "\\item 'yourself'\n",
       "\\item 'yourselves'\n",
       "\\item 'he'\n",
       "\\item 'him'\n",
       "\\item 'his'\n",
       "\\item 'himself'\n",
       "\\item 'she'\n",
       "\\item 'her'\n",
       "\\item 'hers'\n",
       "\\item 'herself'\n",
       "\\item 'it'\n",
       "\\item 'its'\n",
       "\\item 'itself'\n",
       "\\item 'they'\n",
       "\\item 'them'\n",
       "\\item 'their'\n",
       "\\item 'theirs'\n",
       "\\item 'themselves'\n",
       "\\item 'what'\n",
       "\\item 'which'\n",
       "\\item 'who'\n",
       "\\item 'whom'\n",
       "\\item 'this'\n",
       "\\item 'that'\n",
       "\\item 'these'\n",
       "\\item 'those'\n",
       "\\item 'am'\n",
       "\\item 'is'\n",
       "\\item 'are'\n",
       "\\item 'was'\n",
       "\\item 'were'\n",
       "\\item 'be'\n",
       "\\item 'been'\n",
       "\\item 'being'\n",
       "\\item 'have'\n",
       "\\item 'has'\n",
       "\\item 'had'\n",
       "\\item 'having'\n",
       "\\item 'do'\n",
       "\\item 'does'\n",
       "\\item 'did'\n",
       "\\item 'doing'\n",
       "\\item 'would'\n",
       "\\item 'should'\n",
       "\\item 'could'\n",
       "\\item 'ought'\n",
       "\\item 'i\\textbackslash{}'m'\n",
       "\\item 'you\\textbackslash{}'re'\n",
       "\\item 'he\\textbackslash{}'s'\n",
       "\\item 'she\\textbackslash{}'s'\n",
       "\\item 'it\\textbackslash{}'s'\n",
       "\\item 'we\\textbackslash{}'re'\n",
       "\\item 'they\\textbackslash{}'re'\n",
       "\\item 'i\\textbackslash{}'ve'\n",
       "\\item 'you\\textbackslash{}'ve'\n",
       "\\item 'we\\textbackslash{}'ve'\n",
       "\\item 'they\\textbackslash{}'ve'\n",
       "\\item 'i\\textbackslash{}'d'\n",
       "\\item 'you\\textbackslash{}'d'\n",
       "\\item 'he\\textbackslash{}'d'\n",
       "\\item 'she\\textbackslash{}'d'\n",
       "\\item 'we\\textbackslash{}'d'\n",
       "\\item 'they\\textbackslash{}'d'\n",
       "\\item 'i\\textbackslash{}'ll'\n",
       "\\item 'you\\textbackslash{}'ll'\n",
       "\\item 'he\\textbackslash{}'ll'\n",
       "\\item 'she\\textbackslash{}'ll'\n",
       "\\item 'we\\textbackslash{}'ll'\n",
       "\\item 'they\\textbackslash{}'ll'\n",
       "\\item 'isn\\textbackslash{}'t'\n",
       "\\item 'aren\\textbackslash{}'t'\n",
       "\\item 'wasn\\textbackslash{}'t'\n",
       "\\item 'weren\\textbackslash{}'t'\n",
       "\\item 'hasn\\textbackslash{}'t'\n",
       "\\item 'haven\\textbackslash{}'t'\n",
       "\\item 'hadn\\textbackslash{}'t'\n",
       "\\item 'doesn\\textbackslash{}'t'\n",
       "\\item 'don\\textbackslash{}'t'\n",
       "\\item 'didn\\textbackslash{}'t'\n",
       "\\item 'won\\textbackslash{}'t'\n",
       "\\item 'wouldn\\textbackslash{}'t'\n",
       "\\item 'shan\\textbackslash{}'t'\n",
       "\\item 'shouldn\\textbackslash{}'t'\n",
       "\\item 'can\\textbackslash{}'t'\n",
       "\\item 'cannot'\n",
       "\\item 'couldn\\textbackslash{}'t'\n",
       "\\item 'mustn\\textbackslash{}'t'\n",
       "\\item 'let\\textbackslash{}'s'\n",
       "\\item 'that\\textbackslash{}'s'\n",
       "\\item 'who\\textbackslash{}'s'\n",
       "\\item 'what\\textbackslash{}'s'\n",
       "\\item 'here\\textbackslash{}'s'\n",
       "\\item 'there\\textbackslash{}'s'\n",
       "\\item 'when\\textbackslash{}'s'\n",
       "\\item 'where\\textbackslash{}'s'\n",
       "\\item 'why\\textbackslash{}'s'\n",
       "\\item 'how\\textbackslash{}'s'\n",
       "\\item 'a'\n",
       "\\item 'an'\n",
       "\\item 'the'\n",
       "\\item 'and'\n",
       "\\item 'but'\n",
       "\\item 'if'\n",
       "\\item 'or'\n",
       "\\item 'because'\n",
       "\\item 'as'\n",
       "\\item 'until'\n",
       "\\item 'while'\n",
       "\\item 'of'\n",
       "\\item 'at'\n",
       "\\item 'by'\n",
       "\\item 'for'\n",
       "\\item 'with'\n",
       "\\item 'about'\n",
       "\\item 'against'\n",
       "\\item 'between'\n",
       "\\item 'into'\n",
       "\\item 'through'\n",
       "\\item 'during'\n",
       "\\item 'before'\n",
       "\\item 'after'\n",
       "\\item 'above'\n",
       "\\item 'below'\n",
       "\\item 'to'\n",
       "\\item 'from'\n",
       "\\item 'up'\n",
       "\\item 'down'\n",
       "\\item 'in'\n",
       "\\item 'out'\n",
       "\\item 'on'\n",
       "\\item 'off'\n",
       "\\item 'over'\n",
       "\\item 'under'\n",
       "\\item 'again'\n",
       "\\item 'further'\n",
       "\\item 'then'\n",
       "\\item 'once'\n",
       "\\item 'here'\n",
       "\\item 'there'\n",
       "\\item 'when'\n",
       "\\item 'where'\n",
       "\\item 'why'\n",
       "\\item 'how'\n",
       "\\item 'all'\n",
       "\\item 'any'\n",
       "\\item 'both'\n",
       "\\item 'each'\n",
       "\\item 'few'\n",
       "\\item 'more'\n",
       "\\item 'most'\n",
       "\\item 'other'\n",
       "\\item 'some'\n",
       "\\item 'such'\n",
       "\\item 'no'\n",
       "\\item 'nor'\n",
       "\\item 'not'\n",
       "\\item 'only'\n",
       "\\item 'own'\n",
       "\\item 'same'\n",
       "\\item 'so'\n",
       "\\item 'than'\n",
       "\\item 'too'\n",
       "\\item 'very'\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 'i'\n",
       "2. 'me'\n",
       "3. 'my'\n",
       "4. 'myself'\n",
       "5. 'we'\n",
       "6. 'our'\n",
       "7. 'ours'\n",
       "8. 'ourselves'\n",
       "9. 'you'\n",
       "10. 'your'\n",
       "11. 'yours'\n",
       "12. 'yourself'\n",
       "13. 'yourselves'\n",
       "14. 'he'\n",
       "15. 'him'\n",
       "16. 'his'\n",
       "17. 'himself'\n",
       "18. 'she'\n",
       "19. 'her'\n",
       "20. 'hers'\n",
       "21. 'herself'\n",
       "22. 'it'\n",
       "23. 'its'\n",
       "24. 'itself'\n",
       "25. 'they'\n",
       "26. 'them'\n",
       "27. 'their'\n",
       "28. 'theirs'\n",
       "29. 'themselves'\n",
       "30. 'what'\n",
       "31. 'which'\n",
       "32. 'who'\n",
       "33. 'whom'\n",
       "34. 'this'\n",
       "35. 'that'\n",
       "36. 'these'\n",
       "37. 'those'\n",
       "38. 'am'\n",
       "39. 'is'\n",
       "40. 'are'\n",
       "41. 'was'\n",
       "42. 'were'\n",
       "43. 'be'\n",
       "44. 'been'\n",
       "45. 'being'\n",
       "46. 'have'\n",
       "47. 'has'\n",
       "48. 'had'\n",
       "49. 'having'\n",
       "50. 'do'\n",
       "51. 'does'\n",
       "52. 'did'\n",
       "53. 'doing'\n",
       "54. 'would'\n",
       "55. 'should'\n",
       "56. 'could'\n",
       "57. 'ought'\n",
       "58. 'i\\'m'\n",
       "59. 'you\\'re'\n",
       "60. 'he\\'s'\n",
       "61. 'she\\'s'\n",
       "62. 'it\\'s'\n",
       "63. 'we\\'re'\n",
       "64. 'they\\'re'\n",
       "65. 'i\\'ve'\n",
       "66. 'you\\'ve'\n",
       "67. 'we\\'ve'\n",
       "68. 'they\\'ve'\n",
       "69. 'i\\'d'\n",
       "70. 'you\\'d'\n",
       "71. 'he\\'d'\n",
       "72. 'she\\'d'\n",
       "73. 'we\\'d'\n",
       "74. 'they\\'d'\n",
       "75. 'i\\'ll'\n",
       "76. 'you\\'ll'\n",
       "77. 'he\\'ll'\n",
       "78. 'she\\'ll'\n",
       "79. 'we\\'ll'\n",
       "80. 'they\\'ll'\n",
       "81. 'isn\\'t'\n",
       "82. 'aren\\'t'\n",
       "83. 'wasn\\'t'\n",
       "84. 'weren\\'t'\n",
       "85. 'hasn\\'t'\n",
       "86. 'haven\\'t'\n",
       "87. 'hadn\\'t'\n",
       "88. 'doesn\\'t'\n",
       "89. 'don\\'t'\n",
       "90. 'didn\\'t'\n",
       "91. 'won\\'t'\n",
       "92. 'wouldn\\'t'\n",
       "93. 'shan\\'t'\n",
       "94. 'shouldn\\'t'\n",
       "95. 'can\\'t'\n",
       "96. 'cannot'\n",
       "97. 'couldn\\'t'\n",
       "98. 'mustn\\'t'\n",
       "99. 'let\\'s'\n",
       "100. 'that\\'s'\n",
       "101. 'who\\'s'\n",
       "102. 'what\\'s'\n",
       "103. 'here\\'s'\n",
       "104. 'there\\'s'\n",
       "105. 'when\\'s'\n",
       "106. 'where\\'s'\n",
       "107. 'why\\'s'\n",
       "108. 'how\\'s'\n",
       "109. 'a'\n",
       "110. 'an'\n",
       "111. 'the'\n",
       "112. 'and'\n",
       "113. 'but'\n",
       "114. 'if'\n",
       "115. 'or'\n",
       "116. 'because'\n",
       "117. 'as'\n",
       "118. 'until'\n",
       "119. 'while'\n",
       "120. 'of'\n",
       "121. 'at'\n",
       "122. 'by'\n",
       "123. 'for'\n",
       "124. 'with'\n",
       "125. 'about'\n",
       "126. 'against'\n",
       "127. 'between'\n",
       "128. 'into'\n",
       "129. 'through'\n",
       "130. 'during'\n",
       "131. 'before'\n",
       "132. 'after'\n",
       "133. 'above'\n",
       "134. 'below'\n",
       "135. 'to'\n",
       "136. 'from'\n",
       "137. 'up'\n",
       "138. 'down'\n",
       "139. 'in'\n",
       "140. 'out'\n",
       "141. 'on'\n",
       "142. 'off'\n",
       "143. 'over'\n",
       "144. 'under'\n",
       "145. 'again'\n",
       "146. 'further'\n",
       "147. 'then'\n",
       "148. 'once'\n",
       "149. 'here'\n",
       "150. 'there'\n",
       "151. 'when'\n",
       "152. 'where'\n",
       "153. 'why'\n",
       "154. 'how'\n",
       "155. 'all'\n",
       "156. 'any'\n",
       "157. 'both'\n",
       "158. 'each'\n",
       "159. 'few'\n",
       "160. 'more'\n",
       "161. 'most'\n",
       "162. 'other'\n",
       "163. 'some'\n",
       "164. 'such'\n",
       "165. 'no'\n",
       "166. 'nor'\n",
       "167. 'not'\n",
       "168. 'only'\n",
       "169. 'own'\n",
       "170. 'same'\n",
       "171. 'so'\n",
       "172. 'than'\n",
       "173. 'too'\n",
       "174. 'very'\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "  [1] \"i\"          \"me\"         \"my\"         \"myself\"     \"we\"        \n",
       "  [6] \"our\"        \"ours\"       \"ourselves\"  \"you\"        \"your\"      \n",
       " [11] \"yours\"      \"yourself\"   \"yourselves\" \"he\"         \"him\"       \n",
       " [16] \"his\"        \"himself\"    \"she\"        \"her\"        \"hers\"      \n",
       " [21] \"herself\"    \"it\"         \"its\"        \"itself\"     \"they\"      \n",
       " [26] \"them\"       \"their\"      \"theirs\"     \"themselves\" \"what\"      \n",
       " [31] \"which\"      \"who\"        \"whom\"       \"this\"       \"that\"      \n",
       " [36] \"these\"      \"those\"      \"am\"         \"is\"         \"are\"       \n",
       " [41] \"was\"        \"were\"       \"be\"         \"been\"       \"being\"     \n",
       " [46] \"have\"       \"has\"        \"had\"        \"having\"     \"do\"        \n",
       " [51] \"does\"       \"did\"        \"doing\"      \"would\"      \"should\"    \n",
       " [56] \"could\"      \"ought\"      \"i'm\"        \"you're\"     \"he's\"      \n",
       " [61] \"she's\"      \"it's\"       \"we're\"      \"they're\"    \"i've\"      \n",
       " [66] \"you've\"     \"we've\"      \"they've\"    \"i'd\"        \"you'd\"     \n",
       " [71] \"he'd\"       \"she'd\"      \"we'd\"       \"they'd\"     \"i'll\"      \n",
       " [76] \"you'll\"     \"he'll\"      \"she'll\"     \"we'll\"      \"they'll\"   \n",
       " [81] \"isn't\"      \"aren't\"     \"wasn't\"     \"weren't\"    \"hasn't\"    \n",
       " [86] \"haven't\"    \"hadn't\"     \"doesn't\"    \"don't\"      \"didn't\"    \n",
       " [91] \"won't\"      \"wouldn't\"   \"shan't\"     \"shouldn't\"  \"can't\"     \n",
       " [96] \"cannot\"     \"couldn't\"   \"mustn't\"    \"let's\"      \"that's\"    \n",
       "[101] \"who's\"      \"what's\"     \"here's\"     \"there's\"    \"when's\"    \n",
       "[106] \"where's\"    \"why's\"      \"how's\"      \"a\"          \"an\"        \n",
       "[111] \"the\"        \"and\"        \"but\"        \"if\"         \"or\"        \n",
       "[116] \"because\"    \"as\"         \"until\"      \"while\"      \"of\"        \n",
       "[121] \"at\"         \"by\"         \"for\"        \"with\"       \"about\"     \n",
       "[126] \"against\"    \"between\"    \"into\"       \"through\"    \"during\"    \n",
       "[131] \"before\"     \"after\"      \"above\"      \"below\"      \"to\"        \n",
       "[136] \"from\"       \"up\"         \"down\"       \"in\"         \"out\"       \n",
       "[141] \"on\"         \"off\"        \"over\"       \"under\"      \"again\"     \n",
       "[146] \"further\"    \"then\"       \"once\"       \"here\"       \"there\"     \n",
       "[151] \"when\"       \"where\"      \"why\"        \"how\"        \"all\"       \n",
       "[156] \"any\"        \"both\"       \"each\"       \"few\"        \"more\"      \n",
       "[161] \"most\"       \"other\"      \"some\"       \"such\"       \"no\"        \n",
       "[166] \"nor\"        \"not\"        \"only\"       \"own\"        \"same\"      \n",
       "[171] \"so\"         \"than\"       \"too\"        \"very\"      "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sprintf('tm패키지의 en불용어 단어수: %d개', length(stopwords('en')))\n",
    "sprintf('tm패키지의 en불용어 목록')\n",
    "stopwords(kind = 'en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tm패키지의 SMART불용어 목록"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "'tm패키지의 SMART불용어 단어수: 571개'"
      ],
      "text/latex": [
       "'tm패키지의 SMART불용어 단어수: 571개'"
      ],
      "text/markdown": [
       "'tm패키지의 SMART불용어 단어수: 571개'"
      ],
      "text/plain": [
       "[1] \"tm패키지의 SMART불용어 단어수: 571개\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "'tm패키지의 SMART불용어 목록'"
      ],
      "text/latex": [
       "'tm패키지의 SMART불용어 목록'"
      ],
      "text/markdown": [
       "'tm패키지의 SMART불용어 목록'"
      ],
      "text/plain": [
       "[1] \"tm패키지의 SMART불용어 목록\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>'a'</li>\n",
       "\t<li>'a\\'s'</li>\n",
       "\t<li>'able'</li>\n",
       "\t<li>'about'</li>\n",
       "\t<li>'above'</li>\n",
       "\t<li>'according'</li>\n",
       "\t<li>'accordingly'</li>\n",
       "\t<li>'across'</li>\n",
       "\t<li>'actually'</li>\n",
       "\t<li>'after'</li>\n",
       "\t<li>'afterwards'</li>\n",
       "\t<li>'again'</li>\n",
       "\t<li>'against'</li>\n",
       "\t<li>'ain\\'t'</li>\n",
       "\t<li>'all'</li>\n",
       "\t<li>'allow'</li>\n",
       "\t<li>'allows'</li>\n",
       "\t<li>'almost'</li>\n",
       "\t<li>'alone'</li>\n",
       "\t<li>'along'</li>\n",
       "\t<li>'already'</li>\n",
       "\t<li>'also'</li>\n",
       "\t<li>'although'</li>\n",
       "\t<li>'always'</li>\n",
       "\t<li>'am'</li>\n",
       "\t<li>'among'</li>\n",
       "\t<li>'amongst'</li>\n",
       "\t<li>'an'</li>\n",
       "\t<li>'and'</li>\n",
       "\t<li>'another'</li>\n",
       "\t<li>'any'</li>\n",
       "\t<li>'anybody'</li>\n",
       "\t<li>'anyhow'</li>\n",
       "\t<li>'anyone'</li>\n",
       "\t<li>'anything'</li>\n",
       "\t<li>'anyway'</li>\n",
       "\t<li>'anyways'</li>\n",
       "\t<li>'anywhere'</li>\n",
       "\t<li>'apart'</li>\n",
       "\t<li>'appear'</li>\n",
       "\t<li>'appreciate'</li>\n",
       "\t<li>'appropriate'</li>\n",
       "\t<li>'are'</li>\n",
       "\t<li>'aren\\'t'</li>\n",
       "\t<li>'around'</li>\n",
       "\t<li>'as'</li>\n",
       "\t<li>'aside'</li>\n",
       "\t<li>'ask'</li>\n",
       "\t<li>'asking'</li>\n",
       "\t<li>'associated'</li>\n",
       "\t<li>'at'</li>\n",
       "\t<li>'available'</li>\n",
       "\t<li>'away'</li>\n",
       "\t<li>'awfully'</li>\n",
       "\t<li>'b'</li>\n",
       "\t<li>'be'</li>\n",
       "\t<li>'became'</li>\n",
       "\t<li>'because'</li>\n",
       "\t<li>'become'</li>\n",
       "\t<li>'becomes'</li>\n",
       "\t<li>'becoming'</li>\n",
       "\t<li>'been'</li>\n",
       "\t<li>'before'</li>\n",
       "\t<li>'beforehand'</li>\n",
       "\t<li>'behind'</li>\n",
       "\t<li>'being'</li>\n",
       "\t<li>'believe'</li>\n",
       "\t<li>'below'</li>\n",
       "\t<li>'beside'</li>\n",
       "\t<li>'besides'</li>\n",
       "\t<li>'best'</li>\n",
       "\t<li>'better'</li>\n",
       "\t<li>'between'</li>\n",
       "\t<li>'beyond'</li>\n",
       "\t<li>'both'</li>\n",
       "\t<li>'brief'</li>\n",
       "\t<li>'but'</li>\n",
       "\t<li>'by'</li>\n",
       "\t<li>'c'</li>\n",
       "\t<li>'c\\'mon'</li>\n",
       "\t<li>'c\\'s'</li>\n",
       "\t<li>'came'</li>\n",
       "\t<li>'can'</li>\n",
       "\t<li>'can\\'t'</li>\n",
       "\t<li>'cannot'</li>\n",
       "\t<li>'cant'</li>\n",
       "\t<li>'cause'</li>\n",
       "\t<li>'causes'</li>\n",
       "\t<li>'certain'</li>\n",
       "\t<li>'certainly'</li>\n",
       "\t<li>'changes'</li>\n",
       "\t<li>'clearly'</li>\n",
       "\t<li>'co'</li>\n",
       "\t<li>'com'</li>\n",
       "\t<li>'come'</li>\n",
       "\t<li>'comes'</li>\n",
       "\t<li>'concerning'</li>\n",
       "\t<li>'consequently'</li>\n",
       "\t<li>'consider'</li>\n",
       "\t<li>'considering'</li>\n",
       "\t<li>'contain'</li>\n",
       "\t<li>'containing'</li>\n",
       "\t<li>'contains'</li>\n",
       "\t<li>'corresponding'</li>\n",
       "\t<li>'could'</li>\n",
       "\t<li>'couldn\\'t'</li>\n",
       "\t<li>'course'</li>\n",
       "\t<li>'currently'</li>\n",
       "\t<li>'d'</li>\n",
       "\t<li>'definitely'</li>\n",
       "\t<li>'described'</li>\n",
       "\t<li>'despite'</li>\n",
       "\t<li>'did'</li>\n",
       "\t<li>'didn\\'t'</li>\n",
       "\t<li>'different'</li>\n",
       "\t<li>'do'</li>\n",
       "\t<li>'does'</li>\n",
       "\t<li>'doesn\\'t'</li>\n",
       "\t<li>'doing'</li>\n",
       "\t<li>'don\\'t'</li>\n",
       "\t<li>'done'</li>\n",
       "\t<li>'down'</li>\n",
       "\t<li>'downwards'</li>\n",
       "\t<li>'during'</li>\n",
       "\t<li>'e'</li>\n",
       "\t<li>'each'</li>\n",
       "\t<li>'edu'</li>\n",
       "\t<li>'eg'</li>\n",
       "\t<li>'eight'</li>\n",
       "\t<li>'either'</li>\n",
       "\t<li>'else'</li>\n",
       "\t<li>'elsewhere'</li>\n",
       "\t<li>'enough'</li>\n",
       "\t<li>'entirely'</li>\n",
       "\t<li>'especially'</li>\n",
       "\t<li>'et'</li>\n",
       "\t<li>'etc'</li>\n",
       "\t<li>'even'</li>\n",
       "\t<li>'ever'</li>\n",
       "\t<li>'every'</li>\n",
       "\t<li>'everybody'</li>\n",
       "\t<li>'everyone'</li>\n",
       "\t<li>'everything'</li>\n",
       "\t<li>'everywhere'</li>\n",
       "\t<li>'ex'</li>\n",
       "\t<li>'exactly'</li>\n",
       "\t<li>'example'</li>\n",
       "\t<li>'except'</li>\n",
       "\t<li>'f'</li>\n",
       "\t<li>'far'</li>\n",
       "\t<li>'few'</li>\n",
       "\t<li>'fifth'</li>\n",
       "\t<li>'first'</li>\n",
       "\t<li>'five'</li>\n",
       "\t<li>'followed'</li>\n",
       "\t<li>'following'</li>\n",
       "\t<li>'follows'</li>\n",
       "\t<li>'for'</li>\n",
       "\t<li>'former'</li>\n",
       "\t<li>'formerly'</li>\n",
       "\t<li>'forth'</li>\n",
       "\t<li>'four'</li>\n",
       "\t<li>'from'</li>\n",
       "\t<li>'further'</li>\n",
       "\t<li>'furthermore'</li>\n",
       "\t<li>'g'</li>\n",
       "\t<li>'get'</li>\n",
       "\t<li>'gets'</li>\n",
       "\t<li>'getting'</li>\n",
       "\t<li>'given'</li>\n",
       "\t<li>'gives'</li>\n",
       "\t<li>'go'</li>\n",
       "\t<li>'goes'</li>\n",
       "\t<li>'going'</li>\n",
       "\t<li>'gone'</li>\n",
       "\t<li>'got'</li>\n",
       "\t<li>'gotten'</li>\n",
       "\t<li>'greetings'</li>\n",
       "\t<li>'h'</li>\n",
       "\t<li>'had'</li>\n",
       "\t<li>'hadn\\'t'</li>\n",
       "\t<li>'happens'</li>\n",
       "\t<li>'hardly'</li>\n",
       "\t<li>'has'</li>\n",
       "\t<li>'hasn\\'t'</li>\n",
       "\t<li>'have'</li>\n",
       "\t<li>'haven\\'t'</li>\n",
       "\t<li>'having'</li>\n",
       "\t<li>'he'</li>\n",
       "\t<li>'he\\'s'</li>\n",
       "\t<li>'hello'</li>\n",
       "\t<li>'help'</li>\n",
       "\t<li>'hence'</li>\n",
       "\t<li>'her'</li>\n",
       "\t<li>'here'</li>\n",
       "\t<li>'here\\'s'</li>\n",
       "\t<li>'hereafter'</li>\n",
       "\t<li>'hereby'</li>\n",
       "\t<li>'herein'</li>\n",
       "\t<li>'hereupon'</li>\n",
       "\t<li>'hers'</li>\n",
       "\t<li>'herself'</li>\n",
       "\t<li>'hi'</li>\n",
       "\t<li>'him'</li>\n",
       "\t<li>'himself'</li>\n",
       "\t<li>'his'</li>\n",
       "\t<li>'hither'</li>\n",
       "\t<li>'hopefully'</li>\n",
       "\t<li>'how'</li>\n",
       "\t<li>'howbeit'</li>\n",
       "\t<li>'however'</li>\n",
       "\t<li>'i'</li>\n",
       "\t<li>'i\\'d'</li>\n",
       "\t<li>'i\\'ll'</li>\n",
       "\t<li>'i\\'m'</li>\n",
       "\t<li>'i\\'ve'</li>\n",
       "\t<li>'ie'</li>\n",
       "\t<li>'if'</li>\n",
       "\t<li>'ignored'</li>\n",
       "\t<li>'immediate'</li>\n",
       "\t<li>'in'</li>\n",
       "\t<li>'inasmuch'</li>\n",
       "\t<li>'inc'</li>\n",
       "\t<li>'indeed'</li>\n",
       "\t<li>'indicate'</li>\n",
       "\t<li>'indicated'</li>\n",
       "\t<li>'indicates'</li>\n",
       "\t<li>'inner'</li>\n",
       "\t<li>'insofar'</li>\n",
       "\t<li>'instead'</li>\n",
       "\t<li>'into'</li>\n",
       "\t<li>'inward'</li>\n",
       "\t<li>'is'</li>\n",
       "\t<li>'isn\\'t'</li>\n",
       "\t<li>'it'</li>\n",
       "\t<li>'it\\'d'</li>\n",
       "\t<li>'it\\'ll'</li>\n",
       "\t<li>'it\\'s'</li>\n",
       "\t<li>'its'</li>\n",
       "\t<li>'itself'</li>\n",
       "\t<li>'j'</li>\n",
       "\t<li>'just'</li>\n",
       "\t<li>'k'</li>\n",
       "\t<li>'keep'</li>\n",
       "\t<li>'keeps'</li>\n",
       "\t<li>'kept'</li>\n",
       "\t<li>'know'</li>\n",
       "\t<li>'knows'</li>\n",
       "\t<li>'known'</li>\n",
       "\t<li>'l'</li>\n",
       "\t<li>'last'</li>\n",
       "\t<li>'lately'</li>\n",
       "\t<li>'later'</li>\n",
       "\t<li>'latter'</li>\n",
       "\t<li>'latterly'</li>\n",
       "\t<li>'least'</li>\n",
       "\t<li>'less'</li>\n",
       "\t<li>'lest'</li>\n",
       "\t<li>'let'</li>\n",
       "\t<li>'let\\'s'</li>\n",
       "\t<li>'like'</li>\n",
       "\t<li>'liked'</li>\n",
       "\t<li>'likely'</li>\n",
       "\t<li>'little'</li>\n",
       "\t<li>'look'</li>\n",
       "\t<li>'looking'</li>\n",
       "\t<li>'looks'</li>\n",
       "\t<li>'ltd'</li>\n",
       "\t<li>'m'</li>\n",
       "\t<li>'mainly'</li>\n",
       "\t<li>'many'</li>\n",
       "\t<li>'may'</li>\n",
       "\t<li>'maybe'</li>\n",
       "\t<li>'me'</li>\n",
       "\t<li>'mean'</li>\n",
       "\t<li>'meanwhile'</li>\n",
       "\t<li>'merely'</li>\n",
       "\t<li>'might'</li>\n",
       "\t<li>'more'</li>\n",
       "\t<li>'moreover'</li>\n",
       "\t<li>'most'</li>\n",
       "\t<li>'mostly'</li>\n",
       "\t<li>'much'</li>\n",
       "\t<li>'must'</li>\n",
       "\t<li>'my'</li>\n",
       "\t<li>'myself'</li>\n",
       "\t<li>'n'</li>\n",
       "\t<li>'name'</li>\n",
       "\t<li>'namely'</li>\n",
       "\t<li>'nd'</li>\n",
       "\t<li>'near'</li>\n",
       "\t<li>'nearly'</li>\n",
       "\t<li>'necessary'</li>\n",
       "\t<li>'need'</li>\n",
       "\t<li>'needs'</li>\n",
       "\t<li>'neither'</li>\n",
       "\t<li>'never'</li>\n",
       "\t<li>'nevertheless'</li>\n",
       "\t<li>'new'</li>\n",
       "\t<li>'next'</li>\n",
       "\t<li>'nine'</li>\n",
       "\t<li>'no'</li>\n",
       "\t<li>'nobody'</li>\n",
       "\t<li>'non'</li>\n",
       "\t<li>'none'</li>\n",
       "\t<li>'noone'</li>\n",
       "\t<li>'nor'</li>\n",
       "\t<li>'normally'</li>\n",
       "\t<li>'not'</li>\n",
       "\t<li>'nothing'</li>\n",
       "\t<li>'novel'</li>\n",
       "\t<li>'now'</li>\n",
       "\t<li>'nowhere'</li>\n",
       "\t<li>'o'</li>\n",
       "\t<li>'obviously'</li>\n",
       "\t<li>'of'</li>\n",
       "\t<li>'off'</li>\n",
       "\t<li>'often'</li>\n",
       "\t<li>'oh'</li>\n",
       "\t<li>'ok'</li>\n",
       "\t<li>'okay'</li>\n",
       "\t<li>'old'</li>\n",
       "\t<li>'on'</li>\n",
       "\t<li>'once'</li>\n",
       "\t<li>'one'</li>\n",
       "\t<li>'ones'</li>\n",
       "\t<li>'only'</li>\n",
       "\t<li>'onto'</li>\n",
       "\t<li>'or'</li>\n",
       "\t<li>'other'</li>\n",
       "\t<li>'others'</li>\n",
       "\t<li>'otherwise'</li>\n",
       "\t<li>'ought'</li>\n",
       "\t<li>'our'</li>\n",
       "\t<li>'ours'</li>\n",
       "\t<li>'ourselves'</li>\n",
       "\t<li>'out'</li>\n",
       "\t<li>'outside'</li>\n",
       "\t<li>'over'</li>\n",
       "\t<li>'overall'</li>\n",
       "\t<li>'own'</li>\n",
       "\t<li>'p'</li>\n",
       "\t<li>'particular'</li>\n",
       "\t<li>'particularly'</li>\n",
       "\t<li>'per'</li>\n",
       "\t<li>'perhaps'</li>\n",
       "\t<li>'placed'</li>\n",
       "\t<li>'please'</li>\n",
       "\t<li>'plus'</li>\n",
       "\t<li>'possible'</li>\n",
       "\t<li>'presumably'</li>\n",
       "\t<li>'probably'</li>\n",
       "\t<li>'provides'</li>\n",
       "\t<li>'q'</li>\n",
       "\t<li>'que'</li>\n",
       "\t<li>'quite'</li>\n",
       "\t<li>'qv'</li>\n",
       "\t<li>'r'</li>\n",
       "\t<li>'rather'</li>\n",
       "\t<li>'rd'</li>\n",
       "\t<li>'re'</li>\n",
       "\t<li>'really'</li>\n",
       "\t<li>'reasonably'</li>\n",
       "\t<li>'regarding'</li>\n",
       "\t<li>'regardless'</li>\n",
       "\t<li>'regards'</li>\n",
       "\t<li>'relatively'</li>\n",
       "\t<li>'respectively'</li>\n",
       "\t<li>'right'</li>\n",
       "\t<li>'s'</li>\n",
       "\t<li>'said'</li>\n",
       "\t<li>'same'</li>\n",
       "\t<li>'saw'</li>\n",
       "\t<li>'say'</li>\n",
       "\t<li>'saying'</li>\n",
       "\t<li>'says'</li>\n",
       "\t<li>'second'</li>\n",
       "\t<li>'secondly'</li>\n",
       "\t<li>'see'</li>\n",
       "\t<li>'seeing'</li>\n",
       "\t<li>'seem'</li>\n",
       "\t<li>'seemed'</li>\n",
       "\t<li>'seeming'</li>\n",
       "\t<li>'seems'</li>\n",
       "\t<li>'seen'</li>\n",
       "\t<li>'self'</li>\n",
       "\t<li>'selves'</li>\n",
       "\t<li>'sensible'</li>\n",
       "\t<li>'sent'</li>\n",
       "\t<li>'serious'</li>\n",
       "\t<li>'seriously'</li>\n",
       "\t<li>'seven'</li>\n",
       "\t<li>'several'</li>\n",
       "\t<li>'shall'</li>\n",
       "\t<li>'she'</li>\n",
       "\t<li>'should'</li>\n",
       "\t<li>'shouldn\\'t'</li>\n",
       "\t<li>'since'</li>\n",
       "\t<li>'six'</li>\n",
       "\t<li>'so'</li>\n",
       "\t<li>'some'</li>\n",
       "\t<li>'somebody'</li>\n",
       "\t<li>'somehow'</li>\n",
       "\t<li>'someone'</li>\n",
       "\t<li>'something'</li>\n",
       "\t<li>'sometime'</li>\n",
       "\t<li>'sometimes'</li>\n",
       "\t<li>'somewhat'</li>\n",
       "\t<li>'somewhere'</li>\n",
       "\t<li>'soon'</li>\n",
       "\t<li>'sorry'</li>\n",
       "\t<li>'specified'</li>\n",
       "\t<li>'specify'</li>\n",
       "\t<li>'specifying'</li>\n",
       "\t<li>'still'</li>\n",
       "\t<li>'sub'</li>\n",
       "\t<li>'such'</li>\n",
       "\t<li>'sup'</li>\n",
       "\t<li>'sure'</li>\n",
       "\t<li>'t'</li>\n",
       "\t<li>'t\\'s'</li>\n",
       "\t<li>'take'</li>\n",
       "\t<li>'taken'</li>\n",
       "\t<li>'tell'</li>\n",
       "\t<li>'tends'</li>\n",
       "\t<li>'th'</li>\n",
       "\t<li>'than'</li>\n",
       "\t<li>'thank'</li>\n",
       "\t<li>'thanks'</li>\n",
       "\t<li>'thanx'</li>\n",
       "\t<li>'that'</li>\n",
       "\t<li>'that\\'s'</li>\n",
       "\t<li>'thats'</li>\n",
       "\t<li>'the'</li>\n",
       "\t<li>'their'</li>\n",
       "\t<li>'theirs'</li>\n",
       "\t<li>'them'</li>\n",
       "\t<li>'themselves'</li>\n",
       "\t<li>'then'</li>\n",
       "\t<li>'thence'</li>\n",
       "\t<li>'there'</li>\n",
       "\t<li>'there\\'s'</li>\n",
       "\t<li>'thereafter'</li>\n",
       "\t<li>'thereby'</li>\n",
       "\t<li>'therefore'</li>\n",
       "\t<li>'therein'</li>\n",
       "\t<li>'theres'</li>\n",
       "\t<li>'thereupon'</li>\n",
       "\t<li>'these'</li>\n",
       "\t<li>'they'</li>\n",
       "\t<li>'they\\'d'</li>\n",
       "\t<li>'they\\'ll'</li>\n",
       "\t<li>'they\\'re'</li>\n",
       "\t<li>'they\\'ve'</li>\n",
       "\t<li>'think'</li>\n",
       "\t<li>'third'</li>\n",
       "\t<li>'this'</li>\n",
       "\t<li>'thorough'</li>\n",
       "\t<li>'thoroughly'</li>\n",
       "\t<li>'those'</li>\n",
       "\t<li>'though'</li>\n",
       "\t<li>'three'</li>\n",
       "\t<li>'through'</li>\n",
       "\t<li>'throughout'</li>\n",
       "\t<li>'thru'</li>\n",
       "\t<li>'thus'</li>\n",
       "\t<li>'to'</li>\n",
       "\t<li>'together'</li>\n",
       "\t<li>'too'</li>\n",
       "\t<li>'took'</li>\n",
       "\t<li>'toward'</li>\n",
       "\t<li>'towards'</li>\n",
       "\t<li>'tried'</li>\n",
       "\t<li>'tries'</li>\n",
       "\t<li>'truly'</li>\n",
       "\t<li>'try'</li>\n",
       "\t<li>'trying'</li>\n",
       "\t<li>'twice'</li>\n",
       "\t<li>'two'</li>\n",
       "\t<li>'u'</li>\n",
       "\t<li>'un'</li>\n",
       "\t<li>'under'</li>\n",
       "\t<li>'unfortunately'</li>\n",
       "\t<li>'unless'</li>\n",
       "\t<li>'unlikely'</li>\n",
       "\t<li>'until'</li>\n",
       "\t<li>'unto'</li>\n",
       "\t<li>'up'</li>\n",
       "\t<li>'upon'</li>\n",
       "\t<li>'us'</li>\n",
       "\t<li>'use'</li>\n",
       "\t<li>'used'</li>\n",
       "\t<li>'useful'</li>\n",
       "\t<li>'uses'</li>\n",
       "\t<li>'using'</li>\n",
       "\t<li>'usually'</li>\n",
       "\t<li>'uucp'</li>\n",
       "\t<li>'v'</li>\n",
       "\t<li>'value'</li>\n",
       "\t<li>'various'</li>\n",
       "\t<li>'very'</li>\n",
       "\t<li>'via'</li>\n",
       "\t<li>'viz'</li>\n",
       "\t<li>'vs'</li>\n",
       "\t<li>'w'</li>\n",
       "\t<li>'want'</li>\n",
       "\t<li>'wants'</li>\n",
       "\t<li>'was'</li>\n",
       "\t<li>'wasn\\'t'</li>\n",
       "\t<li>'way'</li>\n",
       "\t<li>'we'</li>\n",
       "\t<li>'we\\'d'</li>\n",
       "\t<li>'we\\'ll'</li>\n",
       "\t<li>'we\\'re'</li>\n",
       "\t<li>'we\\'ve'</li>\n",
       "\t<li>'welcome'</li>\n",
       "\t<li>'well'</li>\n",
       "\t<li>'went'</li>\n",
       "\t<li>'were'</li>\n",
       "\t<li>'weren\\'t'</li>\n",
       "\t<li>'what'</li>\n",
       "\t<li>'what\\'s'</li>\n",
       "\t<li>'whatever'</li>\n",
       "\t<li>'when'</li>\n",
       "\t<li>'whence'</li>\n",
       "\t<li>'whenever'</li>\n",
       "\t<li>'where'</li>\n",
       "\t<li>'where\\'s'</li>\n",
       "\t<li>'whereafter'</li>\n",
       "\t<li>'whereas'</li>\n",
       "\t<li>'whereby'</li>\n",
       "\t<li>'wherein'</li>\n",
       "\t<li>'whereupon'</li>\n",
       "\t<li>'wherever'</li>\n",
       "\t<li>'whether'</li>\n",
       "\t<li>'which'</li>\n",
       "\t<li>'while'</li>\n",
       "\t<li>'whither'</li>\n",
       "\t<li>'who'</li>\n",
       "\t<li>'who\\'s'</li>\n",
       "\t<li>'whoever'</li>\n",
       "\t<li>'whole'</li>\n",
       "\t<li>'whom'</li>\n",
       "\t<li>'whose'</li>\n",
       "\t<li>'why'</li>\n",
       "\t<li>'will'</li>\n",
       "\t<li>'willing'</li>\n",
       "\t<li>'wish'</li>\n",
       "\t<li>'with'</li>\n",
       "\t<li>'within'</li>\n",
       "\t<li>'without'</li>\n",
       "\t<li>'won\\'t'</li>\n",
       "\t<li>'wonder'</li>\n",
       "\t<li>'would'</li>\n",
       "\t<li>'would'</li>\n",
       "\t<li>'wouldn\\'t'</li>\n",
       "\t<li>'x'</li>\n",
       "\t<li>'y'</li>\n",
       "\t<li>'yes'</li>\n",
       "\t<li>'yet'</li>\n",
       "\t<li>'you'</li>\n",
       "\t<li>'you\\'d'</li>\n",
       "\t<li>'you\\'ll'</li>\n",
       "\t<li>'you\\'re'</li>\n",
       "\t<li>'you\\'ve'</li>\n",
       "\t<li>'your'</li>\n",
       "\t<li>'yours'</li>\n",
       "\t<li>'yourself'</li>\n",
       "\t<li>'yourselves'</li>\n",
       "\t<li>'z'</li>\n",
       "\t<li>'zero'</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 'a'\n",
       "\\item 'a\\textbackslash{}'s'\n",
       "\\item 'able'\n",
       "\\item 'about'\n",
       "\\item 'above'\n",
       "\\item 'according'\n",
       "\\item 'accordingly'\n",
       "\\item 'across'\n",
       "\\item 'actually'\n",
       "\\item 'after'\n",
       "\\item 'afterwards'\n",
       "\\item 'again'\n",
       "\\item 'against'\n",
       "\\item 'ain\\textbackslash{}'t'\n",
       "\\item 'all'\n",
       "\\item 'allow'\n",
       "\\item 'allows'\n",
       "\\item 'almost'\n",
       "\\item 'alone'\n",
       "\\item 'along'\n",
       "\\item 'already'\n",
       "\\item 'also'\n",
       "\\item 'although'\n",
       "\\item 'always'\n",
       "\\item 'am'\n",
       "\\item 'among'\n",
       "\\item 'amongst'\n",
       "\\item 'an'\n",
       "\\item 'and'\n",
       "\\item 'another'\n",
       "\\item 'any'\n",
       "\\item 'anybody'\n",
       "\\item 'anyhow'\n",
       "\\item 'anyone'\n",
       "\\item 'anything'\n",
       "\\item 'anyway'\n",
       "\\item 'anyways'\n",
       "\\item 'anywhere'\n",
       "\\item 'apart'\n",
       "\\item 'appear'\n",
       "\\item 'appreciate'\n",
       "\\item 'appropriate'\n",
       "\\item 'are'\n",
       "\\item 'aren\\textbackslash{}'t'\n",
       "\\item 'around'\n",
       "\\item 'as'\n",
       "\\item 'aside'\n",
       "\\item 'ask'\n",
       "\\item 'asking'\n",
       "\\item 'associated'\n",
       "\\item 'at'\n",
       "\\item 'available'\n",
       "\\item 'away'\n",
       "\\item 'awfully'\n",
       "\\item 'b'\n",
       "\\item 'be'\n",
       "\\item 'became'\n",
       "\\item 'because'\n",
       "\\item 'become'\n",
       "\\item 'becomes'\n",
       "\\item 'becoming'\n",
       "\\item 'been'\n",
       "\\item 'before'\n",
       "\\item 'beforehand'\n",
       "\\item 'behind'\n",
       "\\item 'being'\n",
       "\\item 'believe'\n",
       "\\item 'below'\n",
       "\\item 'beside'\n",
       "\\item 'besides'\n",
       "\\item 'best'\n",
       "\\item 'better'\n",
       "\\item 'between'\n",
       "\\item 'beyond'\n",
       "\\item 'both'\n",
       "\\item 'brief'\n",
       "\\item 'but'\n",
       "\\item 'by'\n",
       "\\item 'c'\n",
       "\\item 'c\\textbackslash{}'mon'\n",
       "\\item 'c\\textbackslash{}'s'\n",
       "\\item 'came'\n",
       "\\item 'can'\n",
       "\\item 'can\\textbackslash{}'t'\n",
       "\\item 'cannot'\n",
       "\\item 'cant'\n",
       "\\item 'cause'\n",
       "\\item 'causes'\n",
       "\\item 'certain'\n",
       "\\item 'certainly'\n",
       "\\item 'changes'\n",
       "\\item 'clearly'\n",
       "\\item 'co'\n",
       "\\item 'com'\n",
       "\\item 'come'\n",
       "\\item 'comes'\n",
       "\\item 'concerning'\n",
       "\\item 'consequently'\n",
       "\\item 'consider'\n",
       "\\item 'considering'\n",
       "\\item 'contain'\n",
       "\\item 'containing'\n",
       "\\item 'contains'\n",
       "\\item 'corresponding'\n",
       "\\item 'could'\n",
       "\\item 'couldn\\textbackslash{}'t'\n",
       "\\item 'course'\n",
       "\\item 'currently'\n",
       "\\item 'd'\n",
       "\\item 'definitely'\n",
       "\\item 'described'\n",
       "\\item 'despite'\n",
       "\\item 'did'\n",
       "\\item 'didn\\textbackslash{}'t'\n",
       "\\item 'different'\n",
       "\\item 'do'\n",
       "\\item 'does'\n",
       "\\item 'doesn\\textbackslash{}'t'\n",
       "\\item 'doing'\n",
       "\\item 'don\\textbackslash{}'t'\n",
       "\\item 'done'\n",
       "\\item 'down'\n",
       "\\item 'downwards'\n",
       "\\item 'during'\n",
       "\\item 'e'\n",
       "\\item 'each'\n",
       "\\item 'edu'\n",
       "\\item 'eg'\n",
       "\\item 'eight'\n",
       "\\item 'either'\n",
       "\\item 'else'\n",
       "\\item 'elsewhere'\n",
       "\\item 'enough'\n",
       "\\item 'entirely'\n",
       "\\item 'especially'\n",
       "\\item 'et'\n",
       "\\item 'etc'\n",
       "\\item 'even'\n",
       "\\item 'ever'\n",
       "\\item 'every'\n",
       "\\item 'everybody'\n",
       "\\item 'everyone'\n",
       "\\item 'everything'\n",
       "\\item 'everywhere'\n",
       "\\item 'ex'\n",
       "\\item 'exactly'\n",
       "\\item 'example'\n",
       "\\item 'except'\n",
       "\\item 'f'\n",
       "\\item 'far'\n",
       "\\item 'few'\n",
       "\\item 'fifth'\n",
       "\\item 'first'\n",
       "\\item 'five'\n",
       "\\item 'followed'\n",
       "\\item 'following'\n",
       "\\item 'follows'\n",
       "\\item 'for'\n",
       "\\item 'former'\n",
       "\\item 'formerly'\n",
       "\\item 'forth'\n",
       "\\item 'four'\n",
       "\\item 'from'\n",
       "\\item 'further'\n",
       "\\item 'furthermore'\n",
       "\\item 'g'\n",
       "\\item 'get'\n",
       "\\item 'gets'\n",
       "\\item 'getting'\n",
       "\\item 'given'\n",
       "\\item 'gives'\n",
       "\\item 'go'\n",
       "\\item 'goes'\n",
       "\\item 'going'\n",
       "\\item 'gone'\n",
       "\\item 'got'\n",
       "\\item 'gotten'\n",
       "\\item 'greetings'\n",
       "\\item 'h'\n",
       "\\item 'had'\n",
       "\\item 'hadn\\textbackslash{}'t'\n",
       "\\item 'happens'\n",
       "\\item 'hardly'\n",
       "\\item 'has'\n",
       "\\item 'hasn\\textbackslash{}'t'\n",
       "\\item 'have'\n",
       "\\item 'haven\\textbackslash{}'t'\n",
       "\\item 'having'\n",
       "\\item 'he'\n",
       "\\item 'he\\textbackslash{}'s'\n",
       "\\item 'hello'\n",
       "\\item 'help'\n",
       "\\item 'hence'\n",
       "\\item 'her'\n",
       "\\item 'here'\n",
       "\\item 'here\\textbackslash{}'s'\n",
       "\\item 'hereafter'\n",
       "\\item 'hereby'\n",
       "\\item 'herein'\n",
       "\\item 'hereupon'\n",
       "\\item 'hers'\n",
       "\\item 'herself'\n",
       "\\item 'hi'\n",
       "\\item 'him'\n",
       "\\item 'himself'\n",
       "\\item 'his'\n",
       "\\item 'hither'\n",
       "\\item 'hopefully'\n",
       "\\item 'how'\n",
       "\\item 'howbeit'\n",
       "\\item 'however'\n",
       "\\item 'i'\n",
       "\\item 'i\\textbackslash{}'d'\n",
       "\\item 'i\\textbackslash{}'ll'\n",
       "\\item 'i\\textbackslash{}'m'\n",
       "\\item 'i\\textbackslash{}'ve'\n",
       "\\item 'ie'\n",
       "\\item 'if'\n",
       "\\item 'ignored'\n",
       "\\item 'immediate'\n",
       "\\item 'in'\n",
       "\\item 'inasmuch'\n",
       "\\item 'inc'\n",
       "\\item 'indeed'\n",
       "\\item 'indicate'\n",
       "\\item 'indicated'\n",
       "\\item 'indicates'\n",
       "\\item 'inner'\n",
       "\\item 'insofar'\n",
       "\\item 'instead'\n",
       "\\item 'into'\n",
       "\\item 'inward'\n",
       "\\item 'is'\n",
       "\\item 'isn\\textbackslash{}'t'\n",
       "\\item 'it'\n",
       "\\item 'it\\textbackslash{}'d'\n",
       "\\item 'it\\textbackslash{}'ll'\n",
       "\\item 'it\\textbackslash{}'s'\n",
       "\\item 'its'\n",
       "\\item 'itself'\n",
       "\\item 'j'\n",
       "\\item 'just'\n",
       "\\item 'k'\n",
       "\\item 'keep'\n",
       "\\item 'keeps'\n",
       "\\item 'kept'\n",
       "\\item 'know'\n",
       "\\item 'knows'\n",
       "\\item 'known'\n",
       "\\item 'l'\n",
       "\\item 'last'\n",
       "\\item 'lately'\n",
       "\\item 'later'\n",
       "\\item 'latter'\n",
       "\\item 'latterly'\n",
       "\\item 'least'\n",
       "\\item 'less'\n",
       "\\item 'lest'\n",
       "\\item 'let'\n",
       "\\item 'let\\textbackslash{}'s'\n",
       "\\item 'like'\n",
       "\\item 'liked'\n",
       "\\item 'likely'\n",
       "\\item 'little'\n",
       "\\item 'look'\n",
       "\\item 'looking'\n",
       "\\item 'looks'\n",
       "\\item 'ltd'\n",
       "\\item 'm'\n",
       "\\item 'mainly'\n",
       "\\item 'many'\n",
       "\\item 'may'\n",
       "\\item 'maybe'\n",
       "\\item 'me'\n",
       "\\item 'mean'\n",
       "\\item 'meanwhile'\n",
       "\\item 'merely'\n",
       "\\item 'might'\n",
       "\\item 'more'\n",
       "\\item 'moreover'\n",
       "\\item 'most'\n",
       "\\item 'mostly'\n",
       "\\item 'much'\n",
       "\\item 'must'\n",
       "\\item 'my'\n",
       "\\item 'myself'\n",
       "\\item 'n'\n",
       "\\item 'name'\n",
       "\\item 'namely'\n",
       "\\item 'nd'\n",
       "\\item 'near'\n",
       "\\item 'nearly'\n",
       "\\item 'necessary'\n",
       "\\item 'need'\n",
       "\\item 'needs'\n",
       "\\item 'neither'\n",
       "\\item 'never'\n",
       "\\item 'nevertheless'\n",
       "\\item 'new'\n",
       "\\item 'next'\n",
       "\\item 'nine'\n",
       "\\item 'no'\n",
       "\\item 'nobody'\n",
       "\\item 'non'\n",
       "\\item 'none'\n",
       "\\item 'noone'\n",
       "\\item 'nor'\n",
       "\\item 'normally'\n",
       "\\item 'not'\n",
       "\\item 'nothing'\n",
       "\\item 'novel'\n",
       "\\item 'now'\n",
       "\\item 'nowhere'\n",
       "\\item 'o'\n",
       "\\item 'obviously'\n",
       "\\item 'of'\n",
       "\\item 'off'\n",
       "\\item 'often'\n",
       "\\item 'oh'\n",
       "\\item 'ok'\n",
       "\\item 'okay'\n",
       "\\item 'old'\n",
       "\\item 'on'\n",
       "\\item 'once'\n",
       "\\item 'one'\n",
       "\\item 'ones'\n",
       "\\item 'only'\n",
       "\\item 'onto'\n",
       "\\item 'or'\n",
       "\\item 'other'\n",
       "\\item 'others'\n",
       "\\item 'otherwise'\n",
       "\\item 'ought'\n",
       "\\item 'our'\n",
       "\\item 'ours'\n",
       "\\item 'ourselves'\n",
       "\\item 'out'\n",
       "\\item 'outside'\n",
       "\\item 'over'\n",
       "\\item 'overall'\n",
       "\\item 'own'\n",
       "\\item 'p'\n",
       "\\item 'particular'\n",
       "\\item 'particularly'\n",
       "\\item 'per'\n",
       "\\item 'perhaps'\n",
       "\\item 'placed'\n",
       "\\item 'please'\n",
       "\\item 'plus'\n",
       "\\item 'possible'\n",
       "\\item 'presumably'\n",
       "\\item 'probably'\n",
       "\\item 'provides'\n",
       "\\item 'q'\n",
       "\\item 'que'\n",
       "\\item 'quite'\n",
       "\\item 'qv'\n",
       "\\item 'r'\n",
       "\\item 'rather'\n",
       "\\item 'rd'\n",
       "\\item 're'\n",
       "\\item 'really'\n",
       "\\item 'reasonably'\n",
       "\\item 'regarding'\n",
       "\\item 'regardless'\n",
       "\\item 'regards'\n",
       "\\item 'relatively'\n",
       "\\item 'respectively'\n",
       "\\item 'right'\n",
       "\\item 's'\n",
       "\\item 'said'\n",
       "\\item 'same'\n",
       "\\item 'saw'\n",
       "\\item 'say'\n",
       "\\item 'saying'\n",
       "\\item 'says'\n",
       "\\item 'second'\n",
       "\\item 'secondly'\n",
       "\\item 'see'\n",
       "\\item 'seeing'\n",
       "\\item 'seem'\n",
       "\\item 'seemed'\n",
       "\\item 'seeming'\n",
       "\\item 'seems'\n",
       "\\item 'seen'\n",
       "\\item 'self'\n",
       "\\item 'selves'\n",
       "\\item 'sensible'\n",
       "\\item 'sent'\n",
       "\\item 'serious'\n",
       "\\item 'seriously'\n",
       "\\item 'seven'\n",
       "\\item 'several'\n",
       "\\item 'shall'\n",
       "\\item 'she'\n",
       "\\item 'should'\n",
       "\\item 'shouldn\\textbackslash{}'t'\n",
       "\\item 'since'\n",
       "\\item 'six'\n",
       "\\item 'so'\n",
       "\\item 'some'\n",
       "\\item 'somebody'\n",
       "\\item 'somehow'\n",
       "\\item 'someone'\n",
       "\\item 'something'\n",
       "\\item 'sometime'\n",
       "\\item 'sometimes'\n",
       "\\item 'somewhat'\n",
       "\\item 'somewhere'\n",
       "\\item 'soon'\n",
       "\\item 'sorry'\n",
       "\\item 'specified'\n",
       "\\item 'specify'\n",
       "\\item 'specifying'\n",
       "\\item 'still'\n",
       "\\item 'sub'\n",
       "\\item 'such'\n",
       "\\item 'sup'\n",
       "\\item 'sure'\n",
       "\\item 't'\n",
       "\\item 't\\textbackslash{}'s'\n",
       "\\item 'take'\n",
       "\\item 'taken'\n",
       "\\item 'tell'\n",
       "\\item 'tends'\n",
       "\\item 'th'\n",
       "\\item 'than'\n",
       "\\item 'thank'\n",
       "\\item 'thanks'\n",
       "\\item 'thanx'\n",
       "\\item 'that'\n",
       "\\item 'that\\textbackslash{}'s'\n",
       "\\item 'thats'\n",
       "\\item 'the'\n",
       "\\item 'their'\n",
       "\\item 'theirs'\n",
       "\\item 'them'\n",
       "\\item 'themselves'\n",
       "\\item 'then'\n",
       "\\item 'thence'\n",
       "\\item 'there'\n",
       "\\item 'there\\textbackslash{}'s'\n",
       "\\item 'thereafter'\n",
       "\\item 'thereby'\n",
       "\\item 'therefore'\n",
       "\\item 'therein'\n",
       "\\item 'theres'\n",
       "\\item 'thereupon'\n",
       "\\item 'these'\n",
       "\\item 'they'\n",
       "\\item 'they\\textbackslash{}'d'\n",
       "\\item 'they\\textbackslash{}'ll'\n",
       "\\item 'they\\textbackslash{}'re'\n",
       "\\item 'they\\textbackslash{}'ve'\n",
       "\\item 'think'\n",
       "\\item 'third'\n",
       "\\item 'this'\n",
       "\\item 'thorough'\n",
       "\\item 'thoroughly'\n",
       "\\item 'those'\n",
       "\\item 'though'\n",
       "\\item 'three'\n",
       "\\item 'through'\n",
       "\\item 'throughout'\n",
       "\\item 'thru'\n",
       "\\item 'thus'\n",
       "\\item 'to'\n",
       "\\item 'together'\n",
       "\\item 'too'\n",
       "\\item 'took'\n",
       "\\item 'toward'\n",
       "\\item 'towards'\n",
       "\\item 'tried'\n",
       "\\item 'tries'\n",
       "\\item 'truly'\n",
       "\\item 'try'\n",
       "\\item 'trying'\n",
       "\\item 'twice'\n",
       "\\item 'two'\n",
       "\\item 'u'\n",
       "\\item 'un'\n",
       "\\item 'under'\n",
       "\\item 'unfortunately'\n",
       "\\item 'unless'\n",
       "\\item 'unlikely'\n",
       "\\item 'until'\n",
       "\\item 'unto'\n",
       "\\item 'up'\n",
       "\\item 'upon'\n",
       "\\item 'us'\n",
       "\\item 'use'\n",
       "\\item 'used'\n",
       "\\item 'useful'\n",
       "\\item 'uses'\n",
       "\\item 'using'\n",
       "\\item 'usually'\n",
       "\\item 'uucp'\n",
       "\\item 'v'\n",
       "\\item 'value'\n",
       "\\item 'various'\n",
       "\\item 'very'\n",
       "\\item 'via'\n",
       "\\item 'viz'\n",
       "\\item 'vs'\n",
       "\\item 'w'\n",
       "\\item 'want'\n",
       "\\item 'wants'\n",
       "\\item 'was'\n",
       "\\item 'wasn\\textbackslash{}'t'\n",
       "\\item 'way'\n",
       "\\item 'we'\n",
       "\\item 'we\\textbackslash{}'d'\n",
       "\\item 'we\\textbackslash{}'ll'\n",
       "\\item 'we\\textbackslash{}'re'\n",
       "\\item 'we\\textbackslash{}'ve'\n",
       "\\item 'welcome'\n",
       "\\item 'well'\n",
       "\\item 'went'\n",
       "\\item 'were'\n",
       "\\item 'weren\\textbackslash{}'t'\n",
       "\\item 'what'\n",
       "\\item 'what\\textbackslash{}'s'\n",
       "\\item 'whatever'\n",
       "\\item 'when'\n",
       "\\item 'whence'\n",
       "\\item 'whenever'\n",
       "\\item 'where'\n",
       "\\item 'where\\textbackslash{}'s'\n",
       "\\item 'whereafter'\n",
       "\\item 'whereas'\n",
       "\\item 'whereby'\n",
       "\\item 'wherein'\n",
       "\\item 'whereupon'\n",
       "\\item 'wherever'\n",
       "\\item 'whether'\n",
       "\\item 'which'\n",
       "\\item 'while'\n",
       "\\item 'whither'\n",
       "\\item 'who'\n",
       "\\item 'who\\textbackslash{}'s'\n",
       "\\item 'whoever'\n",
       "\\item 'whole'\n",
       "\\item 'whom'\n",
       "\\item 'whose'\n",
       "\\item 'why'\n",
       "\\item 'will'\n",
       "\\item 'willing'\n",
       "\\item 'wish'\n",
       "\\item 'with'\n",
       "\\item 'within'\n",
       "\\item 'without'\n",
       "\\item 'won\\textbackslash{}'t'\n",
       "\\item 'wonder'\n",
       "\\item 'would'\n",
       "\\item 'would'\n",
       "\\item 'wouldn\\textbackslash{}'t'\n",
       "\\item 'x'\n",
       "\\item 'y'\n",
       "\\item 'yes'\n",
       "\\item 'yet'\n",
       "\\item 'you'\n",
       "\\item 'you\\textbackslash{}'d'\n",
       "\\item 'you\\textbackslash{}'ll'\n",
       "\\item 'you\\textbackslash{}'re'\n",
       "\\item 'you\\textbackslash{}'ve'\n",
       "\\item 'your'\n",
       "\\item 'yours'\n",
       "\\item 'yourself'\n",
       "\\item 'yourselves'\n",
       "\\item 'z'\n",
       "\\item 'zero'\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 'a'\n",
       "2. 'a\\'s'\n",
       "3. 'able'\n",
       "4. 'about'\n",
       "5. 'above'\n",
       "6. 'according'\n",
       "7. 'accordingly'\n",
       "8. 'across'\n",
       "9. 'actually'\n",
       "10. 'after'\n",
       "11. 'afterwards'\n",
       "12. 'again'\n",
       "13. 'against'\n",
       "14. 'ain\\'t'\n",
       "15. 'all'\n",
       "16. 'allow'\n",
       "17. 'allows'\n",
       "18. 'almost'\n",
       "19. 'alone'\n",
       "20. 'along'\n",
       "21. 'already'\n",
       "22. 'also'\n",
       "23. 'although'\n",
       "24. 'always'\n",
       "25. 'am'\n",
       "26. 'among'\n",
       "27. 'amongst'\n",
       "28. 'an'\n",
       "29. 'and'\n",
       "30. 'another'\n",
       "31. 'any'\n",
       "32. 'anybody'\n",
       "33. 'anyhow'\n",
       "34. 'anyone'\n",
       "35. 'anything'\n",
       "36. 'anyway'\n",
       "37. 'anyways'\n",
       "38. 'anywhere'\n",
       "39. 'apart'\n",
       "40. 'appear'\n",
       "41. 'appreciate'\n",
       "42. 'appropriate'\n",
       "43. 'are'\n",
       "44. 'aren\\'t'\n",
       "45. 'around'\n",
       "46. 'as'\n",
       "47. 'aside'\n",
       "48. 'ask'\n",
       "49. 'asking'\n",
       "50. 'associated'\n",
       "51. 'at'\n",
       "52. 'available'\n",
       "53. 'away'\n",
       "54. 'awfully'\n",
       "55. 'b'\n",
       "56. 'be'\n",
       "57. 'became'\n",
       "58. 'because'\n",
       "59. 'become'\n",
       "60. 'becomes'\n",
       "61. 'becoming'\n",
       "62. 'been'\n",
       "63. 'before'\n",
       "64. 'beforehand'\n",
       "65. 'behind'\n",
       "66. 'being'\n",
       "67. 'believe'\n",
       "68. 'below'\n",
       "69. 'beside'\n",
       "70. 'besides'\n",
       "71. 'best'\n",
       "72. 'better'\n",
       "73. 'between'\n",
       "74. 'beyond'\n",
       "75. 'both'\n",
       "76. 'brief'\n",
       "77. 'but'\n",
       "78. 'by'\n",
       "79. 'c'\n",
       "80. 'c\\'mon'\n",
       "81. 'c\\'s'\n",
       "82. 'came'\n",
       "83. 'can'\n",
       "84. 'can\\'t'\n",
       "85. 'cannot'\n",
       "86. 'cant'\n",
       "87. 'cause'\n",
       "88. 'causes'\n",
       "89. 'certain'\n",
       "90. 'certainly'\n",
       "91. 'changes'\n",
       "92. 'clearly'\n",
       "93. 'co'\n",
       "94. 'com'\n",
       "95. 'come'\n",
       "96. 'comes'\n",
       "97. 'concerning'\n",
       "98. 'consequently'\n",
       "99. 'consider'\n",
       "100. 'considering'\n",
       "101. 'contain'\n",
       "102. 'containing'\n",
       "103. 'contains'\n",
       "104. 'corresponding'\n",
       "105. 'could'\n",
       "106. 'couldn\\'t'\n",
       "107. 'course'\n",
       "108. 'currently'\n",
       "109. 'd'\n",
       "110. 'definitely'\n",
       "111. 'described'\n",
       "112. 'despite'\n",
       "113. 'did'\n",
       "114. 'didn\\'t'\n",
       "115. 'different'\n",
       "116. 'do'\n",
       "117. 'does'\n",
       "118. 'doesn\\'t'\n",
       "119. 'doing'\n",
       "120. 'don\\'t'\n",
       "121. 'done'\n",
       "122. 'down'\n",
       "123. 'downwards'\n",
       "124. 'during'\n",
       "125. 'e'\n",
       "126. 'each'\n",
       "127. 'edu'\n",
       "128. 'eg'\n",
       "129. 'eight'\n",
       "130. 'either'\n",
       "131. 'else'\n",
       "132. 'elsewhere'\n",
       "133. 'enough'\n",
       "134. 'entirely'\n",
       "135. 'especially'\n",
       "136. 'et'\n",
       "137. 'etc'\n",
       "138. 'even'\n",
       "139. 'ever'\n",
       "140. 'every'\n",
       "141. 'everybody'\n",
       "142. 'everyone'\n",
       "143. 'everything'\n",
       "144. 'everywhere'\n",
       "145. 'ex'\n",
       "146. 'exactly'\n",
       "147. 'example'\n",
       "148. 'except'\n",
       "149. 'f'\n",
       "150. 'far'\n",
       "151. 'few'\n",
       "152. 'fifth'\n",
       "153. 'first'\n",
       "154. 'five'\n",
       "155. 'followed'\n",
       "156. 'following'\n",
       "157. 'follows'\n",
       "158. 'for'\n",
       "159. 'former'\n",
       "160. 'formerly'\n",
       "161. 'forth'\n",
       "162. 'four'\n",
       "163. 'from'\n",
       "164. 'further'\n",
       "165. 'furthermore'\n",
       "166. 'g'\n",
       "167. 'get'\n",
       "168. 'gets'\n",
       "169. 'getting'\n",
       "170. 'given'\n",
       "171. 'gives'\n",
       "172. 'go'\n",
       "173. 'goes'\n",
       "174. 'going'\n",
       "175. 'gone'\n",
       "176. 'got'\n",
       "177. 'gotten'\n",
       "178. 'greetings'\n",
       "179. 'h'\n",
       "180. 'had'\n",
       "181. 'hadn\\'t'\n",
       "182. 'happens'\n",
       "183. 'hardly'\n",
       "184. 'has'\n",
       "185. 'hasn\\'t'\n",
       "186. 'have'\n",
       "187. 'haven\\'t'\n",
       "188. 'having'\n",
       "189. 'he'\n",
       "190. 'he\\'s'\n",
       "191. 'hello'\n",
       "192. 'help'\n",
       "193. 'hence'\n",
       "194. 'her'\n",
       "195. 'here'\n",
       "196. 'here\\'s'\n",
       "197. 'hereafter'\n",
       "198. 'hereby'\n",
       "199. 'herein'\n",
       "200. 'hereupon'\n",
       "201. 'hers'\n",
       "202. 'herself'\n",
       "203. 'hi'\n",
       "204. 'him'\n",
       "205. 'himself'\n",
       "206. 'his'\n",
       "207. 'hither'\n",
       "208. 'hopefully'\n",
       "209. 'how'\n",
       "210. 'howbeit'\n",
       "211. 'however'\n",
       "212. 'i'\n",
       "213. 'i\\'d'\n",
       "214. 'i\\'ll'\n",
       "215. 'i\\'m'\n",
       "216. 'i\\'ve'\n",
       "217. 'ie'\n",
       "218. 'if'\n",
       "219. 'ignored'\n",
       "220. 'immediate'\n",
       "221. 'in'\n",
       "222. 'inasmuch'\n",
       "223. 'inc'\n",
       "224. 'indeed'\n",
       "225. 'indicate'\n",
       "226. 'indicated'\n",
       "227. 'indicates'\n",
       "228. 'inner'\n",
       "229. 'insofar'\n",
       "230. 'instead'\n",
       "231. 'into'\n",
       "232. 'inward'\n",
       "233. 'is'\n",
       "234. 'isn\\'t'\n",
       "235. 'it'\n",
       "236. 'it\\'d'\n",
       "237. 'it\\'ll'\n",
       "238. 'it\\'s'\n",
       "239. 'its'\n",
       "240. 'itself'\n",
       "241. 'j'\n",
       "242. 'just'\n",
       "243. 'k'\n",
       "244. 'keep'\n",
       "245. 'keeps'\n",
       "246. 'kept'\n",
       "247. 'know'\n",
       "248. 'knows'\n",
       "249. 'known'\n",
       "250. 'l'\n",
       "251. 'last'\n",
       "252. 'lately'\n",
       "253. 'later'\n",
       "254. 'latter'\n",
       "255. 'latterly'\n",
       "256. 'least'\n",
       "257. 'less'\n",
       "258. 'lest'\n",
       "259. 'let'\n",
       "260. 'let\\'s'\n",
       "261. 'like'\n",
       "262. 'liked'\n",
       "263. 'likely'\n",
       "264. 'little'\n",
       "265. 'look'\n",
       "266. 'looking'\n",
       "267. 'looks'\n",
       "268. 'ltd'\n",
       "269. 'm'\n",
       "270. 'mainly'\n",
       "271. 'many'\n",
       "272. 'may'\n",
       "273. 'maybe'\n",
       "274. 'me'\n",
       "275. 'mean'\n",
       "276. 'meanwhile'\n",
       "277. 'merely'\n",
       "278. 'might'\n",
       "279. 'more'\n",
       "280. 'moreover'\n",
       "281. 'most'\n",
       "282. 'mostly'\n",
       "283. 'much'\n",
       "284. 'must'\n",
       "285. 'my'\n",
       "286. 'myself'\n",
       "287. 'n'\n",
       "288. 'name'\n",
       "289. 'namely'\n",
       "290. 'nd'\n",
       "291. 'near'\n",
       "292. 'nearly'\n",
       "293. 'necessary'\n",
       "294. 'need'\n",
       "295. 'needs'\n",
       "296. 'neither'\n",
       "297. 'never'\n",
       "298. 'nevertheless'\n",
       "299. 'new'\n",
       "300. 'next'\n",
       "301. 'nine'\n",
       "302. 'no'\n",
       "303. 'nobody'\n",
       "304. 'non'\n",
       "305. 'none'\n",
       "306. 'noone'\n",
       "307. 'nor'\n",
       "308. 'normally'\n",
       "309. 'not'\n",
       "310. 'nothing'\n",
       "311. 'novel'\n",
       "312. 'now'\n",
       "313. 'nowhere'\n",
       "314. 'o'\n",
       "315. 'obviously'\n",
       "316. 'of'\n",
       "317. 'off'\n",
       "318. 'often'\n",
       "319. 'oh'\n",
       "320. 'ok'\n",
       "321. 'okay'\n",
       "322. 'old'\n",
       "323. 'on'\n",
       "324. 'once'\n",
       "325. 'one'\n",
       "326. 'ones'\n",
       "327. 'only'\n",
       "328. 'onto'\n",
       "329. 'or'\n",
       "330. 'other'\n",
       "331. 'others'\n",
       "332. 'otherwise'\n",
       "333. 'ought'\n",
       "334. 'our'\n",
       "335. 'ours'\n",
       "336. 'ourselves'\n",
       "337. 'out'\n",
       "338. 'outside'\n",
       "339. 'over'\n",
       "340. 'overall'\n",
       "341. 'own'\n",
       "342. 'p'\n",
       "343. 'particular'\n",
       "344. 'particularly'\n",
       "345. 'per'\n",
       "346. 'perhaps'\n",
       "347. 'placed'\n",
       "348. 'please'\n",
       "349. 'plus'\n",
       "350. 'possible'\n",
       "351. 'presumably'\n",
       "352. 'probably'\n",
       "353. 'provides'\n",
       "354. 'q'\n",
       "355. 'que'\n",
       "356. 'quite'\n",
       "357. 'qv'\n",
       "358. 'r'\n",
       "359. 'rather'\n",
       "360. 'rd'\n",
       "361. 're'\n",
       "362. 'really'\n",
       "363. 'reasonably'\n",
       "364. 'regarding'\n",
       "365. 'regardless'\n",
       "366. 'regards'\n",
       "367. 'relatively'\n",
       "368. 'respectively'\n",
       "369. 'right'\n",
       "370. 's'\n",
       "371. 'said'\n",
       "372. 'same'\n",
       "373. 'saw'\n",
       "374. 'say'\n",
       "375. 'saying'\n",
       "376. 'says'\n",
       "377. 'second'\n",
       "378. 'secondly'\n",
       "379. 'see'\n",
       "380. 'seeing'\n",
       "381. 'seem'\n",
       "382. 'seemed'\n",
       "383. 'seeming'\n",
       "384. 'seems'\n",
       "385. 'seen'\n",
       "386. 'self'\n",
       "387. 'selves'\n",
       "388. 'sensible'\n",
       "389. 'sent'\n",
       "390. 'serious'\n",
       "391. 'seriously'\n",
       "392. 'seven'\n",
       "393. 'several'\n",
       "394. 'shall'\n",
       "395. 'she'\n",
       "396. 'should'\n",
       "397. 'shouldn\\'t'\n",
       "398. 'since'\n",
       "399. 'six'\n",
       "400. 'so'\n",
       "401. 'some'\n",
       "402. 'somebody'\n",
       "403. 'somehow'\n",
       "404. 'someone'\n",
       "405. 'something'\n",
       "406. 'sometime'\n",
       "407. 'sometimes'\n",
       "408. 'somewhat'\n",
       "409. 'somewhere'\n",
       "410. 'soon'\n",
       "411. 'sorry'\n",
       "412. 'specified'\n",
       "413. 'specify'\n",
       "414. 'specifying'\n",
       "415. 'still'\n",
       "416. 'sub'\n",
       "417. 'such'\n",
       "418. 'sup'\n",
       "419. 'sure'\n",
       "420. 't'\n",
       "421. 't\\'s'\n",
       "422. 'take'\n",
       "423. 'taken'\n",
       "424. 'tell'\n",
       "425. 'tends'\n",
       "426. 'th'\n",
       "427. 'than'\n",
       "428. 'thank'\n",
       "429. 'thanks'\n",
       "430. 'thanx'\n",
       "431. 'that'\n",
       "432. 'that\\'s'\n",
       "433. 'thats'\n",
       "434. 'the'\n",
       "435. 'their'\n",
       "436. 'theirs'\n",
       "437. 'them'\n",
       "438. 'themselves'\n",
       "439. 'then'\n",
       "440. 'thence'\n",
       "441. 'there'\n",
       "442. 'there\\'s'\n",
       "443. 'thereafter'\n",
       "444. 'thereby'\n",
       "445. 'therefore'\n",
       "446. 'therein'\n",
       "447. 'theres'\n",
       "448. 'thereupon'\n",
       "449. 'these'\n",
       "450. 'they'\n",
       "451. 'they\\'d'\n",
       "452. 'they\\'ll'\n",
       "453. 'they\\'re'\n",
       "454. 'they\\'ve'\n",
       "455. 'think'\n",
       "456. 'third'\n",
       "457. 'this'\n",
       "458. 'thorough'\n",
       "459. 'thoroughly'\n",
       "460. 'those'\n",
       "461. 'though'\n",
       "462. 'three'\n",
       "463. 'through'\n",
       "464. 'throughout'\n",
       "465. 'thru'\n",
       "466. 'thus'\n",
       "467. 'to'\n",
       "468. 'together'\n",
       "469. 'too'\n",
       "470. 'took'\n",
       "471. 'toward'\n",
       "472. 'towards'\n",
       "473. 'tried'\n",
       "474. 'tries'\n",
       "475. 'truly'\n",
       "476. 'try'\n",
       "477. 'trying'\n",
       "478. 'twice'\n",
       "479. 'two'\n",
       "480. 'u'\n",
       "481. 'un'\n",
       "482. 'under'\n",
       "483. 'unfortunately'\n",
       "484. 'unless'\n",
       "485. 'unlikely'\n",
       "486. 'until'\n",
       "487. 'unto'\n",
       "488. 'up'\n",
       "489. 'upon'\n",
       "490. 'us'\n",
       "491. 'use'\n",
       "492. 'used'\n",
       "493. 'useful'\n",
       "494. 'uses'\n",
       "495. 'using'\n",
       "496. 'usually'\n",
       "497. 'uucp'\n",
       "498. 'v'\n",
       "499. 'value'\n",
       "500. 'various'\n",
       "501. 'very'\n",
       "502. 'via'\n",
       "503. 'viz'\n",
       "504. 'vs'\n",
       "505. 'w'\n",
       "506. 'want'\n",
       "507. 'wants'\n",
       "508. 'was'\n",
       "509. 'wasn\\'t'\n",
       "510. 'way'\n",
       "511. 'we'\n",
       "512. 'we\\'d'\n",
       "513. 'we\\'ll'\n",
       "514. 'we\\'re'\n",
       "515. 'we\\'ve'\n",
       "516. 'welcome'\n",
       "517. 'well'\n",
       "518. 'went'\n",
       "519. 'were'\n",
       "520. 'weren\\'t'\n",
       "521. 'what'\n",
       "522. 'what\\'s'\n",
       "523. 'whatever'\n",
       "524. 'when'\n",
       "525. 'whence'\n",
       "526. 'whenever'\n",
       "527. 'where'\n",
       "528. 'where\\'s'\n",
       "529. 'whereafter'\n",
       "530. 'whereas'\n",
       "531. 'whereby'\n",
       "532. 'wherein'\n",
       "533. 'whereupon'\n",
       "534. 'wherever'\n",
       "535. 'whether'\n",
       "536. 'which'\n",
       "537. 'while'\n",
       "538. 'whither'\n",
       "539. 'who'\n",
       "540. 'who\\'s'\n",
       "541. 'whoever'\n",
       "542. 'whole'\n",
       "543. 'whom'\n",
       "544. 'whose'\n",
       "545. 'why'\n",
       "546. 'will'\n",
       "547. 'willing'\n",
       "548. 'wish'\n",
       "549. 'with'\n",
       "550. 'within'\n",
       "551. 'without'\n",
       "552. 'won\\'t'\n",
       "553. 'wonder'\n",
       "554. 'would'\n",
       "555. 'would'\n",
       "556. 'wouldn\\'t'\n",
       "557. 'x'\n",
       "558. 'y'\n",
       "559. 'yes'\n",
       "560. 'yet'\n",
       "561. 'you'\n",
       "562. 'you\\'d'\n",
       "563. 'you\\'ll'\n",
       "564. 'you\\'re'\n",
       "565. 'you\\'ve'\n",
       "566. 'your'\n",
       "567. 'yours'\n",
       "568. 'yourself'\n",
       "569. 'yourselves'\n",
       "570. 'z'\n",
       "571. 'zero'\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "  [1] \"a\"             \"a's\"           \"able\"          \"about\"        \n",
       "  [5] \"above\"         \"according\"     \"accordingly\"   \"across\"       \n",
       "  [9] \"actually\"      \"after\"         \"afterwards\"    \"again\"        \n",
       " [13] \"against\"       \"ain't\"         \"all\"           \"allow\"        \n",
       " [17] \"allows\"        \"almost\"        \"alone\"         \"along\"        \n",
       " [21] \"already\"       \"also\"          \"although\"      \"always\"       \n",
       " [25] \"am\"            \"among\"         \"amongst\"       \"an\"           \n",
       " [29] \"and\"           \"another\"       \"any\"           \"anybody\"      \n",
       " [33] \"anyhow\"        \"anyone\"        \"anything\"      \"anyway\"       \n",
       " [37] \"anyways\"       \"anywhere\"      \"apart\"         \"appear\"       \n",
       " [41] \"appreciate\"    \"appropriate\"   \"are\"           \"aren't\"       \n",
       " [45] \"around\"        \"as\"            \"aside\"         \"ask\"          \n",
       " [49] \"asking\"        \"associated\"    \"at\"            \"available\"    \n",
       " [53] \"away\"          \"awfully\"       \"b\"             \"be\"           \n",
       " [57] \"became\"        \"because\"       \"become\"        \"becomes\"      \n",
       " [61] \"becoming\"      \"been\"          \"before\"        \"beforehand\"   \n",
       " [65] \"behind\"        \"being\"         \"believe\"       \"below\"        \n",
       " [69] \"beside\"        \"besides\"       \"best\"          \"better\"       \n",
       " [73] \"between\"       \"beyond\"        \"both\"          \"brief\"        \n",
       " [77] \"but\"           \"by\"            \"c\"             \"c'mon\"        \n",
       " [81] \"c's\"           \"came\"          \"can\"           \"can't\"        \n",
       " [85] \"cannot\"        \"cant\"          \"cause\"         \"causes\"       \n",
       " [89] \"certain\"       \"certainly\"     \"changes\"       \"clearly\"      \n",
       " [93] \"co\"            \"com\"           \"come\"          \"comes\"        \n",
       " [97] \"concerning\"    \"consequently\"  \"consider\"      \"considering\"  \n",
       "[101] \"contain\"       \"containing\"    \"contains\"      \"corresponding\"\n",
       "[105] \"could\"         \"couldn't\"      \"course\"        \"currently\"    \n",
       "[109] \"d\"             \"definitely\"    \"described\"     \"despite\"      \n",
       "[113] \"did\"           \"didn't\"        \"different\"     \"do\"           \n",
       "[117] \"does\"          \"doesn't\"       \"doing\"         \"don't\"        \n",
       "[121] \"done\"          \"down\"          \"downwards\"     \"during\"       \n",
       "[125] \"e\"             \"each\"          \"edu\"           \"eg\"           \n",
       "[129] \"eight\"         \"either\"        \"else\"          \"elsewhere\"    \n",
       "[133] \"enough\"        \"entirely\"      \"especially\"    \"et\"           \n",
       "[137] \"etc\"           \"even\"          \"ever\"          \"every\"        \n",
       "[141] \"everybody\"     \"everyone\"      \"everything\"    \"everywhere\"   \n",
       "[145] \"ex\"            \"exactly\"       \"example\"       \"except\"       \n",
       "[149] \"f\"             \"far\"           \"few\"           \"fifth\"        \n",
       "[153] \"first\"         \"five\"          \"followed\"      \"following\"    \n",
       "[157] \"follows\"       \"for\"           \"former\"        \"formerly\"     \n",
       "[161] \"forth\"         \"four\"          \"from\"          \"further\"      \n",
       "[165] \"furthermore\"   \"g\"             \"get\"           \"gets\"         \n",
       "[169] \"getting\"       \"given\"         \"gives\"         \"go\"           \n",
       "[173] \"goes\"          \"going\"         \"gone\"          \"got\"          \n",
       "[177] \"gotten\"        \"greetings\"     \"h\"             \"had\"          \n",
       "[181] \"hadn't\"        \"happens\"       \"hardly\"        \"has\"          \n",
       "[185] \"hasn't\"        \"have\"          \"haven't\"       \"having\"       \n",
       "[189] \"he\"            \"he's\"          \"hello\"         \"help\"         \n",
       "[193] \"hence\"         \"her\"           \"here\"          \"here's\"       \n",
       "[197] \"hereafter\"     \"hereby\"        \"herein\"        \"hereupon\"     \n",
       "[201] \"hers\"          \"herself\"       \"hi\"            \"him\"          \n",
       "[205] \"himself\"       \"his\"           \"hither\"        \"hopefully\"    \n",
       "[209] \"how\"           \"howbeit\"       \"however\"       \"i\"            \n",
       "[213] \"i'd\"           \"i'll\"          \"i'm\"           \"i've\"         \n",
       "[217] \"ie\"            \"if\"            \"ignored\"       \"immediate\"    \n",
       "[221] \"in\"            \"inasmuch\"      \"inc\"           \"indeed\"       \n",
       "[225] \"indicate\"      \"indicated\"     \"indicates\"     \"inner\"        \n",
       "[229] \"insofar\"       \"instead\"       \"into\"          \"inward\"       \n",
       "[233] \"is\"            \"isn't\"         \"it\"            \"it'd\"         \n",
       "[237] \"it'll\"         \"it's\"          \"its\"           \"itself\"       \n",
       "[241] \"j\"             \"just\"          \"k\"             \"keep\"         \n",
       "[245] \"keeps\"         \"kept\"          \"know\"          \"knows\"        \n",
       "[249] \"known\"         \"l\"             \"last\"          \"lately\"       \n",
       "[253] \"later\"         \"latter\"        \"latterly\"      \"least\"        \n",
       "[257] \"less\"          \"lest\"          \"let\"           \"let's\"        \n",
       "[261] \"like\"          \"liked\"         \"likely\"        \"little\"       \n",
       "[265] \"look\"          \"looking\"       \"looks\"         \"ltd\"          \n",
       "[269] \"m\"             \"mainly\"        \"many\"          \"may\"          \n",
       "[273] \"maybe\"         \"me\"            \"mean\"          \"meanwhile\"    \n",
       "[277] \"merely\"        \"might\"         \"more\"          \"moreover\"     \n",
       "[281] \"most\"          \"mostly\"        \"much\"          \"must\"         \n",
       "[285] \"my\"            \"myself\"        \"n\"             \"name\"         \n",
       "[289] \"namely\"        \"nd\"            \"near\"          \"nearly\"       \n",
       "[293] \"necessary\"     \"need\"          \"needs\"         \"neither\"      \n",
       "[297] \"never\"         \"nevertheless\"  \"new\"           \"next\"         \n",
       "[301] \"nine\"          \"no\"            \"nobody\"        \"non\"          \n",
       "[305] \"none\"          \"noone\"         \"nor\"           \"normally\"     \n",
       "[309] \"not\"           \"nothing\"       \"novel\"         \"now\"          \n",
       "[313] \"nowhere\"       \"o\"             \"obviously\"     \"of\"           \n",
       "[317] \"off\"           \"often\"         \"oh\"            \"ok\"           \n",
       "[321] \"okay\"          \"old\"           \"on\"            \"once\"         \n",
       "[325] \"one\"           \"ones\"          \"only\"          \"onto\"         \n",
       "[329] \"or\"            \"other\"         \"others\"        \"otherwise\"    \n",
       "[333] \"ought\"         \"our\"           \"ours\"          \"ourselves\"    \n",
       "[337] \"out\"           \"outside\"       \"over\"          \"overall\"      \n",
       "[341] \"own\"           \"p\"             \"particular\"    \"particularly\" \n",
       "[345] \"per\"           \"perhaps\"       \"placed\"        \"please\"       \n",
       "[349] \"plus\"          \"possible\"      \"presumably\"    \"probably\"     \n",
       "[353] \"provides\"      \"q\"             \"que\"           \"quite\"        \n",
       "[357] \"qv\"            \"r\"             \"rather\"        \"rd\"           \n",
       "[361] \"re\"            \"really\"        \"reasonably\"    \"regarding\"    \n",
       "[365] \"regardless\"    \"regards\"       \"relatively\"    \"respectively\" \n",
       "[369] \"right\"         \"s\"             \"said\"          \"same\"         \n",
       "[373] \"saw\"           \"say\"           \"saying\"        \"says\"         \n",
       "[377] \"second\"        \"secondly\"      \"see\"           \"seeing\"       \n",
       "[381] \"seem\"          \"seemed\"        \"seeming\"       \"seems\"        \n",
       "[385] \"seen\"          \"self\"          \"selves\"        \"sensible\"     \n",
       "[389] \"sent\"          \"serious\"       \"seriously\"     \"seven\"        \n",
       "[393] \"several\"       \"shall\"         \"she\"           \"should\"       \n",
       "[397] \"shouldn't\"     \"since\"         \"six\"           \"so\"           \n",
       "[401] \"some\"          \"somebody\"      \"somehow\"       \"someone\"      \n",
       "[405] \"something\"     \"sometime\"      \"sometimes\"     \"somewhat\"     \n",
       "[409] \"somewhere\"     \"soon\"          \"sorry\"         \"specified\"    \n",
       "[413] \"specify\"       \"specifying\"    \"still\"         \"sub\"          \n",
       "[417] \"such\"          \"sup\"           \"sure\"          \"t\"            \n",
       "[421] \"t's\"           \"take\"          \"taken\"         \"tell\"         \n",
       "[425] \"tends\"         \"th\"            \"than\"          \"thank\"        \n",
       "[429] \"thanks\"        \"thanx\"         \"that\"          \"that's\"       \n",
       "[433] \"thats\"         \"the\"           \"their\"         \"theirs\"       \n",
       "[437] \"them\"          \"themselves\"    \"then\"          \"thence\"       \n",
       "[441] \"there\"         \"there's\"       \"thereafter\"    \"thereby\"      \n",
       "[445] \"therefore\"     \"therein\"       \"theres\"        \"thereupon\"    \n",
       "[449] \"these\"         \"they\"          \"they'd\"        \"they'll\"      \n",
       "[453] \"they're\"       \"they've\"       \"think\"         \"third\"        \n",
       "[457] \"this\"          \"thorough\"      \"thoroughly\"    \"those\"        \n",
       "[461] \"though\"        \"three\"         \"through\"       \"throughout\"   \n",
       "[465] \"thru\"          \"thus\"          \"to\"            \"together\"     \n",
       "[469] \"too\"           \"took\"          \"toward\"        \"towards\"      \n",
       "[473] \"tried\"         \"tries\"         \"truly\"         \"try\"          \n",
       "[477] \"trying\"        \"twice\"         \"two\"           \"u\"            \n",
       "[481] \"un\"            \"under\"         \"unfortunately\" \"unless\"       \n",
       "[485] \"unlikely\"      \"until\"         \"unto\"          \"up\"           \n",
       "[489] \"upon\"          \"us\"            \"use\"           \"used\"         \n",
       "[493] \"useful\"        \"uses\"          \"using\"         \"usually\"      \n",
       "[497] \"uucp\"          \"v\"             \"value\"         \"various\"      \n",
       "[501] \"very\"          \"via\"           \"viz\"           \"vs\"           \n",
       "[505] \"w\"             \"want\"          \"wants\"         \"was\"          \n",
       "[509] \"wasn't\"        \"way\"           \"we\"            \"we'd\"         \n",
       "[513] \"we'll\"         \"we're\"         \"we've\"         \"welcome\"      \n",
       "[517] \"well\"          \"went\"          \"were\"          \"weren't\"      \n",
       "[521] \"what\"          \"what's\"        \"whatever\"      \"when\"         \n",
       "[525] \"whence\"        \"whenever\"      \"where\"         \"where's\"      \n",
       "[529] \"whereafter\"    \"whereas\"       \"whereby\"       \"wherein\"      \n",
       "[533] \"whereupon\"     \"wherever\"      \"whether\"       \"which\"        \n",
       "[537] \"while\"         \"whither\"       \"who\"           \"who's\"        \n",
       "[541] \"whoever\"       \"whole\"         \"whom\"          \"whose\"        \n",
       "[545] \"why\"           \"will\"          \"willing\"       \"wish\"         \n",
       "[549] \"with\"          \"within\"        \"without\"       \"won't\"        \n",
       "[553] \"wonder\"        \"would\"         \"would\"         \"wouldn't\"     \n",
       "[557] \"x\"             \"y\"             \"yes\"           \"yet\"          \n",
       "[561] \"you\"           \"you'd\"         \"you'll\"        \"you're\"       \n",
       "[565] \"you've\"        \"your\"          \"yours\"         \"yourself\"     \n",
       "[569] \"yourselves\"    \"z\"             \"zero\"         "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sprintf('tm패키지의 SMART불용어 단어수: %d개', length(stopwords('SMART')))\n",
    "sprintf('tm패키지의 SMART불용어 목록')\n",
    "stopwords(kind = 'SMART')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color = 'blue'>어근동일화(root words identification) 처리 (p.107)</font>\n",
    "* 텍스트셋에 동일한 단어가 문법적 기능에 따라 다양한 표현으로 사용되는 경우 이를 하나의 통일된 단어로 만들어 주는 것이 필요함\n",
    "* 영어의 동사는 3인칭인 경우, 시제에 따라서 문법적 표현형태가 다양해짐\n",
    "* 한국어 동사에서도 가다라는 동사가 가고, 가니, 간 등 맥락에 따라 표현형태가 달라짐"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 간단 텍스트셋 준비\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "'원본내용:'"
      ],
      "text/latex": [
       "'원본내용:'"
      ],
      "text/markdown": [
       "'원본내용:'"
      ],
      "text/plain": [
       "[1] \"원본내용:\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"I am a boy. Your are a boy. He might be a boy. Is he an american? He entered the ICT arena.\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "'객체유형: character객체'"
      ],
      "text/latex": [
       "'객체유형: character객체'"
      ],
      "text/markdown": [
       "'객체유형: character객체'"
      ],
      "text/plain": [
       "[1] \"객체유형: character객체\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "'객체길이: 1요소'"
      ],
      "text/latex": [
       "'객체길이: 1요소'"
      ],
      "text/markdown": [
       "'객체길이: 1요소'"
      ],
      "text/plain": [
       "[1] \"객체길이: 1요소\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "'원본글자수: 91글자'"
      ],
      "text/latex": [
       "'원본글자수: 91글자'"
      ],
      "text/markdown": [
       "'원본글자수: 91글자'"
      ],
      "text/plain": [
       "[1] \"원본글자수: 91글자\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 분석대상 텍스트 데이터 원본 준비\n",
    "my <- 'I am a boy. Your are a boy. He might be a boy. Is he an american? He entered the ICT arena.'\n",
    "\n",
    "sprintf('원본내용:')\n",
    "print(my)\n",
    "\n",
    "sprintf('객체유형: %s객체', class(my))\n",
    "sprintf('객체길이: %d요소', length(my))\n",
    "\n",
    "sprintf('원본글자수: %d글자', nchar(my))\n",
    "# - sprintf('검색대상 객체글자수: %d글자', str_length(my))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 단어단위로 직접 분해하는 경우\n",
    "* 보통은 <font color = 'blue'>str_split(string = 텍스트셋, pattern = <font color = 'red'>검색패턴</font>)</font>을 통해서 텍스트셋을 세부요소로 점진적으로 분해할 수 있음\n",
    "* 현재 주어진 텍스트셋에는 단어 이외에 문장부호나 특수문자 등이 섞여있는 구조라서, 이들에 대한 전처리가 필요하지만 \n",
    "<br>단어경계 옵션인 pattern = boundary('word')를 통해서 바로 텍스트셋을 단어단위로 분해할 수 있음\n",
    "* ==> <font color = 'blue'>str_extract_all(string = my, pattern = <font color = 'red'>boundary('word')</font>)</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 검색패턴이 아닌 단어경계 옵션을 이용한 텍스트셋 직접분해"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "'원본문장:'"
      ],
      "text/latex": [
       "'원본문장:'"
      ],
      "text/markdown": [
       "'원본문장:'"
      ],
      "text/plain": [
       "[1] \"원본문장:\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"I am a boy. Your are a boy. He might be a boy. Is he an american? He entered the ICT arena.\"\n",
      "#----------------------------------------"
     ]
    },
    {
     "data": {
      "text/html": [
       "'단어경계를 기준으로 요소별로 분해:'"
      ],
      "text/latex": [
       "'단어경계를 기준으로 요소별로 분해:'"
      ],
      "text/markdown": [
       "'단어경계를 기준으로 요소별로 분해:'"
      ],
      "text/plain": [
       "[1] \"단어경계를 기준으로 요소별로 분해:\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1]]\n",
      " [1] \"I\"        \"am\"       \"a\"        \"boy\"      \"Your\"     \"are\"     \n",
      " [7] \"a\"        \"boy\"      \"He\"       \"might\"    \"be\"       \"a\"       \n",
      "[13] \"boy\"      \"Is\"       \"he\"       \"an\"       \"american\" \"He\"      \n",
      "[19] \"entered\"  \"the\"      \"ICT\"      \"arena\"   \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "'요소별로 분해된 세부요소 갯수:'"
      ],
      "text/latex": [
       "'요소별로 분해된 세부요소 갯수:'"
      ],
      "text/markdown": [
       "'요소별로 분해된 세부요소 갯수:'"
      ],
      "text/plain": [
       "[1] \"요소별로 분해된 세부요소 갯수:\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1]]\n",
      "[1] 22\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "'요소별로 분해된 세부요소들의 글자 갯수:'"
      ],
      "text/latex": [
       "'요소별로 분해된 세부요소들의 글자 갯수:'"
      ],
      "text/markdown": [
       "'요소별로 분해된 세부요소들의 글자 갯수:'"
      ],
      "text/plain": [
       "[1] \"요소별로 분해된 세부요소들의 글자 갯수:\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1]]\n",
      " [1] 1 2 1 3 4 3 1 3 2 5 2 1 3 2 2 2 8 2 7 3 3 5\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "'요소별로 분해된 세부요소들의 기술통계:'"
      ],
      "text/latex": [
       "'요소별로 분해된 세부요소들의 기술통계:'"
      ],
      "text/markdown": [
       "'요소별로 분해된 세부요소들의 기술통계:'"
      ],
      "text/plain": [
       "[1] \"요소별로 분해된 세부요소들의 기술통계:\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1]]\n",
      "\n",
      "       a      boy       He       am american       an      are    arena \n",
      "       3        3        2        1        1        1        1        1 \n",
      "      be  entered       he        I      ICT       Is    might      the \n",
      "       1        1        1        1        1        1        1        1 \n",
      "    Your \n",
      "       1 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 해당 검색패턴으로 분해\n",
    "\n",
    "sprintf('원본문장:')\n",
    "print(my)\n",
    "cat('#----------------------------------------')\n",
    "\n",
    "sprintf('단어경계를 기준으로 요소별로 분해:')\n",
    "str_extract_all(string = my, pattern = boundary('word')) %>% print\n",
    "\n",
    "sprintf('요소별로 분해된 세부요소 갯수:')\n",
    "str_extract_all(string = my, pattern = boundary('word')) %>% map(length) %>% print\n",
    "\n",
    "sprintf('요소별로 분해된 세부요소들의 글자 갯수:')\n",
    "str_extract_all(string = my, pattern = boundary('word')) %>% map(str_length) %>% print\n",
    "\n",
    "sprintf('요소별로 분해된 세부요소들의 기술통계:')\n",
    "str_extract_all(string = my, pattern = boundary('word')) %>% \n",
    "    map(table) %>% map(sort, decreasing = TRUE) %>% print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 텍스트셋 분해결과에 따른 전처리 방향설정\n",
    "* 분해된 단어 중에서 <font color = 'blue'>be 동사</font>의 파생형태인 <font color = 'red'>am, are, is</font>를  <font color = 'blue'>be</font>로 어근통일화를 실시함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 전처리 방향별 정제 작업실시 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 분해된 단어 중에서 <font color = 'blue'>be 동사</font>의 파생형태인 <font color = 'red'>am, are, is</font>를  <font color = 'blue'>be</font>로 어근통일화를 실시함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전처리 방향별 정규표현식 설정\n",
    "\n",
    "org <- '(am)|(are)|(is)' \n",
    "# - 원본패턴: be동사의 파생형태로 간주될만한 단어목록을 검색하는 패턴설정\n",
    "tgt <- 'be' \n",
    "# - 목표패턴: 모두 be라는 동사표현으로 변경해 동일화하는 패턴설정\n",
    "\n",
    "# ==> 영어단어 안에 있는 am, are라는 표현을 무조건 be로 변경하므로 문제가 발생함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전처리 방향별 정규표현식 설정\n",
    "\n",
    "org <- '(\\\\bam\\\\b)|(\\\\bare\\\\b)|(\\\\bis\\\\b)' \n",
    "# - 원본패턴: 불용단어로 간주될만한 단어목록을 검색하는 패턴설정\n",
    "tgt <- 'be' \n",
    "# - 목표패턴: 불용단어에 대해서 모두 삭제하는 패턴설정\n",
    "\n",
    "# ==> 해당 불용어 표현만 정확하게 찾는 단어경계 설정이 되었으나\n",
    "#     대문자로 시작하는 불용어의 경우 체크가 되지 못하는 문제가 발생함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전처리 방향별 정규표현식 설정\n",
    "\n",
    "org <- '(\\\\b[A|a]m\\\\b)|(\\\\b[A:a]re\\\\b)|(\\\\b[I:i]s\\\\b)'  \n",
    "# - 원본패턴: 불용단어로 간주될만한 단어목록을 검색하는 패턴설정\n",
    "tgt <- 'be' \n",
    "# - 목표패턴: 불용단어에 대해서 모두 삭제하는 패턴설정\n",
    "\n",
    "# ==> 해당 불용어 표현만 정확하게 찾는 단어경계 설정이 되었으며,\n",
    "#     대문자로 시작하는 불용어의 경우도 검색되는 패턴설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "'단어경계를 기준으로 요소별로 분해:'"
      ],
      "text/latex": [
       "'단어경계를 기준으로 요소별로 분해:'"
      ],
      "text/markdown": [
       "'단어경계를 기준으로 요소별로 분해:'"
      ],
      "text/plain": [
       "[1] \"단어경계를 기준으로 요소별로 분해:\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1]]\n",
      " [1] \"I\"        \"be\"       \"a\"        \"boy\"      \"Your\"     \"be\"      \n",
      " [7] \"a\"        \"boy\"      \"He\"       \"might\"    \"be\"       \"a\"       \n",
      "[13] \"boy\"      \"be\"       \"he\"       \"an\"       \"american\" \"He\"      \n",
      "[19] \"entered\"  \"the\"      \"ICT\"      \"arena\"   \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "'요소별로 분해된 세부요소 갯수:'"
      ],
      "text/latex": [
       "'요소별로 분해된 세부요소 갯수:'"
      ],
      "text/markdown": [
       "'요소별로 분해된 세부요소 갯수:'"
      ],
      "text/plain": [
       "[1] \"요소별로 분해된 세부요소 갯수:\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1]]\n",
      "[1] 22\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "'요소별로 분해된 세부요소들의 글자 갯수:'"
      ],
      "text/latex": [
       "'요소별로 분해된 세부요소들의 글자 갯수:'"
      ],
      "text/markdown": [
       "'요소별로 분해된 세부요소들의 글자 갯수:'"
      ],
      "text/plain": [
       "[1] \"요소별로 분해된 세부요소들의 글자 갯수:\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1]]\n",
      " [1] 1 2 1 3 4 2 1 3 2 5 2 1 3 2 2 2 8 2 7 3 3 5\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "'요소별로 분해된 세부요소들의 기술통계:'"
      ],
      "text/latex": [
       "'요소별로 분해된 세부요소들의 기술통계:'"
      ],
      "text/markdown": [
       "'요소별로 분해된 세부요소들의 기술통계:'"
      ],
      "text/plain": [
       "[1] \"요소별로 분해된 세부요소들의 기술통계:\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1]]\n",
      "\n",
      "      be        a      boy       He american       an    arena  entered \n",
      "       4        3        3        2        1        1        1        1 \n",
      "      he        I      ICT    might      the     Your \n",
      "       1        1        1        1        1        1 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 특정패턴에 대한 전처리 실시\n",
    "my_pp <- str_replace_all(string = my, pattern = org, replacement = tgt)\n",
    "\n",
    "sprintf('단어경계를 기준으로 요소별로 분해:')\n",
    "str_extract_all(string = my_pp, pattern = boundary('word')) %>% print\n",
    "\n",
    "sprintf('요소별로 분해된 세부요소 갯수:')\n",
    "str_extract_all(string = my_pp, pattern = boundary('word')) %>% map(length) %>% print\n",
    "\n",
    "sprintf('요소별로 분해된 세부요소들의 글자 갯수:')\n",
    "str_extract_all(string = my_pp, pattern = boundary('word')) %>% map(str_length) %>% print\n",
    "\n",
    "sprintf('요소별로 분해된 세부요소들의 기술통계:')\n",
    "str_extract_all(string = my_pp, pattern = boundary('word')) %>% \n",
    "    map(table) %>% map(sort, decreasing = TRUE) %>% print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color = 'blue'>n그램(n-gram) 적용 (p.111)</font>\n",
    "* 엔그램이란 연이어 등장하는 단어들이 결합해 별도의 단일한 개념으로 사용되는 것을 의미함\n",
    "* 영어의 경우 public opinion이란 표현에서 2개 단어를 별도로 분해하는 것이 아니라 여론이라는 의미를 가진 한 단어로 보아야 하는데, \n",
    "<br>이를 2-그램(2-gram) 또는 바이그램(bygram)이라고 부름\n",
    "\n",
    "* the United States나 Republic of Korea도 각각 3단어로 구성되어 있지만 미국과 한국을 지칭하는 한 개 단어이므로 \n",
    "<br>3-그램(3-gram), 트라이그램(trigram)으로 간주함\n",
    "* 텍스트셋에 있는 특정단어의 연속적인 결합이 하나의 단어의미로 사용될 수 있을 때, 이를 한 단어로 처리하는 과정을 n그램 적용이라고 함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 간단 텍스트셋 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "'원본내용:'"
      ],
      "text/latex": [
       "'원본내용:'"
      ],
      "text/markdown": [
       "'원본내용:'"
      ],
      "text/plain": [
       "[1] \"원본내용:\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"The United States comprises fifty states. In the United States of America, each state has its own laws. However, federal law overrides state law in the United States.\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "'객체유형: character객체'"
      ],
      "text/latex": [
       "'객체유형: character객체'"
      ],
      "text/markdown": [
       "'객체유형: character객체'"
      ],
      "text/plain": [
       "[1] \"객체유형: character객체\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "'객체길이: 1요소'"
      ],
      "text/latex": [
       "'객체길이: 1요소'"
      ],
      "text/markdown": [
       "'객체길이: 1요소'"
      ],
      "text/plain": [
       "[1] \"객체길이: 1요소\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "'원본글자수: 166글자'"
      ],
      "text/latex": [
       "'원본글자수: 166글자'"
      ],
      "text/markdown": [
       "'원본글자수: 166글자'"
      ],
      "text/plain": [
       "[1] \"원본글자수: 166글자\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 분석대상 텍스트 데이터 원본 준비\n",
    "my <- 'The United States comprises fifty states. In the United States of America, each state has its own laws. However, federal law overrides state law in the United States.'\n",
    "\n",
    "sprintf('원본내용:')\n",
    "print(my)\n",
    "\n",
    "sprintf('객체유형: %s객체', class(my))\n",
    "sprintf('객체길이: %d요소', length(my))\n",
    "\n",
    "sprintf('원본글자수: %d글자', nchar(my))\n",
    "# - sprintf('검색대상 객체글자수: %d글자', str_length(my))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 단어단위로 직접 분해하는 경우\n",
    "* 보통은 <font color = 'blue'>str_split(string = 텍스트셋, pattern = <font color = 'red'>검색패턴</font>)</font>을 통해서 텍스트셋을 세부요소로 점진적으로 분해할 수 있음\n",
    "* 현재 주어진 텍스트셋에는 단어 이외에 문장부호나 특수문자 등이 섞여있는 구조라서, 이들에 대한 전처리가 필요하지만 \n",
    "<br>단어경계 옵션인 pattern = boundary('word')를 통해서 바로 텍스트셋을 단어단위로 분해할 수 있음\n",
    "* ==> <font color = 'blue'>str_extract_all(string = my, pattern = <font color = 'red'>boundary('word')</font>)</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 검색패턴이 아닌 단어경계 옵션을 이용한 텍스트셋 직접분해"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "'원본문장:'"
      ],
      "text/latex": [
       "'원본문장:'"
      ],
      "text/markdown": [
       "'원본문장:'"
      ],
      "text/plain": [
       "[1] \"원본문장:\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"The United States comprises fifty states. In the United States of America, each state has its own laws. However, federal law overrides state law in the United States.\"\n",
      "#----------------------------------------"
     ]
    },
    {
     "data": {
      "text/html": [
       "'단어경계를 기준으로 요소별로 분해:'"
      ],
      "text/latex": [
       "'단어경계를 기준으로 요소별로 분해:'"
      ],
      "text/markdown": [
       "'단어경계를 기준으로 요소별로 분해:'"
      ],
      "text/plain": [
       "[1] \"단어경계를 기준으로 요소별로 분해:\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1]]\n",
      " [1] \"The\"       \"United\"    \"States\"    \"comprises\" \"fifty\"     \"states\"   \n",
      " [7] \"In\"        \"the\"       \"United\"    \"States\"    \"of\"        \"America\"  \n",
      "[13] \"each\"      \"state\"     \"has\"       \"its\"       \"own\"       \"laws\"     \n",
      "[19] \"However\"   \"federal\"   \"law\"       \"overrides\" \"state\"     \"law\"      \n",
      "[25] \"in\"        \"the\"       \"United\"    \"States\"   \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "'요소별로 분해된 세부요소 갯수:'"
      ],
      "text/latex": [
       "'요소별로 분해된 세부요소 갯수:'"
      ],
      "text/markdown": [
       "'요소별로 분해된 세부요소 갯수:'"
      ],
      "text/plain": [
       "[1] \"요소별로 분해된 세부요소 갯수:\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1]]\n",
      "[1] 28\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "'요소별로 분해된 세부요소들의 글자 갯수:'"
      ],
      "text/latex": [
       "'요소별로 분해된 세부요소들의 글자 갯수:'"
      ],
      "text/markdown": [
       "'요소별로 분해된 세부요소들의 글자 갯수:'"
      ],
      "text/plain": [
       "[1] \"요소별로 분해된 세부요소들의 글자 갯수:\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1]]\n",
      " [1] 3 6 6 9 5 6 2 3 6 6 2 7 4 5 3 3 3 4 7 7 3 9 5 3 2 3 6 6\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "'요소별로 분해된 세부요소들의 기술통계:'"
      ],
      "text/latex": [
       "'요소별로 분해된 세부요소들의 기술통계:'"
      ],
      "text/markdown": [
       "'요소별로 분해된 세부요소들의 기술통계:'"
      ],
      "text/plain": [
       "[1] \"요소별로 분해된 세부요소들의 기술통계:\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1]]\n",
      "\n",
      "   States    United       law     state       the   America comprises      each \n",
      "        3         3         2         2         2         1         1         1 \n",
      "  federal     fifty       has   However        in        In       its      laws \n",
      "        1         1         1         1         1         1         1         1 \n",
      "       of overrides       own    states       The \n",
      "        1         1         1         1         1 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 해당 검색패턴으로 분해\n",
    "\n",
    "sprintf('원본문장:')\n",
    "print(my)\n",
    "cat('#----------------------------------------')\n",
    "\n",
    "sprintf('단어경계를 기준으로 요소별로 분해:')\n",
    "str_extract_all(string = my, pattern = boundary('word')) %>% print\n",
    "\n",
    "sprintf('요소별로 분해된 세부요소 갯수:')\n",
    "str_extract_all(string = my, pattern = boundary('word')) %>% map(length) %>% print\n",
    "\n",
    "sprintf('요소별로 분해된 세부요소들의 글자 갯수:')\n",
    "str_extract_all(string = my, pattern = boundary('word')) %>% map(str_length) %>% print\n",
    "\n",
    "sprintf('요소별로 분해된 세부요소들의 기술통계:')\n",
    "str_extract_all(string = my, pattern = boundary('word')) %>% \n",
    "    map(table) %>% map(sort, decreasing = TRUE) %>% print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 텍스트셋 분해결과에 따른 전처리 방향설정\n",
    "* 분해된 단어 중에서 <font color = 'blue'>미국</font>을 의미하는 <font color = 'red'>the United States</font>를 <font color = 'blue'>the_United_States</font>으로 처리함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 전처리 방향별 정제 작업실시 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 분해된 단어 중에서 <font color = 'blue'>미국</font>을 의미하는 <font color = 'red'>the United States</font>를 <font color = 'blue'>the_United_States</font>으로 처리함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전처리 방향별 정규표현식 설정\n",
    "\n",
    "org <- 'the United States' \n",
    "# - 원본패턴: the United States'라는 표현을 검색하는 패턴설정\n",
    "tgt <- 'the_United_States' \n",
    "# - 목표패턴: 3-gram으로 처리하는 패턴설정\n",
    "\n",
    "# ==> 대문자로 시작하는 The United States문구의 경우 누락되었으며, \n",
    "#     또다른 표현인 the United States of America에서는 of America라는 단어가 별도로 남게되는 문제발생 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전처리 방향별 정규표현식 설정\n",
    "\n",
    "org <- '[T:t]he United States'  \n",
    "# - 원본패턴: the United States'라는 표현에서 대문자 T도시작하는 표현도 검색하는 패턴설정\n",
    "tgt <- 'the_United_States' \n",
    "# - 목표패턴: 3-gram으로 처리하는 패턴설정\n",
    "\n",
    "# ==> 대문자로 시작하는 문구의 경우도 n-gram으로 처리되는 패턴설정\n",
    "#     여전히 또다른 표현인 the United States of America에서는 of America라는 단어가 별도로 남게되는 문제발생"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전처리 방향별 정규표현식 설정\n",
    "\n",
    "org <- '[T:t]he United States( of America)?'  \n",
    "# - 원본패턴: of America가 붙어 있는 표현도 검색하는 패턴설정\n",
    "tgt <- 'the_United_States' \n",
    "# - 목표패턴: 3-gram으로 처리하는 패턴설정\n",
    "\n",
    "# ==> 대문자로 시작하는 문구의 경우도 n-gram으로 처리되는 패턴설정\n",
    "#     또다른 표현인 the United States of America에서 of America라는 단어가 있는 경우에도 n-gram으로 처리되도록 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "'단어경계를 기준으로 요소별로 분해:'"
      ],
      "text/latex": [
       "'단어경계를 기준으로 요소별로 분해:'"
      ],
      "text/markdown": [
       "'단어경계를 기준으로 요소별로 분해:'"
      ],
      "text/plain": [
       "[1] \"단어경계를 기준으로 요소별로 분해:\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1]]\n",
      " [1] \"the_United_States\" \"comprises\"         \"fifty\"            \n",
      " [4] \"states\"            \"In\"                \"the_United_States\"\n",
      " [7] \"each\"              \"state\"             \"has\"              \n",
      "[10] \"its\"               \"own\"               \"laws\"             \n",
      "[13] \"However\"           \"federal\"           \"law\"              \n",
      "[16] \"overrides\"         \"state\"             \"law\"              \n",
      "[19] \"in\"                \"the_United_States\"\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "'요소별로 분해된 세부요소 갯수:'"
      ],
      "text/latex": [
       "'요소별로 분해된 세부요소 갯수:'"
      ],
      "text/markdown": [
       "'요소별로 분해된 세부요소 갯수:'"
      ],
      "text/plain": [
       "[1] \"요소별로 분해된 세부요소 갯수:\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1]]\n",
      "[1] 20\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "'요소별로 분해된 세부요소들의 글자 갯수:'"
      ],
      "text/latex": [
       "'요소별로 분해된 세부요소들의 글자 갯수:'"
      ],
      "text/markdown": [
       "'요소별로 분해된 세부요소들의 글자 갯수:'"
      ],
      "text/plain": [
       "[1] \"요소별로 분해된 세부요소들의 글자 갯수:\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1]]\n",
      " [1] 17  9  5  6  2 17  4  5  3  3  3  4  7  7  3  9  5  3  2 17\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "'요소별로 분해된 세부요소들의 기술통계:'"
      ],
      "text/latex": [
       "'요소별로 분해된 세부요소들의 기술통계:'"
      ],
      "text/markdown": [
       "'요소별로 분해된 세부요소들의 기술통계:'"
      ],
      "text/plain": [
       "[1] \"요소별로 분해된 세부요소들의 기술통계:\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1]]\n",
      "\n",
      "the_United_States               law             state         comprises \n",
      "                3                 2                 2                 1 \n",
      "             each           federal             fifty               has \n",
      "                1                 1                 1                 1 \n",
      "          However                in                In               its \n",
      "                1                 1                 1                 1 \n",
      "             laws         overrides               own            states \n",
      "                1                 1                 1                 1 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 특정패턴에 대한 전처리 실시\n",
    "my_pp <- str_replace_all(string = my, pattern = org, replacement = tgt)\n",
    "\n",
    "sprintf('단어경계를 기준으로 요소별로 분해:')\n",
    "str_extract_all(string = my_pp, pattern = boundary('word')) %>% print\n",
    "\n",
    "sprintf('요소별로 분해된 세부요소 갯수:')\n",
    "str_extract_all(string = my_pp, pattern = boundary('word')) %>% map(length) %>% print\n",
    "\n",
    "sprintf('요소별로 분해된 세부요소들의 글자 갯수:')\n",
    "str_extract_all(string = my_pp, pattern = boundary('word')) %>% map(str_length) %>% print\n",
    "\n",
    "sprintf('요소별로 분해된 세부요소들의 기술통계:')\n",
    "str_extract_all(string = my_pp, pattern = boundary('word')) %>% \n",
    "    map(table) %>% map(sort, decreasing = TRUE) %>% print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# End of Source"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.5.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "259px",
    "width": "160px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "364px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "448px",
    "left": "906px",
    "right": "20px",
    "top": "228px",
    "width": "521px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
