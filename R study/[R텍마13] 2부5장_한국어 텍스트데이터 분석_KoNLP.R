####################################################################################################
# 1. 필요패키지 준비
####################################################################################################

# --------------------------------------------------
# 1.1 한글자연어처리(형태소분석) KoNLP패키지 설치 & 로딩
# --------------------------------------------------

# 사용중인 PC운영체제에 맞는 자바실행환경이 어느 경로에 설치되어 있는지 경로명을 자바홈 디렉토리로 설정함
Sys.setenv(JAVA_HOME = 'C:/Program Files/Java/jre1.8.0_201')

# KoNLP 한글자연어처리 패키지 인스톨
install.packages('KoNLP')

# 패키지 설치시 특정한 라이브러리 폴더를 직접지정해서 저장하는 방법
# => install.packages("zoo", lib="C:/software/Rpackages")

# 특정패키지 설치시연결되어 있는 다른 패키지도 같이 다운/설치하며, 
# - 다운로드시 접속하는 CRAN사이트를 별도로 지정하하는 방법 
# => install.packages(new_pkg, dependencies = TRUE, repos="http://R-Forge.R-project.org")

# KoNLP 한글자연어처리 패키지 메모리로 로딩
library(KoNLP)

# 패키지 로딩시에 특정한 폴더에 있는 라이브러리를 직접지정해서 로딩하는 방법
# => library("zoo", lib.loc="C:/software/Rpackages")

# --------------------------------------------------
# 1.2 여러 패키지 동시 인스톨 & 로딩
# --------------------------------------------------
# !!!! 가급적 KoNLP패키지를 먼저 인스톨할 수 있도록 배치순서를 제일먼저 설정함

# 필요한 패키지 목록 생성
pkg <- c('KoNLP', 'stringr', 'magrittr', 'purrr', 'dplyr', 'readr', 'tm', 'RWeka')

# 필요패키지 설지여부를 체크해 미설치패키지 목록을 저장
new_pkg <- pkg[!(pkg %in% rownames(installed.packages()))]

# 미설치 패키지 목록이 1개라도 있으면, 일괄 인스톨 실시
if (length(new_pkg)) install.packages(new_pkg, dependencies = TRUE)
# if (length(new_pkg)) install.packages(new_pkg, dependencies = TRUE, repos="http://R-Forge.R-project.org")

# 필요패키지를 일괄 로딩실시
suppressMessages(sapply(pkg, require, character.only = TRUE))  
# - suppressMessages() 함수를 통해 패키지 로딩시 나타나는 복잡한 설명/진행상황 문구 출력억제

####################################################################################################
# 2. 한글자연어처리 KoNLP패키지 소개
####################################################################################################

# --------------------------------------------------
# 2.1 한글형태소사전 이해
# --------------------------------------------------

# 2.1.1 한글형태소 사전을 이용하는 이유
# --------------------------------------------------
# - 기본적으로 tm패키지를 통해서 영어든, 한글이든 텍스트셋에 대해서 
#   단어사이에 있는 공백 1칸을 기준으로 개별단어별로 분해해서 분석을 진행하면 됨
# - 영미권 언어의 경우에는 분해된 단어에서 불용어나 어근동일화 알고리즘을 적용해
#   분석대상 텍스트셋에서 노이즈를 제거해 복잡도를 낮추는 게 가능함
# - 반면에, 한글텍스트셋의 경우에는 별도의 불용어나 어근동일화 관련 알고리즘을 적용하지 못하므로
#   tm패키지만을 적용해 전처리한 결과에는 상당한 노이즈가 포함되어 있는 것임
# - 이에 따라 tm패키지로 일련의 전처리를 진행한 한글코퍼스객체에 대해서
#   별도의 한글형태소 분석기 적용을 통해 한글의 경우 의미전달의 핵심인 명사형 단어를 추출하도록 한것임
# - 이를 통해 분해된 한글단어 중에서 (1) 무의미한 불용어에 해당하는 표현들을 삭제하고, 
#   (2) 단어의미는 동일/유사한데 어미에서 다양한 변화가 주어지는 단어들을 통일하고,
#   (3) 분해되어서 오히려 의미가 희석되는 경우에는 새로운 단어로 등록을 해서 작업을 진행할 수 있음

# 2.1.2 한글형태소 사전 유형 
# --------------------------------------------------

# 2.1.2.1  세종형태소사전(SejongDic)
# - 국립국어원에서 21세기 세종계획의 일환으로 진행된 한글형태소사전(9만단어) 연구결과물을
#   기존의 시스템형태소사전(28만단어)에 결합한 것임

# 세종계획에 관한 시리즈 기사
# [기획] AI 핵심기술 ‘한국어 자연어 처리’ 세종계획
browseURL('http://www.itnews.or.kr/?p=24600')
browseURL('http://www.itnews.or.kr/?p=24631')
browseURL('http://www.itnews.or.kr/?p=24673')

# 인공지능 씨앗 한글 말뭉치, 2007년 멈춰선 까닭
browseURL('https://www.bloter.net/archives/260569')
# - 다양한 한글 말뭉치사전 명칭과 이용방법에 대한 소개

# [한국어와 인공지능] 제2의 세종계획 추진해야
browseURL('http://biz.chosun.com/site/data/html_dir/2016/10/09/2016100900328.html')

# 2.1.2.2 NIA형태소사전(NIADic)
# - 한국정보화진흥원(National Informatization Agency) K-ICT 빅데이터센터에서 
#   기존 형태소 사전의 단어수를 업그레이드하고 여러 분야별 주제어로 분류해 놓아 
#   자체사전 개발이 어려운  중소기업, 스타트업, 연구소, 학교에서 정확도 높은 텍스트 분석 가능
# 
# NIA형태소사전 소개사이트
browseURL('https://kbig.kr/portal/kbig/datacube/niadict.page')
# - 해당사이트 하단에 있는 "바로가기" 링크를 누르면 엑셀형식의 NIA형태소사진 풀내용을 확인할 수 있음
# - 이를 다양한 분석도구에서 기본사전으로 활용이 가능함

# NIA형태소사전에 포함되어 있는 개별사전
# (1) 세종(Sejong) 사전
browseURL('https://cran.r-project.org/web/packages/KoNLP/vignettes/KoNLP-API.html')

# (2) 우리말샘(woorimalsam) 사전
# - 전체 576,045개의 단어, 68개의 단어 카테고리 존재
browseURL('https://htmlpreview.github.io/?https://github.com/haven-jeon/NIADic/blob/master/NIADic/vignettes/woorimalsam-dic.html')
browseURL('https://opendict.korean.go.kr/main')

# (3) 인사이터(insighter)
# - 전체 353,115개의 단어를 비롯해 위치, 브랜드, 인명 세부 카테고리 포함
browseURL('https://htmlpreview.github.io/?https://github.com/haven-jeon/NIADic/blob/master/NIADic/vignettes/insighter-dic.html')

# --------------------------------------------------
# 2.2 KoNLP 형태소 사전 로딩방법
# --------------------------------------------------

# 2.2.1 KoNLP 설치내용 확인
# --------------------------------------------------

# R패키지 라이브러리 설치경로 확인
.libPaths()

# R 기여패키지 설지경로 중  KoNLP 패키지의 사전(Dictionary)가 설정된 경로 확인
dic_loc <- paste(.libPaths()[1], '/KoNLP_dic', sep = '') %>% print 
browseURL(dic_loc)
# - current 폴더에 들어 있는 dic_user.txt 파일을 오픈해 보면 아주 간단한 단어목록을 볼 수 있음

# 형태소사전 파일로딩
dic_file <- paste(dic_loc, '/current/dic_user.txt', sep = '') %>% print
dic_base <- read_delim(file = dic_file, col_names = FALSE, delim = '\t')
head(dic_base)
str(dic_base)

# 2.2.2 세종사전 로딩
# --------------------------------------------------

# 2.2.2.1 세종사전 로딩함수
useSejongDic()
# - 약 37만(370957개) = 세종사전(8만7천여개) + 시스템사전(28만여개) 단어를 로딩해서 준비시킴 
# - 세종사전 로딩후 다시 KoNLP라이브러리의 current 폴더에 있는 
#   dic_user.txt파일을 오픈해 보면 세종사전의 단어목록이 업데이트 되어 있음을 알 수 있음
# - 별도로 useSystemDic()으로 로딩하면, 약 28만(283949) 여개 단어가 포함된 시스템사전만을 이용하는 것임

# 2.2.2.2 세종사전 파일내용

# 세종사전 파일을 데이터프레임으로 로딩
dic_sj <- read_delim(file = dic_file, col_names = FALSE, delim = '\t')
head(dic_sj)
tail(dic_sj)
colnames(dic_sj) <- c('term', 'tag')
str(dic_sj)

# 세종사전 용어와 품사 현황파악
dic_sj %>% select(tag) %>% table %>% sort(decreasing = TRUE) %>% print %>% length
# - 전체 용어(term) 갯수는 87008개이며, 품사태그(pos tag)는 ncn(체언-보통명사-비서술성명사) 1종류임

# 2.2.3 NIA사전 로딩
# --------------------------------------------------
# - 전체 576,045개의 단어, 68개의 단어 카테고리 존재

# 2.2.3.1 NIA사전 로딩함수
useNIADic()
# - 약 98만(983012) 개 정도의 한글단어가 등임록되어 있음
# - NIA사전 로딩후 다시 KoNLP라이브러리의 current 폴더에 있는 
#   dic_user.txt파일을 오픈해 보면 NIA사전의 단어목록이 업데이트 되어 있음을 알 수 있음

# ==> 3가지 사전을 모두 로딩할 필요는 없으며, 가장 나중에 로딩한 사전이 디폴트사전으로 설정됨
#     통상적으로 가장 포괄적인 형태소데이터가 들어가 있는 useNIADic() 사전을 로딩해서 사용하면됨

# 2.2.3.2 NIA사전 파일내용
# NIA사전 파일을 데이터프레임으로 로딩
dic_nia <- read_delim(file = dic_file, col_names = FALSE, delim = '\t')
class(dic_nia)
head(dic_nia)
tail(dic_nia)
colnames(dic_nia) <- c('term', 'tag')
str(dic_nia)

# 2.2.3.3 NIA사전 용어와 품사 현황파악
dic_nia %>% select(tag) %>% table %>% sort(decreasing = TRUE) %>% print %>% length
# - 전체 용어(term) 갯수는 683649개, 품사태그(pos tag)는 23종류인데, 
#   이 중에서 ncn(체언-보통명사-비서술성명사) 유형이 509372개로 가장 많음
# - 이어서 pvg(용언-동사-일반동사)가 119099개로 2순위, 
#   mag(수식언-부사-접속부사)가 22480으로 3번째로 많은 용어가 있는 것으로 나타남

# 2.2.4 NIA사전 중 우리말샘(woorimalsam) 사전로딩
# --------------------------------------------------

# 2.2.4.1 우리말샘(woorimalsam) 로딩
# NIA사전 중 우리말샘 내용을 데이터프레임으로 로딩
dic_wrms <- get_dictionary('woorimalsam')
class(dic_wrms)
head(dic_wrms)
tail(dic_wrms)
str(dic_wrms)

# 2.2.4.2 우리말샘사전 용어와 품사, 카테고리 현황파악

# 우리말샘사전 품사 현황
dic_wrms %>% select(tag) %>% table %>% sort(decreasing = TRUE) %>% print %>% length
# - 전체 용어(term) 갯수는 576045개, 품사태그(pos tag)는 12종류인데, 
#   이 중에서 ncn(체언-보통명사-비서술성명사) 유형이 447531개로 가장 많음
# - 이어서 pvg(용언-동사-일반동사)가 119099개로 2순위, 
#   pad(용언-형용사-지시형용사_가 22410개로 3번째, )
#   mag(수식언-부사-접속부사)가 22480개로 4번째로 많은 용어가 있는 것으로 나타남

# 우리말샘사전 카테고리(한글명칭) 현황
dic_wrms %>% select(category) %>% table %>% sort(decreasing = TRUE) %>% print %>% length
# - 전체 68개 카테고리이며, 일반, 역사, 의학 순으로 나타남

# 우리말샘사전 카테고리(영어명칭) 현황
dic_wrms %>% select(eng_cate) %>% table %>% sort(decreasing = TRUE) %>% print %>% length
# - 전체 67개 카테고리이며, general, history, life 순으로 나타남

# 2.2.5 NIA사전 중 인사이터(insighter) 사전로딩
# --------------------------------------------------
# - 전체 353,115개의 단어를 비롯해 위치, 브랜드, 인명 세부 카테고리 포함

# 2.2.4.1 인사이터(insighter) 로딩
# NIA사전 중 우리말샘 내용을 데이터프레임으로 로딩
dic_inst <- get_dictionary('insighter')
class(dic_inst)
head(dic_inst)
tail(dic_inst)
str(dic_inst)

# 2.2.4.2 인사이터사전 용어와 품사, 카테고리 현황파악

# 인사이터사전 품사 현황
dic_inst %>% select(tag) %>% table %>% sort(decreasing = TRUE) %>% print %>% length
# - 전체 용어(term) 갯수는 353115개, 품사태그(pos tag)는 21종류인데, 
#   이 중에서 ncn(체언-보통명사-비서술성명사) 유형이 219494개로 가장 많음
# - 이어서 pvg(용언-동사-일반동사)가 57966개로 2순위, 
#   mag(수식언-부사-접속부사)가 2144개로 3번째로 많은 용어가 있는 것으로 나타남

# 인사이터사전 카테고리 현황
dic_inst %>% select(in_category) %>% table %>% sort(decreasing = TRUE) %>% print %>% length
# - 전체 10개 카테고리이며, brand_name, people_names, verve 순으로 나타남

# --------------------------------------------------
# 2.3 형태소와 품사관련 주요 개념
# --------------------------------------------------

# 한글 형태소와 품사의 개념
browseURL('https://tip.daum.net/question/92795630')
# - 형태소(Morpheme): 더 이상 분석하면 뜻을 잃어버리는 말의 최소단위
# - 품사(PoS: Part of Speech): 공통된 성질을 지닌 단어끼리 모아 놓은 단어의 갈래

# 한글 형태소분석 및 품사태깅/표지/주석(POS tagging; POS annotation)
browseURL('http://konlpy.org/ko/v0.4.3/morph/')

# 한글형태소 분류체계
browseURL('https://github.com/haven-jeon/KoNLP/blob/master/etcs/KoNLP-API.md')
# - KAIST에서 분류한 형태소 분류와 각 분류항목에 부여된 태그집합
# - 이 한글분류체계 내용 중 N:체언항목에 속한 NC:보통명사가 분석의 핵심임
# - 가장 왼쪽에 있는 첫번째 레벨이 9품사임
#   => S기호, F외국어, N체언, P용언, M수식언, I독립언, J관계언, E어미, X접사 
# - 두번째 레벨의 22개 요소가 22개 품사구문임

####################################################################################################
# 3. NIADic형태소사전을 활용한 형태소 분석
####################################################################################################

# --------------------------------------------------
# 3.1 텍스트셋 준비
# --------------------------------------------------

# 준비된 코퍼스객체 데이터파일 로딩
load('./../datatm/ymbaek_papers_kor_pp.Rdata')
# - 다양한 전처리 작업을 거친 코퍼스객체를 저장한 .Rdata파일을 로딩

# 메모리에 로딩된 코퍼스객체 확인
ls()
# - ymbaek_papers_kor_pp.Rdata이라는 데이터파일이 로딩되면서, 
#   같이 묶여 있던 원본코퍼스객체인 my객체, 엔그램까지 전처리를 적용한 my_pp_gram객체가 
#   각각 별도 객체로 메모리 상에 존재함 

# 전처리된 코퍼스객체 기본조회
my_pp_gram

# --------------------------------------------------
# 3.2 개별 텍스트자료에 대한 한글 형태소&품사 분석
# --------------------------------------------------

# 2.3.1 말뭉치객체 내용 중 특정한 문서 1개를 별도로 추출해 분석대상 문자열로 만듬
# --------------------------------------------------
txt <- my_pp_gram[[1]]$content
txt
class(txt)
# - 일반적인 문자열객체임

# 2.3.2 KoNLP::extractNoun()함수를 이용한 한글명사추출
# --------------------------------------------------
extractNoun(txt)
# - 한글텍스트셋에 들어 있는 다양한 문구/단어들을 형태소로 분해한 다음에 
#   여러가지 품사들 중에서 의미파악이 상대적으로 쉬운 명사를 중심으로 추출함  

# 2.3.3 KoNLP::MorphAnalyzer()함수를 이용한 한글형태소 추출
# --------------------------------------------------
MorphAnalyzer(txt)
# - 한글텍스트셋에 들어 있는 문구/단어들에 대해서 여러 형태소 조합으로 분해를 하여 
#   핵심적인 의미를 다양한 각도에서 파악할 수 있도록 보여줌
# - 각 단어/문구별로 여러개 형태소로 분해된 다음에 각각 어떠한 품사에 속하는지 영어약칭으로 태킹되어 있음

# 2.3.3 형태소 분류체계의 첫번째 레벨인 9개 품사내용 추출
# --------------------------------------------------
# 한글 형태소 중 9개 품사 유형에 속한 태깅내용 추출
txt_pos9 <- SimplePos09(txt)
txt_pos9 %>% unlist %>% print %>% length

# 한글 형태소 22개 품사 중 N:체언 유형에 속한 태깅내용 추출
txt_pos9_nc <- sapply(txt_pos9, str_extract_all, pattern = '[:alpha:]{1,}/N')
txt_pos9_nc %>% unlist %>% print %>% length

# 한글 형태소 22개 품사 중 N:체언 유형에 속한 순수텍스트내용 추출
txt_pos9_nc_out <- sapply(txt_pos9_nc, str_replace_all, pattern = '/N', replacement = '')
txt_pos9_nc_out %>% unlist %>% print %>% length

# 2.3.4 형태소 분류체계의 두번째 레벨인 22개 품사내용 추출
# --------------------------------------------------
# 한글 형태소 중 22개 품사 유형에 속한 태깅내용 추출
txt_pos22 <- SimplePos22(txt)
txt_pos22 %>% unlist %>% print %>% length

# 한글 형태소 22개 품사 중 N:체언 > NC:보통명사 유형에 속한 태깅내용 추출
txt_pos22_nc <- sapply(txt_pos22, str_extract_all, pattern = '[:alpha:]{1,}/NC')
txt_pos22_nc %>% unlist %>% print %>% length

# 한글 형태소 22개 품사 중 N:체언 > NC:보통명사 유형에 속한 순수텍스트내용 추출
txt_pos22_nc_out <- sapply(txt_pos22_nc, str_replace_all, pattern = '/NC', replacement = '')
txt_pos22_nc_out %>% unlist %>% print %>% length

# --------------------------------------------------
# 3.3 tm패키지에 KoNLP 사용을 통한 형태소 분석
# --------------------------------------------------

# 3.3.1 말뭉치 각 문서별로 한글명사추출
# --------------------------------------------------
my_mp <- tm_map(my_pp_gram, content_transformer(extractNoun))
# - 앞서 엔그램까지 전처리가 완료된 my_pp_gram 코퍼스객체를 인풋객체로 사용함

# 객체유형 파악
class(my_mp)
# - tm::tm_map()함수에 KoNLP::extractNoun() 함수를 적용해 각 문서별로 한글명사를 추출했으므로
#   코퍼스객체 특성이 계속 유지됨

# 각 문서별 한글명사 추출내용 파악
sprintf('말뭉치 문서별 한글명사 추출단어를 content항목에서 파악:')
sapply(my_mp, extract, 1) %>% print
# - 각 문서별로 내용 순서대로 분리된 단어패턴을 보고 어떤 것이 전처리 대상인지 판단(전처리 적용순서는 상관없음)
#   (1) 신규단어등록 필요 ==> 단어가 지나치게 분해되어서 오히려 의미가 희석되는 경우 
#       - NIADic형태소 사전에 등록되어 있지 않아서 하나의 독립적인 명사로 추출되지 못하고 분해되는 단어는 
#         신규단어로 간주하여 NIADic형태소 사전에 새로운 단어로 등록을 해서 extractNoun()함수 적용시 
#         분해되지 않고 하나의 명사로 추출될 수 있도록 함
#       - 신규단어를 직접 하나하나 NIADic에 등록해 나가면서 전처리 가능함 
#       - 별도의 신규단어사전(소스코드상에 벡터객체로, 또는 csv/엑셀파일로)을 만들어 놓고 
#         신규단어목록을 한꺼번에 NIADic에 업데이트 할 수 있음
#   (2) 불용어처리 필요 ==> 무의미한 불용어에 해당하는 경우 
#       - 일정한 패턴이 있는 불용어들은 정규표현식을 통해서 일괄삭제토록 함
#       - 다양한 철자로 인해서 일정한 패턴으로 검색하기 어려운 경우에는
#         직접 1:1변환방식으로 제거를 해나가면서 전처리 가능함 
#       - 별도의 불용어사전(소스코드상에 벡터객체로, 또는 csv/엑셀파일로)을 만들어 놓고 
#         해당하는 불용어 하나하나를 등록해서 일괄 제거함
#   (3) 어근동일화 필요 ==> 단어의미(어간)은 동일/유사한데 어미에서 다양한 변화가 주어지는 경우
#       - 일정한 패턴이 있는 동의/유사어들은 정규표현식을 통해서 일괄 특정단어로 동일화함
#       - 다양한 철자로 인해서 일정한 패턴으로 검색하기 어려운 경우에는
#         직접 1:1변환방식으로 특정단어로 변경을 해나가면서 전처리 가능함 
#       - 별도의 동의/유사어사전(소스코드상에 벡터객체로, 또는 csv/엑셀파일로)을 만들어 놓고 
#         해당하는 신규단어 하나하나를 등록해서 일괄 동일화함 

# 말뭉치 전체내용 중 한글명사 추출내용 빈도수 파악
sprintf('전체 말뭉치에서 한글명사 추출내용을 문자순서 기준 정렬:')
sapply(my_mp, extract, 1) %>%
  unlist %>% table %>% print %>% sum
# - 말뭉치를 구성하는 각 문서가 아닌 전체 내용을 통합했을 때 
#   추출된 한글명사 단어들을 문자순서를 기준으로 정렬해 파악함
# - 이 조회내용을 토대로 명사적 어간표현은 동일한데 어미가 달라서 
#   다른 단어로 카운팅되는 표현들을 찾아내어 전처리 방향성을 검토해야 함
# - 앞서 (1) 신규단어등록, (2) 불용어처리, (3) 어근동일화 라는 3가지 전처리 관점에서 검토필요

sprintf('전체 말뭉치에서 한글명사 추출내용을 빈도수 기준 정렬:')
sapply(my_mp, extract, 1) %>%
  unlist %>% table %>% sort(decreasing = TRUE) %>% print %>% sum
# - 말뭉치를 구성하는 각 문서가 아닌 전체 내용을 통합했을 때 
#   추출된 한글명사 단어들을 출현빈도수를 기준으로 정렬해 파악함
# - 이 조회내용을 토대로 어떠한 빈도가 높은 한글명사 단어에 집중해서 
#   전처리가 필요한지를 검토해야 함
# - 앞서 (1) 신규단어등록, (2) 불용어처리, (3) 어근동일화 라는 3가지 전처리 관점에서 검토필요

# 3.3.2 전체 말뭉치에서 한글명사 추출내용을 데이터프레임 구조로 파악
# --------------------------------------------------

# 전체 말뭉치에서 한글명사 추출내용을 별도의 임시변수에 저장
my_mp_tb <-sapply(my_mp, extract, 1) %>% unlist %>% table
my_mp_tb

# 전체 말뭉치에서 한글명사 추출내용을 별도의 데이터프레임 객체로 저장
my_mp_df <- data.frame(term = names(my_mp_tb), freq = c(my_mp_tb), row.names = NULL)

# 전체 말뭉치에서 한글명사 추출내용을 문자순서를 기준으로 정렬
my_mp_df %>% print %>% nrow
# - 말뭉치를 구성하는 각 문서가 아닌 전체 내용을 통합했을 때 
#   추출된 한글명사 단어들을 문자순서를 기준으로 정렬해 파악함
# - 이 조회내용을 토대로 명사적 어간표현은 동일한데 어미가 달라서 
#   다른 단어로 카운팅되는 표현들을 찾아내어 전처리 방향성을 검토해야 함
# - 앞서 (1) 신규단어등록, (2) 불용어처리, (3) 어근동일화 라는 3가지 전처리 관점에서 검토필요

# 전체 말뭉치에서 한글명사 추출내용을 빈도수를 기준으로 내림차순 정렬
my_mp_df %>% dplyr::arrange(desc(freq), term) %>% print
# - 말뭉치를 구성하는 각 문서가 아닌 전체 내용을 통합했을 때 
#   추출된 한글명사 단어들을 출현빈도수를 기준으로 정렬해 파악함
# - 이 조회내용을 토대로 어떠한 빈도가 높은 한글명사 단어에 집중해서 
#   전처리가 필요한지를 검토해야 함
# - 앞서 (1) 신규단어등록, (2) 불용어처리, (3) 어근동일화 라는 3가지 전처리 관점에서 검토필요

# 전체 말뭉치에서 한글명사 추출내용을 일정수준 이상의 빈도수를 기준으로 내림차순 정렬
my_mp_df %>% arrange(desc(freq), term) %>% filter(freq >= 10) %>% print

# --------------------------------------------------
# 3.4 데이터순환용 base패키지에 KoNLP 사용을 통한 한글단어 탐색
# --------------------------------------------------

# 3.4.1 말뭉치 각 문서별로 한글명사추출
my_mp_base <- sapply(my_pp_gram, extractNoun) %>% print
# - 코퍼스객체를 구성하는 각 문서별로 몇 개 명사로 추출이 되었는지 나타남
# - 앞서 엔그램까지 전처리가 완료된 my_pp_gram 코퍼스객체를 인풋객체로 사용함

my_mp_base <- lapply(my_pp_gram, extractNoun) %>% print
# - 코퍼스객체를 구성하는 각 문서별로 추출된 명사 목록이 나타남
# - 앞서 엔그램까지 전처리가 완료된 my_pp_gram 코퍼스객체를 인풋객체로 사용함

# 객체유형 파악
class(my_mp_base)
# - base:: sapply와 lapply 함수에 KoNLP::extractNoun() 함수를 적용해 각 문서별로 한글명사를 추출했으므로
#   코퍼스객체 특성이 해제되고, 일반 리스트객체 형식으로 만들어짐

# lapply를 통해 추출한 한글단어에 대해서도 동일하게 
# - unlist, table을 적용해 문자순, 빈도순으로 정렬해서 후속분석에 사용이 가능함

### End of Documents ###############################################################################
